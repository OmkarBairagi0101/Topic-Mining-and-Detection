{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7084085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>attented</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>...</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>waiting</th>\n",
       "      <th>wasnnto</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>write</th>\n",
       "      <th>younes</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  algorithm  analysis  application  approximate  assignment  attented  \\\n",
       "0   0          1         1            0            1           0         0   \n",
       "1   0          0         1            1            0           0         0   \n",
       "2   0          0         0            0            0           0         0   \n",
       "3   1          0         0            0            0           0         0   \n",
       "4   0          0         0            0            0           0         0   \n",
       "5   0          0         1            0            0           1         1   \n",
       "6   0          0         0            0            0           0         0   \n",
       "7   0          0         0            0            0           0         0   \n",
       "8   0          0         1            0            0           0         0   \n",
       "9   0          0         0            0            0           0         0   \n",
       "\n",
       "   aug  awesome  bayes  ...  vector  video  visualize  waiting  wasnnto  week  \\\n",
       "0    0        0      1  ...       2      1          1        0        0     0   \n",
       "1    0        0      0  ...       0      0          0        0        0     0   \n",
       "2    0        0      0  ...       0      0          0        0        0     0   \n",
       "3    0        0      0  ...       0      0          0        0        0     0   \n",
       "4    0        0      0  ...       0      0          0        0        0     0   \n",
       "5    2        1      0  ...       1      0          0        1        1     1   \n",
       "6    0        0      0  ...       0      0          0        0        0     0   \n",
       "7    0        0      0  ...       0      0          0        0        0     0   \n",
       "8    0        0      0  ...       1      0          0        0        0     0   \n",
       "9    0        0      0  ...       0      0          0        0        0     0   \n",
       "\n",
       "   word  write  younes  youtube  \n",
       "0     3      1       0        0  \n",
       "1     0      0       0        0  \n",
       "2     0      0       0        0  \n",
       "3     0      0       1        0  \n",
       "4     0      0       0        0  \n",
       "5     1      0       0        0  \n",
       "6     0      0       0        1  \n",
       "7     0      0       0        0  \n",
       "8     0      0       0        0  \n",
       "9     0      0       0        0  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef8f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d76bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approximate</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2  3  4  5  6  7  8  9\n",
       "ai           0  0  0  1  0  0  0  0  0  0\n",
       "algorithm    1  0  0  0  0  0  0  0  0  0\n",
       "analysis     1  1  0  0  0  1  0  0  1  0\n",
       "application  0  1  0  0  0  0  0  0  0  0\n",
       "approximate  1  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22350489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:46,411 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-24 12:21:46,414 : INFO : using symmetric eta at 0.5\n",
      "2022-02-24 12:21:46,415 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:46,423 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:46,446 : INFO : -5.478 per-word bound, 44.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,447 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:46,463 : INFO : topic #0 (0.500): 0.036*\"course\" + 0.028*\"analysis\" + 0.027*\"vector\" + 0.024*\"word\" + 0.019*\"aug\" + 0.019*\"logistic\" + 0.018*\"specialization\" + 0.016*\"using\" + 0.016*\"localitysensitive\" + 0.016*\"translation\"\n",
      "2022-02-24 12:21:46,464 : INFO : topic #1 (0.500): 0.030*\"specialization\" + 0.025*\"instructor\" + 0.025*\"learning\" + 0.020*\"course\" + 0.018*\"helped\" + 0.017*\"deep\" + 0.017*\"model\" + 0.017*\"language\" + 0.016*\"word\" + 0.016*\"using\"\n",
      "2022-02-24 12:21:46,465 : INFO : topic diff=0.565857, rho=1.000000\n",
      "2022-02-24 12:21:46,483 : INFO : -5.114 per-word bound, 34.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,485 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:46,500 : INFO : topic #0 (0.500): 0.040*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.027*\"word\" + 0.021*\"logistic\" + 0.020*\"specialization\" + 0.020*\"using\" + 0.019*\"language\" + 0.018*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,502 : INFO : topic #1 (0.500): 0.030*\"instructor\" + 0.030*\"learning\" + 0.029*\"specialization\" + 0.022*\"helped\" + 0.021*\"deep\" + 0.017*\"model\" + 0.015*\"designed\" + 0.014*\"build\" + 0.013*\"machine\" + 0.013*\"straight\"\n",
      "2022-02-24 12:21:46,504 : INFO : topic diff=0.248715, rho=0.577350\n",
      "2022-02-24 12:21:46,514 : INFO : -4.997 per-word bound, 31.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,515 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:46,524 : INFO : topic #0 (0.500): 0.041*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.028*\"word\" + 0.022*\"logistic\" + 0.021*\"using\" + 0.021*\"specialization\" + 0.021*\"language\" + 0.017*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,525 : INFO : topic #1 (0.500): 0.033*\"instructor\" + 0.033*\"learning\" + 0.028*\"specialization\" + 0.024*\"helped\" + 0.023*\"deep\" + 0.017*\"model\" + 0.015*\"designed\" + 0.015*\"build\" + 0.014*\"straight\" + 0.014*\"detailed\"\n",
      "2022-02-24 12:21:46,526 : INFO : topic diff=0.144994, rho=0.500000\n",
      "2022-02-24 12:21:46,539 : INFO : -4.955 per-word bound, 31.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,540 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:46,550 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.022*\"logistic\" + 0.022*\"specialization\" + 0.022*\"using\" + 0.022*\"language\" + 0.017*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,553 : INFO : topic #1 (0.500): 0.034*\"instructor\" + 0.034*\"learning\" + 0.027*\"specialization\" + 0.025*\"helped\" + 0.024*\"deep\" + 0.016*\"model\" + 0.015*\"designed\" + 0.015*\"build\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,554 : INFO : topic diff=0.078794, rho=0.447214\n",
      "2022-02-24 12:21:46,568 : INFO : -4.942 per-word bound, 30.7 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,570 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:46,576 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"logistic\" + 0.022*\"specialization\" + 0.022*\"using\" + 0.022*\"language\" + 0.017*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,577 : INFO : topic #1 (0.500): 0.035*\"instructor\" + 0.035*\"learning\" + 0.027*\"specialization\" + 0.025*\"helped\" + 0.025*\"deep\" + 0.016*\"model\" + 0.015*\"designed\" + 0.015*\"build\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,578 : INFO : topic diff=0.045246, rho=0.408248\n",
      "2022-02-24 12:21:46,591 : INFO : -4.938 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,591 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:46,600 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"logistic\" + 0.023*\"specialization\" + 0.023*\"using\" + 0.022*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,603 : INFO : topic #1 (0.500): 0.035*\"instructor\" + 0.035*\"learning\" + 0.026*\"specialization\" + 0.025*\"helped\" + 0.025*\"deep\" + 0.016*\"model\" + 0.016*\"designed\" + 0.015*\"build\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,604 : INFO : topic diff=0.026955, rho=0.377964\n",
      "2022-02-24 12:21:46,614 : INFO : -4.936 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,616 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:46,624 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"logistic\" + 0.023*\"specialization\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,625 : INFO : topic #1 (0.500): 0.036*\"instructor\" + 0.036*\"learning\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.025*\"deep\" + 0.016*\"model\" + 0.016*\"designed\" + 0.016*\"build\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,626 : INFO : topic diff=0.016568, rho=0.353553\n",
      "2022-02-24 12:21:46,635 : INFO : -4.935 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,635 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:46,641 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"logistic\" + 0.023*\"specialization\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,642 : INFO : topic #1 (0.500): 0.036*\"learning\" + 0.036*\"instructor\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.026*\"deep\" + 0.016*\"model\" + 0.016*\"designed\" + 0.016*\"build\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,642 : INFO : topic diff=0.010476, rho=0.333333\n",
      "2022-02-24 12:21:46,651 : INFO : -4.935 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,652 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:46,657 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"specialization\" + 0.023*\"logistic\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,659 : INFO : topic #1 (0.500): 0.036*\"learning\" + 0.036*\"instructor\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.026*\"deep\" + 0.016*\"build\" + 0.016*\"designed\" + 0.016*\"model\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,660 : INFO : topic diff=0.006796, rho=0.316228\n",
      "2022-02-24 12:21:46,668 : INFO : -4.935 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,669 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:46,675 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"specialization\" + 0.023*\"logistic\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,676 : INFO : topic #1 (0.500): 0.036*\"learning\" + 0.036*\"instructor\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.026*\"deep\" + 0.016*\"build\" + 0.016*\"designed\" + 0.016*\"model\" + 0.015*\"straight\" + 0.015*\"detailed\"\n",
      "2022-02-24 12:21:46,677 : INFO : topic diff=0.004513, rho=0.301511\n",
      "2022-02-24 12:21:46,680 : INFO : topic #0 (0.500): 0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"specialization\" + 0.023*\"logistic\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,681 : INFO : topic #1 (0.500): 0.036*\"learning\" + 0.036*\"instructor\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.026*\"deep\" + 0.016*\"build\" + 0.016*\"designed\" + 0.016*\"model\" + 0.015*\"straight\" + 0.015*\"detailed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.042*\"course\" + 0.029*\"analysis\" + 0.029*\"vector\" + 0.029*\"word\" + 0.023*\"specialization\" + 0.023*\"logistic\" + 0.023*\"using\" + 0.023*\"language\" + 0.016*\"aug\" + 0.016*\"localitysensitive\"'),\n",
       " (1,\n",
       "  '0.036*\"learning\" + 0.036*\"instructor\" + 0.026*\"specialization\" + 0.026*\"helped\" + 0.026*\"deep\" + 0.016*\"build\" + 0.016*\"designed\" + 0.016*\"model\" + 0.015*\"straight\" + 0.015*\"detailed\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fcf995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:46,694 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2022-02-24 12:21:46,696 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2022-02-24 12:21:46,697 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:46,698 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:46,733 : INFO : -6.115 per-word bound, 69.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,734 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:46,751 : INFO : topic #0 (0.333): 0.037*\"course\" + 0.026*\"aug\" + 0.023*\"build\" + 0.020*\"analysis\" + 0.019*\"helped\" + 0.018*\"vector\" + 0.018*\"specialization\" + 0.017*\"instructor\" + 0.017*\"learning\" + 0.017*\"deep\"\n",
      "2022-02-24 12:21:46,752 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.026*\"vector\" + 0.024*\"language\" + 0.024*\"logistic\" + 0.023*\"instructor\" + 0.023*\"course\" + 0.022*\"word\" + 0.022*\"specialization\" + 0.019*\"using\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,753 : INFO : topic #2 (0.333): 0.033*\"specialization\" + 0.031*\"learning\" + 0.025*\"course\" + 0.022*\"word\" + 0.019*\"using\" + 0.019*\"designed\" + 0.018*\"machine\" + 0.018*\"language\" + 0.018*\"taught\" + 0.018*\"expert\"\n",
      "2022-02-24 12:21:46,754 : INFO : topic diff=1.205493, rho=1.000000\n",
      "2022-02-24 12:21:46,764 : INFO : -5.289 per-word bound, 39.1 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,765 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:46,771 : INFO : topic #0 (0.333): 0.042*\"course\" + 0.029*\"aug\" + 0.020*\"build\" + 0.019*\"analysis\" + 0.018*\"helped\" + 0.018*\"vector\" + 0.018*\"specialization\" + 0.018*\"instructor\" + 0.018*\"learning\" + 0.018*\"deep\"\n",
      "2022-02-24 12:21:46,772 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.026*\"language\" + 0.026*\"logistic\" + 0.026*\"course\" + 0.025*\"word\" + 0.025*\"specialization\" + 0.024*\"using\" + 0.021*\"instructor\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,772 : INFO : topic #2 (0.333): 0.037*\"learning\" + 0.029*\"specialization\" + 0.022*\"designed\" + 0.021*\"taught\" + 0.021*\"expert\" + 0.021*\"machine\" + 0.021*\"deep\" + 0.021*\"coauthor\" + 0.021*\"kaiser\" + 0.021*\"google\"\n",
      "2022-02-24 12:21:46,773 : INFO : topic diff=0.265677, rho=0.577350\n",
      "2022-02-24 12:21:46,784 : INFO : -5.167 per-word bound, 35.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,785 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:46,793 : INFO : topic #0 (0.333): 0.043*\"course\" + 0.030*\"aug\" + 0.019*\"build\" + 0.018*\"analysis\" + 0.018*\"helped\" + 0.018*\"vector\" + 0.018*\"instructor\" + 0.018*\"specialization\" + 0.018*\"deep\" + 0.018*\"ai\"\n",
      "2022-02-24 12:21:46,795 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.026*\"course\" + 0.026*\"word\" + 0.026*\"specialization\" + 0.026*\"using\" + 0.020*\"instructor\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,796 : INFO : topic #2 (0.333): 0.040*\"learning\" + 0.027*\"specialization\" + 0.023*\"designed\" + 0.023*\"taught\" + 0.023*\"expert\" + 0.023*\"deep\" + 0.023*\"machine\" + 0.023*\"coauthor\" + 0.023*\"kaiser\" + 0.023*\"google\"\n",
      "2022-02-24 12:21:46,797 : INFO : topic diff=0.132430, rho=0.500000\n",
      "2022-02-24 12:21:46,806 : INFO : -5.136 per-word bound, 35.2 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,807 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:46,814 : INFO : topic #0 (0.333): 0.044*\"course\" + 0.031*\"aug\" + 0.019*\"build\" + 0.018*\"analysis\" + 0.018*\"helped\" + 0.018*\"instructor\" + 0.018*\"vector\" + 0.018*\"specialization\" + 0.018*\"deep\" + 0.018*\"ai\"\n",
      "2022-02-24 12:21:46,815 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"word\" + 0.027*\"specialization\" + 0.027*\"using\" + 0.019*\"instructor\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,817 : INFO : topic #2 (0.333): 0.042*\"learning\" + 0.026*\"specialization\" + 0.024*\"designed\" + 0.024*\"taught\" + 0.024*\"deep\" + 0.024*\"expert\" + 0.024*\"machine\" + 0.024*\"coauthor\" + 0.024*\"kaiser\" + 0.024*\"google\"\n",
      "2022-02-24 12:21:46,819 : INFO : topic diff=0.072529, rho=0.447214\n",
      "2022-02-24 12:21:46,829 : INFO : -5.125 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,831 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:46,838 : INFO : topic #0 (0.333): 0.044*\"course\" + 0.031*\"aug\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"instructor\" + 0.018*\"vector\" + 0.018*\"deep\" + 0.018*\"specialization\" + 0.018*\"ai\"\n",
      "2022-02-24 12:21:46,839 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"specialization\" + 0.027*\"word\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"instructor\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,840 : INFO : topic #2 (0.333): 0.043*\"learning\" + 0.026*\"specialization\" + 0.024*\"designed\" + 0.024*\"deep\" + 0.024*\"taught\" + 0.024*\"expert\" + 0.024*\"coauthor\" + 0.024*\"kaiser\" + 0.024*\"machine\" + 0.024*\"google\"\n",
      "2022-02-24 12:21:46,841 : INFO : topic diff=0.041480, rho=0.408248\n",
      "2022-02-24 12:21:46,853 : INFO : -5.122 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,854 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:46,860 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"build\" + 0.018*\"instructor\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"vector\" + 0.018*\"machine\" + 0.018*\"ai\" + 0.018*\"stanford\"\n",
      "2022-02-24 12:21:46,861 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"specialization\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"word\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"instructor\" + 0.019*\"text\"\n",
      "2022-02-24 12:21:46,862 : INFO : topic #2 (0.333): 0.043*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n",
      "2022-02-24 12:21:46,863 : INFO : topic diff=0.024551, rho=0.377964\n",
      "2022-02-24 12:21:46,874 : INFO : -5.120 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,875 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:46,881 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"build\" + 0.018*\"instructor\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"vector\" + 0.018*\"machine\" + 0.018*\"ai\" + 0.018*\"stanford\"\n",
      "2022-02-24 12:21:46,885 : INFO : topic #1 (0.333): 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"specialization\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"word\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"\n",
      "2022-02-24 12:21:46,886 : INFO : topic #2 (0.333): 0.043*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n",
      "2022-02-24 12:21:46,888 : INFO : topic diff=0.014994, rho=0.353553\n",
      "2022-02-24 12:21:46,896 : INFO : -5.120 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,897 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:46,904 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"instructor\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"vector\" + 0.018*\"machine\" + 0.018*\"ai\" + 0.018*\"stanford\"\n",
      "2022-02-24 12:21:46,906 : INFO : topic #1 (0.333): 0.027*\"specialization\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"language\" + 0.027*\"word\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"\n",
      "2022-02-24 12:21:46,906 : INFO : topic #2 (0.333): 0.044*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n",
      "2022-02-24 12:21:46,907 : INFO : topic diff=0.009426, rho=0.333333\n",
      "2022-02-24 12:21:46,916 : INFO : -5.120 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:46,918 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:46,925 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"instructor\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"machine\" + 0.018*\"vector\" + 0.018*\"ai\" + 0.018*\"stanford\"\n",
      "2022-02-24 12:21:46,925 : INFO : topic #1 (0.333): 0.027*\"specialization\" + 0.027*\"vector\" + 0.027*\"analysis\" + 0.027*\"word\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"\n",
      "2022-02-24 12:21:46,926 : INFO : topic #2 (0.333): 0.044*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n",
      "2022-02-24 12:21:46,927 : INFO : topic diff=0.006084, rho=0.316228\n",
      "2022-02-24 12:21:46,936 : INFO : -5.120 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:46,937 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:46,942 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"instructor\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"machine\" + 0.018*\"vector\" + 0.018*\"translation\" + 0.018*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,943 : INFO : topic #1 (0.333): 0.027*\"specialization\" + 0.027*\"vector\" + 0.027*\"analysis\" + 0.027*\"word\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"\n",
      "2022-02-24 12:21:46,945 : INFO : topic #2 (0.333): 0.044*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n",
      "2022-02-24 12:21:46,946 : INFO : topic diff=0.004021, rho=0.301511\n",
      "2022-02-24 12:21:46,948 : INFO : topic #0 (0.333): 0.045*\"course\" + 0.031*\"aug\" + 0.018*\"instructor\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"machine\" + 0.018*\"vector\" + 0.018*\"translation\" + 0.018*\"localitysensitive\"\n",
      "2022-02-24 12:21:46,950 : INFO : topic #1 (0.333): 0.027*\"specialization\" + 0.027*\"vector\" + 0.027*\"analysis\" + 0.027*\"word\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"\n",
      "2022-02-24 12:21:46,951 : INFO : topic #2 (0.333): 0.044*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.045*\"course\" + 0.031*\"aug\" + 0.018*\"instructor\" + 0.018*\"build\" + 0.018*\"helped\" + 0.018*\"analysis\" + 0.018*\"machine\" + 0.018*\"vector\" + 0.018*\"translation\" + 0.018*\"localitysensitive\"'),\n",
       " (1,\n",
       "  '0.027*\"specialization\" + 0.027*\"vector\" + 0.027*\"analysis\" + 0.027*\"word\" + 0.027*\"language\" + 0.027*\"logistic\" + 0.027*\"course\" + 0.027*\"using\" + 0.019*\"text\" + 0.019*\"model\"'),\n",
       " (2,\n",
       "  '0.044*\"learning\" + 0.025*\"specialization\" + 0.025*\"designed\" + 0.025*\"deep\" + 0.025*\"taught\" + 0.025*\"expert\" + 0.025*\"coauthor\" + 0.025*\"kaiser\" + 0.025*\"google\" + 0.025*\"staff\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fe173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:46,977 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 12:21:46,979 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 12:21:46,980 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:46,981 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:47,001 : INFO : -6.846 per-word bound, 115.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,002 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:47,015 : INFO : topic #0 (0.250): 0.034*\"language\" + 0.031*\"analysis\" + 0.031*\"specialization\" + 0.026*\"perform\" + 0.026*\"course\" + 0.025*\"vector\" + 0.025*\"word\" + 0.024*\"using\" + 0.023*\"text\" + 0.023*\"natural\"\n",
      "2022-02-24 12:21:47,016 : INFO : topic #1 (0.250): 0.032*\"informative\" + 0.031*\"fun\" + 0.029*\"course\" + 0.025*\"aug\" + 0.019*\"embeddings\" + 0.019*\"translation\" + 0.018*\"localitysensitive\" + 0.018*\"machine\" + 0.017*\"models\" + 0.017*\"vector\"\n",
      "2022-02-24 12:21:47,018 : INFO : topic #2 (0.250): 0.055*\"learning\" + 0.055*\"instructor\" + 0.040*\"specialization\" + 0.038*\"deep\" + 0.025*\"machine\" + 0.023*\"designed\" + 0.021*\"expert\" + 0.021*\"taught\" + 0.021*\"developer\" + 0.021*\"build\"\n",
      "2022-02-24 12:21:47,019 : INFO : topic #3 (0.250): 0.034*\"course\" + 0.026*\"word\" + 0.025*\"logistic\" + 0.025*\"vector\" + 0.022*\"using\" + 0.020*\"model\" + 0.019*\"analysis\" + 0.017*\"tweet\" + 0.016*\"specialization\" + 0.015*\"use\"\n",
      "2022-02-24 12:21:47,020 : INFO : topic diff=1.807584, rho=1.000000\n",
      "2022-02-24 12:21:47,029 : INFO : -5.436 per-word bound, 43.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,031 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:47,039 : INFO : topic #0 (0.250): 0.037*\"language\" + 0.036*\"specialization\" + 0.034*\"course\" + 0.034*\"word\" + 0.029*\"analysis\" + 0.027*\"perform\" + 0.026*\"vector\" + 0.026*\"using\" + 0.025*\"natural\" + 0.024*\"use\"\n",
      "2022-02-24 12:21:47,040 : INFO : topic #1 (0.250): 0.049*\"course\" + 0.035*\"aug\" + 0.026*\"informative\" + 0.025*\"fun\" + 0.021*\"embeddings\" + 0.021*\"translation\" + 0.021*\"localitysensitive\" + 0.021*\"machine\" + 0.021*\"models\" + 0.021*\"attented\"\n",
      "2022-02-24 12:21:47,042 : INFO : topic #2 (0.250): 0.061*\"learning\" + 0.061*\"instructor\" + 0.043*\"specialization\" + 0.042*\"deep\" + 0.025*\"machine\" + 0.024*\"designed\" + 0.023*\"expert\" + 0.023*\"taught\" + 0.023*\"developer\" + 0.023*\"build\"\n",
      "2022-02-24 12:21:47,045 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.023*\"vector\" + 0.021*\"using\" + 0.020*\"model\" + 0.020*\"analysis\" + 0.020*\"course\" + 0.019*\"tweet\" + 0.017*\"youtube\" + 0.017*\"helped\" + 0.017*\"library\"\n",
      "2022-02-24 12:21:47,046 : INFO : topic diff=0.367607, rho=0.577350\n",
      "2022-02-24 12:21:47,059 : INFO : -5.182 per-word bound, 36.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,061 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:47,072 : INFO : topic #0 (0.250): 0.038*\"language\" + 0.037*\"specialization\" + 0.037*\"course\" + 0.036*\"word\" + 0.028*\"analysis\" + 0.027*\"perform\" + 0.027*\"vector\" + 0.026*\"using\" + 0.026*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 12:21:47,073 : INFO : topic #1 (0.250): 0.054*\"course\" + 0.038*\"aug\" + 0.024*\"informative\" + 0.024*\"fun\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"localitysensitive\" + 0.022*\"machine\" + 0.022*\"models\" + 0.022*\"attented\"\n",
      "2022-02-24 12:21:47,074 : INFO : topic #2 (0.250): 0.063*\"learning\" + 0.063*\"instructor\" + 0.044*\"specialization\" + 0.044*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.024*\"expert\" + 0.024*\"taught\" + 0.024*\"build\" + 0.024*\"developer\"\n",
      "2022-02-24 12:21:47,076 : INFO : topic #3 (0.250): 0.034*\"logistic\" + 0.022*\"vector\" + 0.021*\"using\" + 0.020*\"model\" + 0.020*\"analysis\" + 0.020*\"tweet\" + 0.019*\"youtube\" + 0.019*\"library\" + 0.019*\"tensorflow\" + 0.019*\"trax\"\n",
      "2022-02-24 12:21:47,078 : INFO : topic diff=0.180380, rho=0.500000\n",
      "2022-02-24 12:21:47,087 : INFO : -5.119 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,088 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:47,094 : INFO : topic #0 (0.250): 0.038*\"language\" + 0.038*\"specialization\" + 0.038*\"course\" + 0.038*\"word\" + 0.027*\"analysis\" + 0.027*\"perform\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"natural\" + 0.026*\"use\"\n",
      "2022-02-24 12:21:47,095 : INFO : topic #1 (0.250): 0.056*\"course\" + 0.039*\"aug\" + 0.023*\"informative\" + 0.023*\"fun\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"machine\" + 0.022*\"localitysensitive\" + 0.022*\"models\" + 0.022*\"attented\"\n",
      "2022-02-24 12:21:47,096 : INFO : topic #2 (0.250): 0.064*\"learning\" + 0.064*\"instructor\" + 0.045*\"specialization\" + 0.044*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"expert\" + 0.025*\"taught\" + 0.025*\"build\" + 0.025*\"helped\"\n",
      "2022-02-24 12:21:47,097 : INFO : topic #3 (0.250): 0.035*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.020*\"model\" + 0.020*\"analysis\" + 0.020*\"tweet\" + 0.020*\"youtube\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,099 : INFO : topic diff=0.103099, rho=0.447214\n",
      "2022-02-24 12:21:47,108 : INFO : -5.098 per-word bound, 34.2 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,109 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:47,116 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.038*\"specialization\" + 0.038*\"course\" + 0.038*\"word\" + 0.027*\"analysis\" + 0.027*\"perform\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"natural\" + 0.026*\"use\"\n",
      "2022-02-24 12:21:47,118 : INFO : topic #1 (0.250): 0.057*\"course\" + 0.040*\"aug\" + 0.023*\"informative\" + 0.023*\"fun\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"machine\" + 0.022*\"localitysensitive\" + 0.022*\"models\" + 0.022*\"attented\"\n",
      "2022-02-24 12:21:47,118 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"build\" + 0.025*\"helped\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,119 : INFO : topic #3 (0.250): 0.036*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.020*\"model\" + 0.020*\"analysis\" + 0.020*\"tweet\" + 0.020*\"youtube\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,120 : INFO : topic diff=0.061296, rho=0.408248\n",
      "2022-02-24 12:21:47,129 : INFO : -5.090 per-word bound, 34.1 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,130 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:47,137 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.038*\"course\" + 0.038*\"word\" + 0.027*\"analysis\" + 0.027*\"perform\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,138 : INFO : topic #1 (0.250): 0.057*\"course\" + 0.040*\"aug\" + 0.023*\"informative\" + 0.023*\"fun\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"machine\" + 0.022*\"localitysensitive\" + 0.022*\"analysis\" + 0.022*\"models\"\n",
      "2022-02-24 12:21:47,139 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,140 : INFO : topic #3 (0.250): 0.036*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.020*\"analysis\" + 0.020*\"tweet\" + 0.020*\"youtube\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,141 : INFO : topic diff=0.037390, rho=0.377964\n",
      "2022-02-24 12:21:47,151 : INFO : -5.086 per-word bound, 34.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,152 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:47,161 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"course\" + 0.039*\"word\" + 0.027*\"analysis\" + 0.027*\"perform\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,162 : INFO : topic #1 (0.250): 0.058*\"course\" + 0.040*\"aug\" + 0.023*\"informative\" + 0.023*\"fun\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"machine\" + 0.022*\"localitysensitive\" + 0.022*\"analysis\" + 0.022*\"vector\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:47,163 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,164 : INFO : topic #3 (0.250): 0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.020*\"analysis\" + 0.020*\"tweet\" + 0.020*\"library\" + 0.020*\"youtube\" + 0.020*\"tensorflow\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,165 : INFO : topic diff=0.023351, rho=0.353553\n",
      "2022-02-24 12:21:47,174 : INFO : -5.085 per-word bound, 33.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,175 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:47,187 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"course\" + 0.039*\"word\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"perform\" + 0.027*\"using\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,188 : INFO : topic #1 (0.250): 0.058*\"course\" + 0.040*\"aug\" + 0.023*\"informative\" + 0.023*\"fun\" + 0.022*\"embeddings\" + 0.022*\"machine\" + 0.022*\"translation\" + 0.022*\"localitysensitive\" + 0.022*\"analysis\" + 0.022*\"vector\"\n",
      "2022-02-24 12:21:47,189 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,189 : INFO : topic #3 (0.250): 0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.021*\"analysis\" + 0.020*\"tweet\" + 0.020*\"library\" + 0.020*\"youtube\" + 0.020*\"tensorflow\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,190 : INFO : topic diff=0.014918, rho=0.333333\n",
      "2022-02-24 12:21:47,199 : INFO : -5.084 per-word bound, 33.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,200 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:47,206 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"word\" + 0.039*\"course\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"perform\" + 0.027*\"using\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,207 : INFO : topic #1 (0.250): 0.058*\"course\" + 0.040*\"aug\" + 0.022*\"informative\" + 0.022*\"fun\" + 0.022*\"machine\" + 0.022*\"embeddings\" + 0.022*\"translation\" + 0.022*\"analysis\" + 0.022*\"localitysensitive\" + 0.022*\"vector\"\n",
      "2022-02-24 12:21:47,207 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"machine\" + 0.025*\"designed\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,208 : INFO : topic #3 (0.250): 0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.021*\"analysis\" + 0.021*\"tweet\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"youtube\" + 0.020*\"trax\"\n",
      "2022-02-24 12:21:47,209 : INFO : topic diff=0.009738, rho=0.316228\n",
      "2022-02-24 12:21:47,218 : INFO : -5.084 per-word bound, 33.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 12:21:47,219 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:47,225 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"word\" + 0.039*\"course\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"perform\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,226 : INFO : topic #1 (0.250): 0.058*\"course\" + 0.040*\"aug\" + 0.022*\"informative\" + 0.022*\"fun\" + 0.022*\"machine\" + 0.022*\"embeddings\" + 0.022*\"analysis\" + 0.022*\"translation\" + 0.022*\"localitysensitive\" + 0.022*\"vector\"\n",
      "2022-02-24 12:21:47,227 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"designed\" + 0.025*\"machine\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,228 : INFO : topic #3 (0.250): 0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.021*\"analysis\" + 0.021*\"tweet\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\" + 0.020*\"youtube\"\n",
      "2022-02-24 12:21:47,229 : INFO : topic diff=0.006486, rho=0.301511\n",
      "2022-02-24 12:21:47,230 : INFO : topic #0 (0.250): 0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"word\" + 0.039*\"course\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"perform\" + 0.027*\"natural\" + 0.027*\"use\"\n",
      "2022-02-24 12:21:47,232 : INFO : topic #1 (0.250): 0.058*\"course\" + 0.040*\"aug\" + 0.022*\"informative\" + 0.022*\"fun\" + 0.022*\"machine\" + 0.022*\"embeddings\" + 0.022*\"analysis\" + 0.022*\"translation\" + 0.022*\"localitysensitive\" + 0.022*\"vector\"\n",
      "2022-02-24 12:21:47,234 : INFO : topic #2 (0.250): 0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"designed\" + 0.025*\"machine\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"\n",
      "2022-02-24 12:21:47,235 : INFO : topic #3 (0.250): 0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.021*\"analysis\" + 0.021*\"tweet\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\" + 0.020*\"youtube\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.039*\"language\" + 0.039*\"specialization\" + 0.039*\"word\" + 0.039*\"course\" + 0.027*\"analysis\" + 0.027*\"vector\" + 0.027*\"using\" + 0.027*\"perform\" + 0.027*\"natural\" + 0.027*\"use\"'),\n",
       " (1,\n",
       "  '0.058*\"course\" + 0.040*\"aug\" + 0.022*\"informative\" + 0.022*\"fun\" + 0.022*\"machine\" + 0.022*\"embeddings\" + 0.022*\"analysis\" + 0.022*\"translation\" + 0.022*\"localitysensitive\" + 0.022*\"vector\"'),\n",
       " (2,\n",
       "  '0.065*\"learning\" + 0.065*\"instructor\" + 0.045*\"specialization\" + 0.045*\"deep\" + 0.025*\"designed\" + 0.025*\"machine\" + 0.025*\"helped\" + 0.025*\"build\" + 0.025*\"expert\" + 0.025*\"taught\"'),\n",
       " (3,\n",
       "  '0.037*\"logistic\" + 0.021*\"vector\" + 0.021*\"using\" + 0.021*\"model\" + 0.021*\"analysis\" + 0.021*\"tweet\" + 0.020*\"library\" + 0.020*\"tensorflow\" + 0.020*\"trax\" + 0.020*\"youtube\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b6dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77935955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  locality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0     rating  â â  students enrolled course   nat...\n",
       "1  by end specialization  designed nlp applicatio...\n",
       "2  this specialization designed taught two expert...\n",
       "3  younes bensouda mourri instructor ai stanford ...\n",
       "4  åukasz kaiser staff research scientist google...\n",
       "5  machine translation  word embeddings  locality...\n",
       "6  the lecture exciting detailed  though little h...\n",
       "7                          other  i informative fun \n",
       "8  from lesson sentiment analysis logistic regres...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b8af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â students course language processing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>specialization application perform sentiment a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes instructor ai stanford university learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture straight regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentiment analysis regression learn feature ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â students course language processing s...\n",
       "1  specialization application perform sentiment a...\n",
       "2                    specialization machine learning\n",
       "3  younes instructor ai stanford university learn...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings sentiment ...\n",
       "6                  lecture straight regression model\n",
       "7                                                fun\n",
       "8  sentiment analysis regression learn feature ve...\n",
       "9         instructor instructor curriculum developer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009ca2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>brain</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>coauthor</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  assignment  aug  awesome  bayes  brain  chatbot  \\\n",
       "0   0         1            0           0    0        0      1      0        0   \n",
       "1   0         1            1           0    0        0      0      0        1   \n",
       "2   0         0            0           0    0        0      0      0        0   \n",
       "3   1         0            0           0    0        0      0      0        0   \n",
       "4   0         0            0           0    0        0      0      1        0   \n",
       "5   0         1            0           1    1        1      0      0        0   \n",
       "6   0         0            0           0    0        0      0      0        0   \n",
       "7   0         0            0           0    0        0      0      0        0   \n",
       "8   0         1            0           0    0        0      0      0        0   \n",
       "9   0         0            0           0    0        0      0      0        0   \n",
       "\n",
       "   coauthor  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0         0  ...      1      0           0    2       2      1          1   \n",
       "1         0  ...      0      0           0    0       0      0          0   \n",
       "2         0  ...      0      0           0    0       0      0          0   \n",
       "3         0  ...      0      0           1    0       0      0          0   \n",
       "4         1  ...      0      1           0    0       0      0          0   \n",
       "5         0  ...      0      0           0    0       1      0          0   \n",
       "6         0  ...      0      0           0    0       0      0          0   \n",
       "7         0  ...      0      0           0    0       0      0          0   \n",
       "8         0  ...      1      0           0    0       1      0          0   \n",
       "9         0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e9ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6639ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,011 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-24 12:21:49,014 : INFO : using symmetric eta at 0.5\n",
      "2022-02-24 12:21:49,016 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:49,018 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:49,047 : INFO : -5.007 per-word bound, 32.2 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,048 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:49,070 : INFO : topic #0 (0.500): 0.041*\"regression\" + 0.038*\"specialization\" + 0.035*\"course\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.033*\"instructor\" + 0.025*\"vector\" + 0.023*\"language\" + 0.022*\"word\" + 0.021*\"perform\"\n",
      "2022-02-24 12:21:49,071 : INFO : topic #1 (0.500): 0.047*\"course\" + 0.037*\"word\" + 0.033*\"vector\" + 0.031*\"specialization\" + 0.030*\"space\" + 0.023*\"sentiment\" + 0.023*\"processing\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"translation\"\n",
      "2022-02-24 12:21:49,072 : INFO : topic diff=0.525132, rho=1.000000\n",
      "2022-02-24 12:21:49,093 : INFO : -4.688 per-word bound, 25.8 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,094 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:49,111 : INFO : topic #0 (0.500): 0.043*\"regression\" + 0.040*\"specialization\" + 0.039*\"instructor\" + 0.033*\"analysis\" + 0.033*\"sentiment\" + 0.024*\"learning\" + 0.022*\"vector\" + 0.021*\"language\" + 0.020*\"perform\" + 0.020*\"course\"\n",
      "2022-02-24 12:21:49,113 : INFO : topic #1 (0.500): 0.060*\"course\" + 0.043*\"word\" + 0.036*\"vector\" + 0.034*\"space\" + 0.029*\"specialization\" + 0.025*\"sentiment\" + 0.025*\"processing\" + 0.025*\"language\" + 0.025*\"analysis\" + 0.025*\"translation\"\n",
      "2022-02-24 12:21:49,114 : INFO : topic diff=0.310636, rho=0.577350\n",
      "2022-02-24 12:21:49,128 : INFO : -4.551 per-word bound, 23.4 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,129 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:49,137 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.042*\"specialization\" + 0.041*\"instructor\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.027*\"learning\" + 0.020*\"vector\" + 0.020*\"language\" + 0.020*\"perform\" + 0.019*\"tweet\"\n",
      "2022-02-24 12:21:49,139 : INFO : topic #1 (0.500): 0.064*\"course\" + 0.046*\"word\" + 0.037*\"vector\" + 0.036*\"space\" + 0.029*\"specialization\" + 0.026*\"sentiment\" + 0.026*\"language\" + 0.026*\"analysis\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,140 : INFO : topic diff=0.159789, rho=0.500000\n",
      "2022-02-24 12:21:49,151 : INFO : -4.514 per-word bound, 22.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,153 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:49,162 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.043*\"instructor\" + 0.042*\"specialization\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.029*\"learning\" + 0.019*\"language\" + 0.019*\"vector\" + 0.019*\"perform\" + 0.019*\"tweet\"\n",
      "2022-02-24 12:21:49,164 : INFO : topic #1 (0.500): 0.066*\"course\" + 0.047*\"word\" + 0.037*\"vector\" + 0.036*\"space\" + 0.028*\"specialization\" + 0.026*\"sentiment\" + 0.026*\"language\" + 0.026*\"analysis\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,166 : INFO : topic diff=0.087009, rho=0.447214\n",
      "2022-02-24 12:21:49,177 : INFO : -4.503 per-word bound, 22.7 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,178 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:49,186 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.043*\"instructor\" + 0.043*\"specialization\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.030*\"learning\" + 0.019*\"language\" + 0.019*\"perform\" + 0.019*\"tweet\" + 0.019*\"vector\"\n",
      "2022-02-24 12:21:49,187 : INFO : topic #1 (0.500): 0.067*\"course\" + 0.047*\"word\" + 0.037*\"vector\" + 0.037*\"space\" + 0.028*\"specialization\" + 0.027*\"sentiment\" + 0.026*\"language\" + 0.026*\"analysis\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,188 : INFO : topic diff=0.050179, rho=0.408248\n",
      "2022-02-24 12:21:49,197 : INFO : -4.498 per-word bound, 22.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,198 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:49,205 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.044*\"instructor\" + 0.043*\"specialization\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.031*\"learning\" + 0.019*\"perform\" + 0.019*\"language\" + 0.019*\"text\" + 0.019*\"translate\"\n",
      "2022-02-24 12:21:49,206 : INFO : topic #1 (0.500): 0.068*\"course\" + 0.047*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,207 : INFO : topic diff=0.030180, rho=0.377964\n",
      "2022-02-24 12:21:49,220 : INFO : -4.497 per-word bound, 22.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,221 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:49,231 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.044*\"instructor\" + 0.043*\"specialization\" + 0.032*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"learning\" + 0.019*\"perform\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\"\n",
      "2022-02-24 12:21:49,233 : INFO : topic #1 (0.500): 0.068*\"course\" + 0.047*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,235 : INFO : topic diff=0.018789, rho=0.353553\n",
      "2022-02-24 12:21:49,243 : INFO : -4.496 per-word bound, 22.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,244 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:49,251 : INFO : topic #0 (0.500): 0.044*\"regression\" + 0.044*\"instructor\" + 0.044*\"specialization\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"learning\" + 0.019*\"perform\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\"\n",
      "2022-02-24 12:21:49,252 : INFO : topic #1 (0.500): 0.068*\"course\" + 0.047*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,252 : INFO : topic diff=0.012055, rho=0.333333\n",
      "2022-02-24 12:21:49,264 : INFO : -4.496 per-word bound, 22.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,265 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:49,271 : INFO : topic #0 (0.500): 0.044*\"instructor\" + 0.044*\"regression\" + 0.044*\"specialization\" + 0.031*\"learning\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\" + 0.019*\"tool\"\n",
      "2022-02-24 12:21:49,272 : INFO : topic #1 (0.500): 0.069*\"course\" + 0.048*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,273 : INFO : topic diff=0.007942, rho=0.316228\n",
      "2022-02-24 12:21:49,282 : INFO : -4.495 per-word bound, 22.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,283 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:49,291 : INFO : topic #0 (0.500): 0.044*\"instructor\" + 0.044*\"regression\" + 0.044*\"specialization\" + 0.032*\"learning\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\" + 0.019*\"tool\"\n",
      "2022-02-24 12:21:49,292 : INFO : topic #1 (0.500): 0.069*\"course\" + 0.048*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n",
      "2022-02-24 12:21:49,293 : INFO : topic diff=0.005357, rho=0.301511\n",
      "2022-02-24 12:21:49,295 : INFO : topic #0 (0.500): 0.044*\"instructor\" + 0.044*\"regression\" + 0.044*\"specialization\" + 0.032*\"learning\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\" + 0.019*\"tool\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,299 : INFO : topic #1 (0.500): 0.069*\"course\" + 0.048*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.044*\"instructor\" + 0.044*\"regression\" + 0.044*\"specialization\" + 0.032*\"learning\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.019*\"text\" + 0.019*\"translate\" + 0.019*\"application\" + 0.019*\"tool\"'),\n",
       " (1,\n",
       "  '0.069*\"course\" + 0.048*\"word\" + 0.038*\"vector\" + 0.037*\"space\" + 0.027*\"specialization\" + 0.027*\"sentiment\" + 0.027*\"analysis\" + 0.027*\"language\" + 0.026*\"processing\" + 0.026*\"translation\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1549bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,322 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2022-02-24 12:21:49,324 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2022-02-24 12:21:49,325 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:49,327 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:49,344 : INFO : -5.572 per-word bound, 47.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,345 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:49,372 : INFO : topic #0 (0.333): 0.061*\"instructor\" + 0.050*\"specialization\" + 0.046*\"course\" + 0.043*\"learning\" + 0.033*\"machine\" + 0.025*\"university\" + 0.025*\"ai\" + 0.025*\"younes\" + 0.025*\"stanford\" + 0.024*\"curriculum\"\n",
      "2022-02-24 12:21:49,373 : INFO : topic #1 (0.333): 0.048*\"course\" + 0.047*\"regression\" + 0.040*\"vector\" + 0.039*\"word\" + 0.032*\"sentiment\" + 0.032*\"space\" + 0.032*\"analysis\" + 0.025*\"tweet\" + 0.024*\"language\" + 0.024*\"model\"\n",
      "2022-02-24 12:21:49,375 : INFO : topic #2 (0.333): 0.041*\"specialization\" + 0.041*\"sentiment\" + 0.041*\"analysis\" + 0.041*\"application\" + 0.040*\"chatbot\" + 0.040*\"perform\" + 0.040*\"translate\" + 0.040*\"tool\" + 0.040*\"language\" + 0.040*\"text\"\n",
      "2022-02-24 12:21:49,377 : INFO : topic diff=1.287656, rho=1.000000\n",
      "2022-02-24 12:21:49,387 : INFO : -4.761 per-word bound, 27.1 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,388 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:49,395 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.049*\"specialization\" + 0.046*\"learning\" + 0.043*\"course\" + 0.039*\"machine\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,396 : INFO : topic #1 (0.333): 0.050*\"course\" + 0.047*\"regression\" + 0.042*\"vector\" + 0.041*\"word\" + 0.032*\"sentiment\" + 0.032*\"space\" + 0.032*\"analysis\" + 0.025*\"tweet\" + 0.025*\"language\" + 0.025*\"model\"\n",
      "2022-02-24 12:21:49,396 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.041*\"sentiment\" + 0.041*\"analysis\" + 0.041*\"perform\" + 0.041*\"application\" + 0.041*\"chatbot\" + 0.041*\"translate\" + 0.041*\"tool\" + 0.041*\"language\" + 0.041*\"text\"\n",
      "2022-02-24 12:21:49,398 : INFO : topic diff=0.131218, rho=0.577350\n",
      "2022-02-24 12:21:49,408 : INFO : -4.727 per-word bound, 26.5 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,409 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:49,413 : INFO : topic #0 (0.333): 0.066*\"instructor\" + 0.047*\"specialization\" + 0.046*\"learning\" + 0.043*\"course\" + 0.043*\"machine\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,415 : INFO : topic #1 (0.333): 0.050*\"course\" + 0.047*\"regression\" + 0.043*\"vector\" + 0.042*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.025*\"tweet\" + 0.025*\"language\" + 0.025*\"model\"\n",
      "2022-02-24 12:21:49,417 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.041*\"perform\" + 0.041*\"application\" + 0.041*\"chatbot\" + 0.041*\"translate\" + 0.041*\"tool\" + 0.041*\"language\" + 0.041*\"text\"\n",
      "2022-02-24 12:21:49,417 : INFO : topic diff=0.074697, rho=0.500000\n",
      "2022-02-24 12:21:49,426 : INFO : -4.711 per-word bound, 26.2 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,427 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:49,436 : INFO : topic #0 (0.333): 0.066*\"instructor\" + 0.047*\"specialization\" + 0.046*\"learning\" + 0.044*\"machine\" + 0.043*\"course\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,437 : INFO : topic #1 (0.333): 0.050*\"course\" + 0.048*\"regression\" + 0.044*\"vector\" + 0.043*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"tweet\" + 0.026*\"language\" + 0.025*\"specialization\"\n",
      "2022-02-24 12:21:49,438 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.041*\"application\" + 0.041*\"chatbot\" + 0.041*\"translate\" + 0.041*\"tool\" + 0.041*\"language\" + 0.041*\"text\"\n",
      "2022-02-24 12:21:49,439 : INFO : topic diff=0.046684, rho=0.447214\n",
      "2022-02-24 12:21:49,451 : INFO : -4.704 per-word bound, 26.1 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,452 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:49,458 : INFO : topic #0 (0.333): 0.066*\"instructor\" + 0.046*\"specialization\" + 0.046*\"learning\" + 0.044*\"machine\" + 0.044*\"course\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,459 : INFO : topic #1 (0.333): 0.049*\"course\" + 0.048*\"regression\" + 0.044*\"vector\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"tweet\" + 0.026*\"language\" + 0.026*\"specialization\"\n",
      "2022-02-24 12:21:49,460 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.041*\"text\" + 0.041*\"language\"\n",
      "2022-02-24 12:21:49,460 : INFO : topic diff=0.030490, rho=0.408248\n",
      "2022-02-24 12:21:49,472 : INFO : -4.700 per-word bound, 26.0 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,472 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:49,480 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"learning\" + 0.046*\"specialization\" + 0.045*\"machine\" + 0.044*\"course\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,481 : INFO : topic #1 (0.333): 0.049*\"course\" + 0.048*\"regression\" + 0.045*\"vector\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,484 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n",
      "2022-02-24 12:21:49,485 : INFO : topic diff=0.020415, rho=0.377964\n",
      "2022-02-24 12:21:49,498 : INFO : -4.699 per-word bound, 26.0 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,500 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:49,505 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"learning\" + 0.045*\"specialization\" + 0.045*\"course\" + 0.045*\"machine\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,506 : INFO : topic #1 (0.333): 0.049*\"course\" + 0.048*\"regression\" + 0.045*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,507 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n",
      "2022-02-24 12:21:49,508 : INFO : topic diff=0.014001, rho=0.353553\n",
      "2022-02-24 12:21:49,518 : INFO : -4.698 per-word bound, 25.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,520 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:49,526 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"learning\" + 0.046*\"course\" + 0.045*\"specialization\" + 0.045*\"machine\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,527 : INFO : topic #1 (0.333): 0.048*\"course\" + 0.048*\"regression\" + 0.045*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,528 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,529 : INFO : topic diff=0.009806, rho=0.333333\n",
      "2022-02-24 12:21:49,540 : INFO : -4.697 per-word bound, 25.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,542 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:49,548 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"course\" + 0.045*\"learning\" + 0.045*\"specialization\" + 0.045*\"machine\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,550 : INFO : topic #1 (0.333): 0.048*\"regression\" + 0.048*\"course\" + 0.045*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,551 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n",
      "2022-02-24 12:21:49,552 : INFO : topic diff=0.007003, rho=0.316228\n",
      "2022-02-24 12:21:49,560 : INFO : -4.697 per-word bound, 25.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,561 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:49,573 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"course\" + 0.045*\"learning\" + 0.045*\"machine\" + 0.045*\"specialization\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,574 : INFO : topic #1 (0.333): 0.048*\"regression\" + 0.048*\"course\" + 0.046*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,575 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n",
      "2022-02-24 12:21:49,576 : INFO : topic diff=0.005094, rho=0.301511\n",
      "2022-02-24 12:21:49,579 : INFO : topic #0 (0.333): 0.065*\"instructor\" + 0.046*\"course\" + 0.045*\"learning\" + 0.045*\"machine\" + 0.045*\"specialization\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"\n",
      "2022-02-24 12:21:49,581 : INFO : topic #1 (0.333): 0.048*\"regression\" + 0.048*\"course\" + 0.046*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,582 : INFO : topic #2 (0.333): 0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.065*\"instructor\" + 0.046*\"course\" + 0.045*\"learning\" + 0.045*\"machine\" + 0.045*\"specialization\" + 0.026*\"university\" + 0.026*\"ai\" + 0.026*\"younes\" + 0.026*\"stanford\" + 0.026*\"curriculum\"'),\n",
       " (1,\n",
       "  '0.048*\"regression\" + 0.048*\"course\" + 0.046*\"vector\" + 0.045*\"word\" + 0.033*\"space\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"specialization\" + 0.026*\"language\" + 0.026*\"tweet\"'),\n",
       " (2,\n",
       "  '0.042*\"specialization\" + 0.042*\"sentiment\" + 0.042*\"analysis\" + 0.042*\"perform\" + 0.042*\"application\" + 0.042*\"chatbot\" + 0.042*\"translate\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"language\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32882e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,606 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 12:21:49,608 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 12:21:49,609 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:49,610 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:49,625 : INFO : -6.260 per-word bound, 76.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,626 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:49,645 : INFO : topic #0 (0.250): 0.039*\"instructor\" + 0.037*\"word\" + 0.035*\"course\" + 0.030*\"language\" + 0.028*\"specialization\" + 0.027*\"use\" + 0.026*\"space\" + 0.025*\"relationship\" + 0.025*\"vector\" + 0.023*\"processing\"\n",
      "2022-02-24 12:21:49,647 : INFO : topic #1 (0.250): 0.071*\"regression\" + 0.065*\"course\" + 0.046*\"sentiment\" + 0.045*\"vector\" + 0.044*\"analysis\" + 0.028*\"tweet\" + 0.027*\"model\" + 0.027*\"learn\" + 0.027*\"feature\" + 0.027*\"lecture\"\n",
      "2022-02-24 12:21:49,650 : INFO : topic #2 (0.250): 0.072*\"specialization\" + 0.072*\"learning\" + 0.046*\"machine\" + 0.040*\"instructor\" + 0.040*\"ai\" + 0.040*\"stanford\" + 0.040*\"university\" + 0.040*\"younes\" + 0.038*\"fun\" + 0.021*\"course\"\n",
      "2022-02-24 12:21:49,651 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.044*\"language\" + 0.040*\"sentiment\" + 0.036*\"perform\" + 0.036*\"analysis\" + 0.035*\"course\" + 0.034*\"word\" + 0.029*\"tool\" + 0.029*\"text\" + 0.029*\"processing\"\n",
      "2022-02-24 12:21:49,652 : INFO : topic diff=1.925035, rho=1.000000\n",
      "2022-02-24 12:21:49,665 : INFO : -4.937 per-word bound, 30.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,668 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:49,679 : INFO : topic #0 (0.250): 0.043*\"word\" + 0.042*\"course\" + 0.035*\"instructor\" + 0.031*\"language\" + 0.030*\"specialization\" + 0.030*\"use\" + 0.030*\"space\" + 0.029*\"relationship\" + 0.029*\"vector\" + 0.029*\"processing\"\n",
      "2022-02-24 12:21:49,681 : INFO : topic #1 (0.250): 0.069*\"regression\" + 0.066*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.045*\"analysis\" + 0.027*\"tweet\" + 0.026*\"model\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\"\n",
      "2022-02-24 12:21:49,685 : INFO : topic #2 (0.250): 0.077*\"specialization\" + 0.077*\"learning\" + 0.046*\"machine\" + 0.043*\"instructor\" + 0.043*\"ai\" + 0.043*\"stanford\" + 0.043*\"university\" + 0.043*\"younes\" + 0.042*\"fun\" + 0.014*\"course\"\n",
      "2022-02-24 12:21:49,687 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.046*\"language\" + 0.043*\"sentiment\" + 0.042*\"perform\" + 0.041*\"analysis\" + 0.038*\"tool\" + 0.038*\"text\" + 0.038*\"translate\" + 0.037*\"chatbot\" + 0.037*\"application\"\n",
      "2022-02-24 12:21:49,689 : INFO : topic diff=0.250519, rho=0.577350\n",
      "2022-02-24 12:21:49,703 : INFO : -4.832 per-word bound, 28.5 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,704 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:49,718 : INFO : topic #0 (0.250): 0.045*\"word\" + 0.044*\"course\" + 0.033*\"instructor\" + 0.032*\"language\" + 0.031*\"specialization\" + 0.031*\"use\" + 0.031*\"space\" + 0.031*\"relationship\" + 0.031*\"vector\" + 0.030*\"processing\"\n",
      "2022-02-24 12:21:49,720 : INFO : topic #1 (0.250): 0.068*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"tweet\" + 0.026*\"model\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\"\n",
      "2022-02-24 12:21:49,721 : INFO : topic #2 (0.250): 0.079*\"specialization\" + 0.079*\"learning\" + 0.046*\"machine\" + 0.044*\"instructor\" + 0.044*\"ai\" + 0.044*\"stanford\" + 0.044*\"university\" + 0.044*\"younes\" + 0.044*\"fun\" + 0.012*\"course\"\n",
      "2022-02-24 12:21:49,723 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.046*\"language\" + 0.045*\"sentiment\" + 0.044*\"perform\" + 0.044*\"analysis\" + 0.042*\"tool\" + 0.042*\"text\" + 0.042*\"translate\" + 0.042*\"chatbot\" + 0.042*\"application\"\n",
      "2022-02-24 12:21:49,724 : INFO : topic diff=0.129914, rho=0.500000\n",
      "2022-02-24 12:21:49,743 : INFO : -4.802 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,745 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:49,757 : INFO : topic #0 (0.250): 0.045*\"word\" + 0.045*\"course\" + 0.033*\"instructor\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.031*\"use\" + 0.031*\"space\" + 0.031*\"relationship\" + 0.031*\"vector\" + 0.031*\"processing\"\n",
      "2022-02-24 12:21:49,758 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"tweet\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"model\" + 0.026*\"lecture\"\n",
      "2022-02-24 12:21:49,760 : INFO : topic #2 (0.250): 0.080*\"learning\" + 0.080*\"specialization\" + 0.045*\"machine\" + 0.045*\"instructor\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.044*\"fun\" + 0.011*\"course\"\n",
      "2022-02-24 12:21:49,762 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.046*\"sentiment\" + 0.045*\"perform\" + 0.045*\"analysis\" + 0.044*\"tool\" + 0.044*\"text\" + 0.044*\"translate\" + 0.044*\"chatbot\" + 0.044*\"application\"\n",
      "2022-02-24 12:21:49,764 : INFO : topic diff=0.073071, rho=0.447214\n",
      "2022-02-24 12:21:49,779 : INFO : -4.792 per-word bound, 27.7 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,781 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:49,792 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.045*\"course\" + 0.032*\"instructor\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"use\" + 0.032*\"space\" + 0.032*\"relationship\" + 0.032*\"vector\" + 0.031*\"processing\"\n",
      "2022-02-24 12:21:49,793 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"tweet\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"\n",
      "2022-02-24 12:21:49,793 : INFO : topic #2 (0.250): 0.081*\"learning\" + 0.081*\"specialization\" + 0.045*\"machine\" + 0.045*\"instructor\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"fun\" + 0.010*\"course\"\n",
      "2022-02-24 12:21:49,794 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.046*\"sentiment\" + 0.046*\"perform\" + 0.046*\"analysis\" + 0.045*\"tool\" + 0.045*\"text\" + 0.045*\"translate\" + 0.045*\"chatbot\" + 0.045*\"application\"\n",
      "2022-02-24 12:21:49,796 : INFO : topic diff=0.042586, rho=0.408248\n",
      "2022-02-24 12:21:49,805 : INFO : -4.788 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,807 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:49,813 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"instructor\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"use\" + 0.032*\"space\" + 0.032*\"relationship\" + 0.032*\"vector\" + 0.032*\"processing\"\n",
      "2022-02-24 12:21:49,815 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"tweet\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"\n",
      "2022-02-24 12:21:49,817 : INFO : topic #2 (0.250): 0.081*\"learning\" + 0.081*\"specialization\" + 0.045*\"instructor\" + 0.045*\"machine\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"fun\" + 0.010*\"course\"\n",
      "2022-02-24 12:21:49,818 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"perform\" + 0.046*\"analysis\" + 0.046*\"tool\" + 0.046*\"text\" + 0.046*\"translate\" + 0.046*\"chatbot\" + 0.046*\"application\"\n",
      "2022-02-24 12:21:49,819 : INFO : topic diff=0.025531, rho=0.377964\n",
      "2022-02-24 12:21:49,830 : INFO : -4.787 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,832 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:49,839 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"instructor\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"use\" + 0.032*\"space\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\"\n",
      "2022-02-24 12:21:49,841 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"tweet\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:49,842 : INFO : topic #2 (0.250): 0.081*\"learning\" + 0.081*\"specialization\" + 0.046*\"instructor\" + 0.045*\"machine\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"fun\" + 0.009*\"course\"\n",
      "2022-02-24 12:21:49,842 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"perform\" + 0.047*\"analysis\" + 0.046*\"tool\" + 0.046*\"text\" + 0.046*\"translate\" + 0.046*\"chatbot\" + 0.046*\"application\"\n",
      "2022-02-24 12:21:49,843 : INFO : topic diff=0.015724, rho=0.353553\n",
      "2022-02-24 12:21:49,854 : INFO : -4.786 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,855 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:49,861 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"instructor\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"use\" + 0.032*\"space\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\"\n",
      "2022-02-24 12:21:49,863 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"vector\" + 0.046*\"analysis\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"tweet\" + 0.026*\"lecture\" + 0.026*\"straight\"\n",
      "2022-02-24 12:21:49,864 : INFO : topic #2 (0.250): 0.082*\"learning\" + 0.082*\"specialization\" + 0.046*\"instructor\" + 0.045*\"machine\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"fun\" + 0.009*\"course\"\n",
      "2022-02-24 12:21:49,867 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"tool\" + 0.047*\"text\" + 0.047*\"translate\" + 0.047*\"chatbot\" + 0.047*\"application\"\n",
      "2022-02-24 12:21:49,868 : INFO : topic diff=0.009938, rho=0.333333\n",
      "2022-02-24 12:21:49,877 : INFO : -4.786 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,879 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:49,886 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"space\" + 0.032*\"use\" + 0.032*\"instructor\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\"\n",
      "2022-02-24 12:21:49,887 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"analysis\" + 0.046*\"vector\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\" + 0.026*\"tweet\"\n",
      "2022-02-24 12:21:49,888 : INFO : topic #2 (0.250): 0.082*\"learning\" + 0.082*\"specialization\" + 0.046*\"instructor\" + 0.045*\"machine\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"fun\" + 0.009*\"course\"\n",
      "2022-02-24 12:21:49,889 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"tool\" + 0.047*\"text\" + 0.047*\"translate\" + 0.047*\"chatbot\" + 0.047*\"application\"\n",
      "2022-02-24 12:21:49,891 : INFO : topic diff=0.006436, rho=0.316228\n",
      "2022-02-24 12:21:49,906 : INFO : -4.786 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 12:21:49,909 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:49,918 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"space\" + 0.032*\"use\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\" + 0.032*\"instructor\"\n",
      "2022-02-24 12:21:49,919 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"analysis\" + 0.046*\"vector\" + 0.026*\"machine\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"\n",
      "2022-02-24 12:21:49,920 : INFO : topic #2 (0.250): 0.082*\"learning\" + 0.082*\"specialization\" + 0.046*\"instructor\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"machine\" + 0.045*\"fun\" + 0.009*\"course\"\n",
      "2022-02-24 12:21:49,921 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"tool\" + 0.047*\"text\" + 0.047*\"translate\" + 0.047*\"chatbot\" + 0.047*\"application\"\n",
      "2022-02-24 12:21:49,922 : INFO : topic diff=0.004262, rho=0.301511\n",
      "2022-02-24 12:21:49,923 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"space\" + 0.032*\"use\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\" + 0.032*\"instructor\"\n",
      "2022-02-24 12:21:49,925 : INFO : topic #1 (0.250): 0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"analysis\" + 0.046*\"vector\" + 0.026*\"machine\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"\n",
      "2022-02-24 12:21:49,926 : INFO : topic #2 (0.250): 0.082*\"learning\" + 0.082*\"specialization\" + 0.046*\"instructor\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"machine\" + 0.045*\"fun\" + 0.009*\"course\"\n",
      "2022-02-24 12:21:49,927 : INFO : topic #3 (0.250): 0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"tool\" + 0.047*\"text\" + 0.047*\"translate\" + 0.047*\"chatbot\" + 0.047*\"application\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.046*\"word\" + 0.046*\"course\" + 0.032*\"language\" + 0.032*\"specialization\" + 0.032*\"space\" + 0.032*\"use\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\" + 0.032*\"instructor\"'),\n",
       " (1,\n",
       "  '0.067*\"regression\" + 0.067*\"course\" + 0.046*\"sentiment\" + 0.046*\"analysis\" + 0.046*\"vector\" + 0.026*\"machine\" + 0.026*\"learn\" + 0.026*\"feature\" + 0.026*\"lecture\" + 0.026*\"straight\"'),\n",
       " (2,\n",
       "  '0.082*\"learning\" + 0.082*\"specialization\" + 0.046*\"instructor\" + 0.045*\"ai\" + 0.045*\"stanford\" + 0.045*\"university\" + 0.045*\"younes\" + 0.045*\"machine\" + 0.045*\"fun\" + 0.009*\"course\"'),\n",
       " (3,\n",
       "  '0.047*\"specialization\" + 0.047*\"language\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"tool\" + 0.047*\"text\" + 0.047*\"translate\" + 0.047*\"chatbot\" + 0.047*\"application\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d643340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "924e6236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â â students course natural language pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end specialization nlp application perform sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization expert nlp machine deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes mourri instructor ai stanford universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings localityse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture detailed little hard straight helped r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lesson sentiment analysis logistic regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â â students course natural language pr...\n",
       "1  end specialization nlp application perform sen...\n",
       "2    specialization expert nlp machine deep learning\n",
       "3  younes mourri instructor ai stanford universit...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings localityse...\n",
       "6  lecture detailed little hard straight helped r...\n",
       "7                            other i informative fun\n",
       "8  lesson sentiment analysis logistic regression ...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d43240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>best</th>\n",
       "      <th>binary</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  approximate  assignment  aug  awesome  bayes  \\\n",
       "0   0         1            0            1           0    0        0      1   \n",
       "1   0         1            1            0           0    0        0      0   \n",
       "2   0         0            0            0           0    0        0      0   \n",
       "3   1         0            0            0           0    0        0      0   \n",
       "4   0         0            0            0           0    0        0      0   \n",
       "5   0         1            0            0           1    1        1      0   \n",
       "6   0         0            0            0           0    0        0      0   \n",
       "7   0         0            0            0           0    0        0      0   \n",
       "8   0         1            0            0           0    0        0      0   \n",
       "9   0         0            0            0           0    0        0      0   \n",
       "\n",
       "   best  binary  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0     0       0  ...      1      0           0    2       2      1          1   \n",
       "1     0       0  ...      0      0           0    0       0      0          0   \n",
       "2     0       0  ...      0      0           0    0       0      0          0   \n",
       "3     0       0  ...      0      0           1    0       0      0          0   \n",
       "4     0       0  ...      0      1           0    0       0      0          0   \n",
       "5     1       0  ...      0      0           0    0       1      0          0   \n",
       "6     0       0  ...      0      0           0    0       0      0          0   \n",
       "7     0       0  ...      0      0           0    0       0      0          0   \n",
       "8     0       1  ...      1      0           0    0       1      0          0   \n",
       "9     0       0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 98 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79beb7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:50,180 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-24 12:21:50,184 : INFO : using symmetric eta at 0.5\n",
      "2022-02-24 12:21:50,187 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:50,190 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:50,223 : INFO : -5.415 per-word bound, 42.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,224 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:50,246 : INFO : topic #0 (0.500): 0.040*\"specialization\" + 0.023*\"language\" + 0.022*\"word\" + 0.022*\"course\" + 0.021*\"learning\" + 0.020*\"nlp\" + 0.020*\"deep\" + 0.019*\"analysis\" + 0.019*\"perform\" + 0.018*\"instructor\"\n",
      "2022-02-24 12:21:50,247 : INFO : topic #1 (0.500): 0.035*\"course\" + 0.026*\"regression\" + 0.025*\"vector\" + 0.023*\"sentiment\" + 0.022*\"analysis\" + 0.021*\"logistic\" + 0.019*\"space\" + 0.019*\"word\" + 0.015*\"tweet\" + 0.015*\"instructor\"\n",
      "2022-02-24 12:21:50,248 : INFO : topic diff=0.640071, rho=1.000000\n",
      "2022-02-24 12:21:50,267 : INFO : -5.036 per-word bound, 32.8 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,267 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:50,284 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.025*\"language\" + 0.025*\"word\" + 0.024*\"course\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"analysis\" + 0.019*\"perform\" + 0.018*\"sentiment\"\n",
      "2022-02-24 12:21:50,285 : INFO : topic #1 (0.500): 0.034*\"course\" + 0.025*\"regression\" + 0.024*\"vector\" + 0.023*\"sentiment\" + 0.022*\"analysis\" + 0.022*\"logistic\" + 0.018*\"instructor\" + 0.016*\"space\" + 0.016*\"word\" + 0.014*\"tweet\"\n",
      "2022-02-24 12:21:50,286 : INFO : topic diff=0.193434, rho=0.577350\n",
      "2022-02-24 12:21:50,295 : INFO : -4.978 per-word bound, 31.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,296 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:50,306 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.025*\"language\" + 0.025*\"word\" + 0.025*\"course\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"analysis\" + 0.019*\"perform\" + 0.018*\"sentiment\"\n",
      "2022-02-24 12:21:50,307 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"regression\" + 0.023*\"vector\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.022*\"logistic\" + 0.020*\"instructor\" + 0.015*\"space\" + 0.015*\"word\" + 0.014*\"tweet\"\n",
      "2022-02-24 12:21:50,308 : INFO : topic diff=0.092197, rho=0.500000\n",
      "2022-02-24 12:21:50,318 : INFO : -4.963 per-word bound, 31.2 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,319 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:50,325 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"language\" + 0.026*\"course\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"analysis\" + 0.019*\"perform\" + 0.018*\"sentiment\"\n",
      "2022-02-24 12:21:50,326 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"vector\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.022*\"logistic\" + 0.021*\"instructor\" + 0.014*\"space\" + 0.014*\"word\" + 0.014*\"tweet\"\n",
      "2022-02-24 12:21:50,326 : INFO : topic diff=0.047360, rho=0.447214\n",
      "2022-02-24 12:21:50,343 : INFO : -4.958 per-word bound, 31.1 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,344 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:50,353 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"language\" + 0.026*\"course\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"analysis\" + 0.019*\"perform\" + 0.019*\"sentiment\"\n",
      "2022-02-24 12:21:50,354 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"vector\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"logistic\" + 0.022*\"instructor\" + 0.014*\"space\" + 0.014*\"word\" + 0.014*\"tweet\"\n",
      "2022-02-24 12:21:50,355 : INFO : topic diff=0.025957, rho=0.408248\n",
      "2022-02-24 12:21:50,364 : INFO : -4.957 per-word bound, 31.1 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,366 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:50,373 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"language\" + 0.026*\"course\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"perform\" + 0.019*\"analysis\" + 0.019*\"sentiment\"\n",
      "2022-02-24 12:21:50,374 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"vector\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"logistic\" + 0.022*\"instructor\" + 0.014*\"space\" + 0.014*\"word\" + 0.014*\"machine\"\n",
      "2022-02-24 12:21:50,375 : INFO : topic diff=0.014978, rho=0.377964\n",
      "2022-02-24 12:21:50,385 : INFO : -4.956 per-word bound, 31.0 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,386 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:50,396 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"learning\" + 0.019*\"nlp\" + 0.019*\"deep\" + 0.019*\"perform\" + 0.019*\"analysis\" + 0.019*\"vector\"\n",
      "2022-02-24 12:21:50,398 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"vector\" + 0.023*\"analysis\" + 0.023*\"logistic\" + 0.022*\"instructor\" + 0.014*\"machine\" + 0.014*\"space\" + 0.014*\"word\"\n",
      "2022-02-24 12:21:50,399 : INFO : topic diff=0.009022, rho=0.353553\n",
      "2022-02-24 12:21:50,409 : INFO : -4.956 per-word bound, 31.0 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,410 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:50,417 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"nlp\" + 0.019*\"learning\" + 0.019*\"deep\" + 0.019*\"perform\" + 0.019*\"vector\" + 0.019*\"space\"\n",
      "2022-02-24 12:21:50,418 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"vector\" + 0.023*\"logistic\" + 0.023*\"instructor\" + 0.014*\"machine\" + 0.014*\"space\" + 0.014*\"text\"\n",
      "2022-02-24 12:21:50,419 : INFO : topic diff=0.005636, rho=0.333333\n",
      "2022-02-24 12:21:50,428 : INFO : -4.956 per-word bound, 31.0 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,429 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:50,440 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"vector\" + 0.019*\"space\" + 0.019*\"nlp\" + 0.019*\"learning\" + 0.019*\"perform\" + 0.019*\"deep\"\n",
      "2022-02-24 12:21:50,441 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"vector\" + 0.023*\"logistic\" + 0.023*\"instructor\" + 0.014*\"machine\" + 0.014*\"text\" + 0.014*\"space\"\n",
      "2022-02-24 12:21:50,443 : INFO : topic diff=0.003630, rho=0.316228\n",
      "2022-02-24 12:21:50,453 : INFO : -4.956 per-word bound, 31.0 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,454 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:50,464 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"vector\" + 0.019*\"space\" + 0.019*\"perform\" + 0.019*\"nlp\" + 0.019*\"learning\" + 0.019*\"deep\"\n",
      "2022-02-24 12:21:50,466 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"instructor\" + 0.023*\"logistic\" + 0.023*\"vector\" + 0.014*\"machine\" + 0.014*\"text\" + 0.014*\"translation\"\n",
      "2022-02-24 12:21:50,469 : INFO : topic diff=0.002401, rho=0.301511\n",
      "2022-02-24 12:21:50,470 : INFO : topic #0 (0.500): 0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"vector\" + 0.019*\"space\" + 0.019*\"perform\" + 0.019*\"nlp\" + 0.019*\"learning\" + 0.019*\"deep\"\n",
      "2022-02-24 12:21:50,472 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"instructor\" + 0.023*\"logistic\" + 0.023*\"vector\" + 0.014*\"machine\" + 0.014*\"text\" + 0.014*\"translation\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"specialization\" + 0.026*\"word\" + 0.026*\"course\" + 0.026*\"language\" + 0.019*\"vector\" + 0.019*\"space\" + 0.019*\"perform\" + 0.019*\"nlp\" + 0.019*\"learning\" + 0.019*\"deep\"'),\n",
       " (1,\n",
       "  '0.032*\"course\" + 0.023*\"regression\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"instructor\" + 0.023*\"logistic\" + 0.023*\"vector\" + 0.014*\"machine\" + 0.014*\"text\" + 0.014*\"translation\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d88ce9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:50,492 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2022-02-24 12:21:50,493 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2022-02-24 12:21:50,495 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:50,497 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:50,522 : INFO : -6.002 per-word bound, 64.1 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,523 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:50,538 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.027*\"specialization\" + 0.027*\"vector\" + 0.027*\"language\" + 0.027*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,539 : INFO : topic #1 (0.333): 0.053*\"instructor\" + 0.030*\"senior\" + 0.030*\"curriculum\" + 0.030*\"developer\" + 0.029*\"fun\" + 0.028*\"informative\" + 0.016*\"coauthor\" + 0.015*\"transformer\" + 0.015*\"scientist\" + 0.015*\"brain\"\n",
      "2022-02-24 12:21:50,540 : INFO : topic #2 (0.333): 0.041*\"regression\" + 0.029*\"specialization\" + 0.029*\"logistic\" + 0.029*\"learning\" + 0.028*\"deep\" + 0.017*\"vector\" + 0.017*\"sentiment\" + 0.017*\"tweet\" + 0.017*\"analysis\" + 0.017*\"instructor\"\n",
      "2022-02-24 12:21:50,541 : INFO : topic diff=1.448197, rho=1.000000\n",
      "2022-02-24 12:21:50,552 : INFO : -5.039 per-word bound, 32.9 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,553 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:50,558 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"analysis\" + 0.028*\"specialization\" + 0.028*\"sentiment\" + 0.028*\"vector\" + 0.028*\"language\" + 0.027*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,559 : INFO : topic #1 (0.333): 0.056*\"instructor\" + 0.032*\"senior\" + 0.032*\"curriculum\" + 0.032*\"developer\" + 0.031*\"fun\" + 0.031*\"informative\" + 0.012*\"coauthor\" + 0.012*\"transformer\" + 0.011*\"scientist\" + 0.011*\"brain\"\n",
      "2022-02-24 12:21:50,561 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"specialization\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.016*\"vector\" + 0.016*\"tweet\" + 0.016*\"sentiment\" + 0.016*\"analysis\" + 0.016*\"text\"\n",
      "2022-02-24 12:21:50,563 : INFO : topic diff=0.079419, rho=0.577350\n",
      "2022-02-24 12:21:50,574 : INFO : -5.021 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,575 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:50,583 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"vector\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,585 : INFO : topic #1 (0.333): 0.058*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.032*\"fun\" + 0.032*\"informative\" + 0.010*\"coauthor\" + 0.010*\"transformer\" + 0.010*\"scientist\" + 0.010*\"brain\"\n",
      "2022-02-24 12:21:50,586 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"specialization\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.016*\"vector\" + 0.016*\"tweet\" + 0.016*\"text\" + 0.016*\"sentiment\" + 0.016*\"analysis\"\n",
      "2022-02-24 12:21:50,586 : INFO : topic diff=0.035371, rho=0.500000\n",
      "2022-02-24 12:21:50,595 : INFO : -5.017 per-word bound, 32.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,595 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:50,602 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"vector\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,604 : INFO : topic #1 (0.333): 0.058*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.009*\"scientist\" + 0.009*\"brain\"\n",
      "2022-02-24 12:21:50,605 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"tweet\" + 0.016*\"learn\" + 0.016*\"vector\" + 0.016*\"numerical\"\n",
      "2022-02-24 12:21:50,606 : INFO : topic diff=0.017692, rho=0.447214\n",
      "2022-02-24 12:21:50,616 : INFO : -5.016 per-word bound, 32.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,618 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:50,624 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"analysis\" + 0.028*\"vector\" + 0.028*\"sentiment\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,625 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.009*\"scientist\" + 0.009*\"brain\"\n",
      "2022-02-24 12:21:50,626 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"tweet\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\"\n",
      "2022-02-24 12:21:50,627 : INFO : topic diff=0.009515, rho=0.408248\n",
      "2022-02-24 12:21:50,637 : INFO : -5.016 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,638 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:50,644 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,645 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.009*\"scientist\" + 0.009*\"brain\"\n",
      "2022-02-24 12:21:50,646 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"tweet\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\"\n",
      "2022-02-24 12:21:50,648 : INFO : topic diff=0.005412, rho=0.377964\n",
      "2022-02-24 12:21:50,659 : INFO : -5.016 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,660 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:50,668 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"rating\" + 0.019*\"localitysensitive\"\n",
      "2022-02-24 12:21:50,669 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.009*\"scientist\" + 0.009*\"brain\"\n",
      "2022-02-24 12:21:50,670 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"\n",
      "2022-02-24 12:21:50,670 : INFO : topic diff=0.003222, rho=0.353553\n",
      "2022-02-24 12:21:50,683 : INFO : -5.015 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,684 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:50,689 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"language\" + 0.028*\"space\" + 0.019*\"processing\" + 0.019*\"rating\"\n",
      "2022-02-24 12:21:50,690 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.033*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.009*\"scientist\" + 0.009*\"brain\"\n",
      "2022-02-24 12:21:50,690 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:50,691 : INFO : topic diff=0.001993, rho=0.333333\n",
      "2022-02-24 12:21:50,702 : INFO : -5.015 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,703 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:50,708 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"space\" + 0.028*\"language\" + 0.019*\"processing\" + 0.019*\"rating\"\n",
      "2022-02-24 12:21:50,709 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.034*\"senior\" + 0.033*\"curriculum\" + 0.033*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.009*\"coauthor\" + 0.009*\"transformer\" + 0.008*\"scientist\" + 0.008*\"brain\"\n",
      "2022-02-24 12:21:50,710 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"\n",
      "2022-02-24 12:21:50,712 : INFO : topic diff=0.001273, rho=0.316228\n",
      "2022-02-24 12:21:50,724 : INFO : -5.015 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,725 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:50,732 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"space\" + 0.028*\"language\" + 0.019*\"processing\" + 0.019*\"natural\"\n",
      "2022-02-24 12:21:50,734 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.034*\"senior\" + 0.034*\"curriculum\" + 0.034*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.008*\"coauthor\" + 0.008*\"transformer\" + 0.008*\"scientist\" + 0.008*\"brain\"\n",
      "2022-02-24 12:21:50,735 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"\n",
      "2022-02-24 12:21:50,736 : INFO : topic diff=0.000836, rho=0.301511\n",
      "2022-02-24 12:21:50,737 : INFO : topic #0 (0.333): 0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"space\" + 0.028*\"language\" + 0.019*\"processing\" + 0.019*\"natural\"\n",
      "2022-02-24 12:21:50,738 : INFO : topic #1 (0.333): 0.059*\"instructor\" + 0.034*\"senior\" + 0.034*\"curriculum\" + 0.034*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.008*\"coauthor\" + 0.008*\"transformer\" + 0.008*\"scientist\" + 0.008*\"brain\"\n",
      "2022-02-24 12:21:50,739 : INFO : topic #2 (0.333): 0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.052*\"course\" + 0.036*\"word\" + 0.028*\"specialization\" + 0.028*\"vector\" + 0.028*\"analysis\" + 0.028*\"sentiment\" + 0.028*\"space\" + 0.028*\"language\" + 0.019*\"processing\" + 0.019*\"natural\"'),\n",
       " (1,\n",
       "  '0.059*\"instructor\" + 0.034*\"senior\" + 0.034*\"curriculum\" + 0.034*\"developer\" + 0.033*\"fun\" + 0.033*\"informative\" + 0.008*\"coauthor\" + 0.008*\"transformer\" + 0.008*\"scientist\" + 0.008*\"brain\"'),\n",
       " (2,\n",
       "  '0.040*\"regression\" + 0.028*\"logistic\" + 0.028*\"learning\" + 0.028*\"deep\" + 0.028*\"specialization\" + 0.016*\"text\" + 0.016*\"learn\" + 0.016*\"numerical\" + 0.016*\"lesson\" + 0.016*\"binary\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe669413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:50,757 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 12:21:50,759 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 12:21:50,760 : INFO : using serial LDA version on this node\n",
      "2022-02-24 12:21:50,761 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 12:21:50,782 : INFO : -6.734 per-word bound, 106.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,784 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 12:21:50,808 : INFO : topic #0 (0.250): 0.040*\"course\" + 0.030*\"specialization\" + 0.025*\"space\" + 0.025*\"word\" + 0.022*\"instructor\" + 0.022*\"learning\" + 0.022*\"deep\" + 0.022*\"younes\" + 0.022*\"university\" + 0.022*\"mourri\"\n",
      "2022-02-24 12:21:50,809 : INFO : topic #1 (0.250): 0.037*\"helped\" + 0.037*\"hard\" + 0.037*\"lecture\" + 0.037*\"regression\" + 0.037*\"straight\" + 0.037*\"model\" + 0.037*\"detailed\" + 0.037*\"little\" + 0.011*\"language\" + 0.011*\"nlp\"\n",
      "2022-02-24 12:21:50,810 : INFO : topic #2 (0.250): 0.051*\"course\" + 0.033*\"specialization\" + 0.029*\"word\" + 0.028*\"machine\" + 0.025*\"vector\" + 0.021*\"space\" + 0.021*\"translation\" + 0.020*\"rating\" + 0.019*\"nlp\" + 0.019*\"deep\"\n",
      "2022-02-24 12:21:50,811 : INFO : topic #3 (0.250): 0.030*\"logistic\" + 0.030*\"analysis\" + 0.029*\"regression\" + 0.028*\"sentiment\" + 0.025*\"instructor\" + 0.024*\"text\" + 0.023*\"vector\" + 0.022*\"language\" + 0.021*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:50,813 : INFO : topic diff=1.798167, rho=1.000000\n",
      "2022-02-24 12:21:50,825 : INFO : -5.415 per-word bound, 42.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,826 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 12:21:50,834 : INFO : topic #0 (0.250): 0.033*\"specialization\" + 0.029*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"mourri\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.026*\"course\"\n",
      "2022-02-24 12:21:50,835 : INFO : topic #1 (0.250): 0.038*\"helped\" + 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.009*\"language\" + 0.009*\"nlp\"\n",
      "2022-02-24 12:21:50,837 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.036*\"word\" + 0.031*\"specialization\" + 0.029*\"vector\" + 0.028*\"space\" + 0.023*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.020*\"localitysensitive\" + 0.020*\"processing\"\n",
      "2022-02-24 12:21:50,838 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.030*\"sentiment\" + 0.029*\"instructor\" + 0.028*\"text\" + 0.020*\"vector\" + 0.019*\"language\" + 0.019*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:50,839 : INFO : topic diff=0.320196, rho=0.577350\n",
      "2022-02-24 12:21:50,851 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,853 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 12:21:50,860 : INFO : topic #0 (0.250): 0.035*\"specialization\" + 0.033*\"learning\" + 0.033*\"deep\" + 0.032*\"instructor\" + 0.032*\"younes\" + 0.032*\"university\" + 0.032*\"mourri\" + 0.032*\"stanford\" + 0.032*\"ai\" + 0.018*\"course\"\n",
      "2022-02-24 12:21:50,862 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"helped\" + 0.038*\"regression\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"language\" + 0.008*\"nlp\"\n",
      "2022-02-24 12:21:50,864 : INFO : topic #2 (0.250): 0.058*\"course\" + 0.038*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.029*\"space\" + 0.022*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:50,866 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.030*\"instructor\" + 0.030*\"text\" + 0.019*\"vector\" + 0.019*\"language\" + 0.018*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:50,868 : INFO : topic diff=0.170799, rho=0.500000\n",
      "2022-02-24 12:21:50,876 : INFO : -5.142 per-word bound, 35.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,877 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 12:21:50,890 : INFO : topic #0 (0.250): 0.036*\"specialization\" + 0.035*\"learning\" + 0.035*\"deep\" + 0.035*\"instructor\" + 0.034*\"younes\" + 0.034*\"university\" + 0.034*\"mourri\" + 0.034*\"stanford\" + 0.034*\"ai\" + 0.013*\"course\"\n",
      "2022-02-24 12:21:50,891 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"language\" + 0.008*\"nlp\"\n",
      "2022-02-24 12:21:50,894 : INFO : topic #2 (0.250): 0.058*\"course\" + 0.039*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.022*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:50,896 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"instructor\" + 0.030*\"text\" + 0.018*\"vector\" + 0.018*\"language\" + 0.018*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:50,899 : INFO : topic diff=0.101063, rho=0.447214\n",
      "2022-02-24 12:21:50,921 : INFO : -5.122 per-word bound, 34.8 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,923 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 12:21:50,947 : INFO : topic #0 (0.250): 0.037*\"specialization\" + 0.036*\"learning\" + 0.036*\"deep\" + 0.036*\"instructor\" + 0.036*\"younes\" + 0.036*\"university\" + 0.036*\"mourri\" + 0.036*\"stanford\" + 0.036*\"ai\" + 0.011*\"course\"\n",
      "2022-02-24 12:21:50,949 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"language\" + 0.008*\"nlp\"\n",
      "2022-02-24 12:21:50,952 : INFO : topic #2 (0.250): 0.058*\"course\" + 0.039*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:50,959 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"instructor\" + 0.031*\"text\" + 0.018*\"vector\" + 0.018*\"language\" + 0.018*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:50,961 : INFO : topic diff=0.060969, rho=0.408248\n",
      "2022-02-24 12:21:50,982 : INFO : -5.114 per-word bound, 34.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:50,984 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 12:21:51,005 : INFO : topic #0 (0.250): 0.037*\"specialization\" + 0.036*\"learning\" + 0.036*\"deep\" + 0.036*\"instructor\" + 0.036*\"younes\" + 0.036*\"university\" + 0.036*\"mourri\" + 0.036*\"stanford\" + 0.036*\"ai\" + 0.010*\"course\"\n",
      "2022-02-24 12:21:51,007 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"language\" + 0.008*\"nlp\"\n",
      "2022-02-24 12:21:51,009 : INFO : topic #2 (0.250): 0.058*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:51,012 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.018*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:51,017 : INFO : topic diff=0.037382, rho=0.377964\n",
      "2022-02-24 12:21:51,024 : INFO : -5.111 per-word bound, 34.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:51,025 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 12:21:51,031 : INFO : topic #0 (0.250): 0.037*\"specialization\" + 0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.009*\"course\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 12:21:51,032 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"language\" + 0.008*\"nlp\"\n",
      "2022-02-24 12:21:51,034 : INFO : topic #2 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:51,035 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.018*\"specialization\" + 0.018*\"tweet\"\n",
      "2022-02-24 12:21:51,036 : INFO : topic diff=0.023364, rho=0.353553\n",
      "2022-02-24 12:21:51,047 : INFO : -5.110 per-word bound, 34.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:51,048 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 12:21:51,054 : INFO : topic #0 (0.250): 0.037*\"specialization\" + 0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.008*\"course\"\n",
      "2022-02-24 12:21:51,055 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"nlp\" + 0.008*\"language\"\n",
      "2022-02-24 12:21:51,056 : INFO : topic #2 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:51,057 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"analysis\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.018*\"specialization\" + 0.017*\"tweet\"\n",
      "2022-02-24 12:21:51,058 : INFO : topic diff=0.014911, rho=0.333333\n",
      "2022-02-24 12:21:51,071 : INFO : -5.110 per-word bound, 34.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:51,072 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 12:21:51,084 : INFO : topic #0 (0.250): 0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"specialization\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.008*\"course\"\n",
      "2022-02-24 12:21:51,086 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"nlp\" + 0.008*\"language\"\n",
      "2022-02-24 12:21:51,088 : INFO : topic #2 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.030*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"localitysensitive\" + 0.021*\"processing\"\n",
      "2022-02-24 12:21:51,089 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"regression\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.017*\"tweet\" + 0.017*\"perform\"\n",
      "2022-02-24 12:21:51,090 : INFO : topic diff=0.009720, rho=0.316228\n",
      "2022-02-24 12:21:51,105 : INFO : -5.109 per-word bound, 34.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 12:21:51,106 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 12:21:51,115 : INFO : topic #0 (0.250): 0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"specialization\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.008*\"course\"\n",
      "2022-02-24 12:21:51,116 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"nlp\" + 0.008*\"language\"\n",
      "2022-02-24 12:21:51,118 : INFO : topic #2 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.031*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"language\" + 0.021*\"localitysensitive\"\n",
      "2022-02-24 12:21:51,120 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"regression\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.017*\"tweet\" + 0.017*\"perform\"\n",
      "2022-02-24 12:21:51,121 : INFO : topic diff=0.006466, rho=0.301511\n",
      "2022-02-24 12:21:51,125 : INFO : topic #0 (0.250): 0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"specialization\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.008*\"course\"\n",
      "2022-02-24 12:21:51,127 : INFO : topic #1 (0.250): 0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"nlp\" + 0.008*\"language\"\n",
      "2022-02-24 12:21:51,129 : INFO : topic #2 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.031*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"language\" + 0.021*\"localitysensitive\"\n",
      "2022-02-24 12:21:51,131 : INFO : topic #3 (0.250): 0.031*\"logistic\" + 0.031*\"regression\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.017*\"tweet\" + 0.017*\"perform\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.037*\"learning\" + 0.037*\"deep\" + 0.037*\"specialization\" + 0.037*\"instructor\" + 0.037*\"younes\" + 0.037*\"university\" + 0.037*\"mourri\" + 0.037*\"stanford\" + 0.037*\"ai\" + 0.008*\"course\"'),\n",
       " (1,\n",
       "  '0.038*\"model\" + 0.038*\"regression\" + 0.038*\"helped\" + 0.038*\"hard\" + 0.038*\"lecture\" + 0.038*\"straight\" + 0.038*\"detailed\" + 0.038*\"little\" + 0.008*\"nlp\" + 0.008*\"language\"'),\n",
       " (2,\n",
       "  '0.059*\"course\" + 0.040*\"word\" + 0.031*\"specialization\" + 0.031*\"vector\" + 0.030*\"space\" + 0.021*\"machine\" + 0.021*\"translation\" + 0.021*\"rating\" + 0.021*\"language\" + 0.021*\"localitysensitive\"'),\n",
       " (3,\n",
       "  '0.031*\"logistic\" + 0.031*\"regression\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"text\" + 0.031*\"instructor\" + 0.018*\"vector\" + 0.018*\"language\" + 0.017*\"tweet\" + 0.017*\"perform\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4daee25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:28,897 : INFO : using symmetric alpha at 0.2\n",
      "2022-02-24 15:25:28,899 : INFO : using symmetric eta at 0.2\n",
      "2022-02-24 15:25:28,900 : INFO : using serial LDA version on this node\n",
      "2022-02-24 15:25:28,901 : INFO : running online (multi-pass) LDA training, 5 topics, 80 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 15:25:28,907 : INFO : -7.566 per-word bound, 189.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,908 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 15:25:28,915 : INFO : topic #0 (0.200): 0.027*\"nlp\" + 0.027*\"sentiment\" + 0.027*\"perform\" + 0.027*\"analysis\" + 0.027*\"end\" + 0.027*\"chatbot\" + 0.027*\"tool\" + 0.027*\"application\" + 0.027*\"language\" + 0.027*\"specialization\"\n",
      "2022-02-24 15:25:28,916 : INFO : topic #1 (0.200): 0.053*\"instructor\" + 0.037*\"specialization\" + 0.037*\"deep\" + 0.037*\"learning\" + 0.030*\"course\" + 0.029*\"machine\" + 0.021*\"analysis\" + 0.020*\"vector\" + 0.020*\"nlp\" + 0.020*\"stanford\"\n",
      "2022-02-24 15:25:28,917 : INFO : topic #2 (0.200): 0.044*\"informative\" + 0.043*\"fun\" + 0.020*\"course\" + 0.018*\"vector\" + 0.017*\"space\" + 0.017*\"specialization\" + 0.016*\"natural\" + 0.016*\"relationship\" + 0.016*\"language\" + 0.015*\"word\"\n",
      "2022-02-24 15:25:28,917 : INFO : topic #3 (0.200): 0.051*\"course\" + 0.041*\"word\" + 0.028*\"space\" + 0.027*\"vector\" + 0.023*\"use\" + 0.022*\"natural\" + 0.021*\"relationship\" + 0.021*\"embeddings\" + 0.020*\"specialization\" + 0.020*\"sentiment\"\n",
      "2022-02-24 15:25:28,918 : INFO : topic #4 (0.200): 0.047*\"logistic\" + 0.046*\"regression\" + 0.029*\"vector\" + 0.026*\"tweet\" + 0.024*\"sentiment\" + 0.023*\"analysis\" + 0.020*\"word\" + 0.019*\"numerical\" + 0.018*\"lesson\" + 0.018*\"language\"\n",
      "2022-02-24 15:25:28,918 : INFO : topic diff=2.387650, rho=1.000000\n",
      "2022-02-24 15:25:28,923 : INFO : -5.497 per-word bound, 45.2 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,923 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 15:25:28,926 : INFO : topic #0 (0.200): 0.025*\"nlp\" + 0.025*\"perform\" + 0.025*\"analysis\" + 0.025*\"end\" + 0.025*\"sentiment\" + 0.025*\"chatbot\" + 0.025*\"tool\" + 0.025*\"application\" + 0.025*\"language\" + 0.025*\"specialization\"\n",
      "2022-02-24 15:25:28,927 : INFO : topic #1 (0.200): 0.066*\"instructor\" + 0.046*\"specialization\" + 0.046*\"deep\" + 0.046*\"learning\" + 0.030*\"machine\" + 0.025*\"nlp\" + 0.025*\"stanford\" + 0.025*\"ai\" + 0.025*\"mourri\" + 0.025*\"university\"\n",
      "2022-02-24 15:25:28,928 : INFO : topic #2 (0.200): 0.050*\"informative\" + 0.050*\"fun\" + 0.014*\"course\" + 0.013*\"vector\" + 0.013*\"space\" + 0.013*\"specialization\" + 0.013*\"natural\" + 0.012*\"relationship\" + 0.012*\"language\" + 0.012*\"word\"\n",
      "2022-02-24 15:25:28,929 : INFO : topic #3 (0.200): 0.060*\"course\" + 0.043*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.022*\"relationship\" + 0.022*\"embeddings\" + 0.022*\"specialization\" + 0.022*\"sentiment\"\n",
      "2022-02-24 15:25:28,929 : INFO : topic #4 (0.200): 0.055*\"logistic\" + 0.055*\"regression\" + 0.032*\"vector\" + 0.030*\"tweet\" + 0.029*\"sentiment\" + 0.029*\"analysis\" + 0.027*\"numerical\" + 0.027*\"lesson\" + 0.027*\"extract\" + 0.027*\"feature\"\n",
      "2022-02-24 15:25:28,930 : INFO : topic diff=0.390347, rho=0.577350\n",
      "2022-02-24 15:25:28,935 : INFO : -5.223 per-word bound, 37.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,936 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 15:25:28,939 : INFO : topic #0 (0.200): 0.024*\"nlp\" + 0.024*\"perform\" + 0.024*\"end\" + 0.024*\"chatbot\" + 0.024*\"analysis\" + 0.024*\"tool\" + 0.024*\"sentiment\" + 0.024*\"application\" + 0.024*\"language\" + 0.024*\"translate\"\n",
      "2022-02-24 15:25:28,939 : INFO : topic #1 (0.200): 0.073*\"instructor\" + 0.050*\"specialization\" + 0.050*\"deep\" + 0.050*\"learning\" + 0.030*\"machine\" + 0.027*\"nlp\" + 0.027*\"stanford\" + 0.027*\"ai\" + 0.027*\"mourri\" + 0.027*\"university\"\n",
      "2022-02-24 15:25:28,940 : INFO : topic #2 (0.200): 0.053*\"informative\" + 0.052*\"fun\" + 0.012*\"course\" + 0.011*\"vector\" + 0.011*\"space\" + 0.011*\"specialization\" + 0.011*\"natural\" + 0.011*\"relationship\" + 0.011*\"language\" + 0.011*\"word\"\n",
      "2022-02-24 15:25:28,940 : INFO : topic #3 (0.200): 0.062*\"course\" + 0.043*\"word\" + 0.032*\"space\" + 0.032*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,941 : INFO : topic #4 (0.200): 0.058*\"logistic\" + 0.058*\"regression\" + 0.033*\"vector\" + 0.032*\"tweet\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.030*\"numerical\" + 0.030*\"lesson\" + 0.030*\"extract\" + 0.030*\"feature\"\n",
      "2022-02-24 15:25:28,942 : INFO : topic diff=0.220979, rho=0.500000\n",
      "2022-02-24 15:25:28,947 : INFO : -5.145 per-word bound, 35.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,948 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 15:25:28,950 : INFO : topic #0 (0.200): 0.024*\"nlp\" + 0.024*\"perform\" + 0.024*\"end\" + 0.024*\"chatbot\" + 0.024*\"tool\" + 0.024*\"application\" + 0.024*\"analysis\" + 0.024*\"sentiment\" + 0.024*\"language\" + 0.024*\"translate\"\n",
      "2022-02-24 15:25:28,951 : INFO : topic #1 (0.200): 0.076*\"instructor\" + 0.052*\"specialization\" + 0.052*\"deep\" + 0.052*\"learning\" + 0.030*\"machine\" + 0.029*\"nlp\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"university\"\n",
      "2022-02-24 15:25:28,951 : INFO : topic #2 (0.200): 0.054*\"informative\" + 0.054*\"fun\" + 0.011*\"course\" + 0.010*\"vector\" + 0.010*\"space\" + 0.010*\"specialization\" + 0.010*\"natural\" + 0.010*\"relationship\" + 0.010*\"language\" + 0.010*\"word\"\n",
      "2022-02-24 15:25:28,952 : INFO : topic #3 (0.200): 0.064*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,952 : INFO : topic #4 (0.200): 0.060*\"logistic\" + 0.060*\"regression\" + 0.033*\"vector\" + 0.033*\"tweet\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"extract\" + 0.032*\"feature\"\n",
      "2022-02-24 15:25:28,953 : INFO : topic diff=0.132875, rho=0.447214\n",
      "2022-02-24 15:25:28,956 : INFO : -5.118 per-word bound, 34.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,957 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 15:25:28,960 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"perform\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"translate\" + 0.023*\"language\"\n",
      "2022-02-24 15:25:28,960 : INFO : topic #1 (0.200): 0.078*\"instructor\" + 0.054*\"specialization\" + 0.054*\"deep\" + 0.054*\"learning\" + 0.030*\"machine\" + 0.029*\"nlp\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"university\"\n",
      "2022-02-24 15:25:28,962 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.010*\"course\" + 0.010*\"vector\" + 0.010*\"space\" + 0.010*\"specialization\" + 0.010*\"natural\" + 0.010*\"relationship\" + 0.010*\"language\" + 0.010*\"word\"\n",
      "2022-02-24 15:25:28,963 : INFO : topic #3 (0.200): 0.064*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,965 : INFO : topic #4 (0.200): 0.061*\"logistic\" + 0.061*\"regression\" + 0.033*\"vector\" + 0.033*\"tweet\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.033*\"numerical\" + 0.033*\"lesson\" + 0.033*\"extract\" + 0.033*\"feature\"\n",
      "2022-02-24 15:25:28,966 : INFO : topic diff=0.080645, rho=0.408248\n",
      "2022-02-24 15:25:28,972 : INFO : -5.108 per-word bound, 34.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,973 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 15:25:28,975 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"perform\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,975 : INFO : topic #1 (0.200): 0.079*\"instructor\" + 0.054*\"specialization\" + 0.054*\"deep\" + 0.054*\"learning\" + 0.030*\"machine\" + 0.030*\"nlp\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"mourri\" + 0.030*\"university\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:28,976 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.010*\"course\" + 0.010*\"vector\" + 0.010*\"space\" + 0.010*\"specialization\" + 0.010*\"natural\" + 0.010*\"relationship\" + 0.010*\"language\" + 0.010*\"word\"\n",
      "2022-02-24 15:25:28,977 : INFO : topic #3 (0.200): 0.064*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,977 : INFO : topic #4 (0.200): 0.061*\"logistic\" + 0.061*\"regression\" + 0.033*\"vector\" + 0.033*\"tweet\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.033*\"numerical\" + 0.033*\"lesson\" + 0.033*\"extract\" + 0.033*\"feature\"\n",
      "2022-02-24 15:25:28,978 : INFO : topic diff=0.049538, rho=0.377964\n",
      "2022-02-24 15:25:28,983 : INFO : -5.104 per-word bound, 34.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,984 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 15:25:28,986 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"perform\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:28,986 : INFO : topic #1 (0.200): 0.080*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"nlp\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"mourri\" + 0.030*\"university\"\n",
      "2022-02-24 15:25:28,987 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.010*\"course\" + 0.010*\"vector\" + 0.010*\"space\" + 0.010*\"specialization\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:28,988 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,988 : INFO : topic #4 (0.200): 0.061*\"regression\" + 0.061*\"logistic\" + 0.034*\"vector\" + 0.033*\"tweet\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.033*\"numerical\" + 0.033*\"lesson\" + 0.033*\"extract\" + 0.033*\"feature\"\n",
      "2022-02-24 15:25:28,989 : INFO : topic diff=0.030967, rho=0.353553\n",
      "2022-02-24 15:25:28,992 : INFO : -5.102 per-word bound, 34.4 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:28,993 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 15:25:28,995 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"perform\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:28,996 : INFO : topic #1 (0.200): 0.080*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:28,997 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.010*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:28,998 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:28,998 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.033*\"analysis\" + 0.033*\"numerical\" + 0.033*\"lesson\" + 0.033*\"extract\" + 0.033*\"feature\"\n",
      "2022-02-24 15:25:28,999 : INFO : topic diff=0.019756, rho=0.333333\n",
      "2022-02-24 15:25:29,003 : INFO : -5.102 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,005 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 15:25:29,007 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"perform\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:29,008 : INFO : topic #1 (0.200): 0.080*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,008 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,009 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"specialization\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:29,009 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.033*\"numerical\" + 0.033*\"lesson\" + 0.033*\"extract\" + 0.033*\"feature\"\n",
      "2022-02-24 15:25:29,009 : INFO : topic diff=0.012871, rho=0.316228\n",
      "2022-02-24 15:25:29,015 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,015 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 15:25:29,018 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"perform\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:29,019 : INFO : topic #1 (0.200): 0.080*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,020 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,020 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\" + 0.023*\"specialization\" + 0.023*\"embeddings\" + 0.023*\"language\"\n",
      "2022-02-24 15:25:29,021 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"text\"\n",
      "2022-02-24 15:25:29,021 : INFO : topic diff=0.008557, rho=0.301511\n",
      "2022-02-24 15:25:29,024 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,025 : INFO : PROGRESS: pass 10, at document #10/10\n",
      "2022-02-24 15:25:29,027 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"perform\" + 0.023*\"translate\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:29,027 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,028 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,028 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"specialization\" + 0.023*\"relationship\" + 0.023*\"embeddings\" + 0.023*\"language\"\n",
      "2022-02-24 15:25:29,029 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"text\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,030 : INFO : topic diff=0.005799, rho=0.288675\n",
      "2022-02-24 15:25:29,034 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,034 : INFO : PROGRESS: pass 11, at document #10/10\n",
      "2022-02-24 15:25:29,036 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,037 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,037 : INFO : topic #2 (0.200): 0.055*\"informative\" + 0.055*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,038 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"specialization\" + 0.023*\"relationship\" + 0.023*\"language\" + 0.023*\"embeddings\"\n",
      "2022-02-24 15:25:29,038 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,038 : INFO : topic diff=0.003999, rho=0.277350\n",
      "2022-02-24 15:25:29,043 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,044 : INFO : PROGRESS: pass 12, at document #10/10\n",
      "2022-02-24 15:25:29,046 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:29,047 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.055*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,047 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,048 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"specialization\" + 0.023*\"natural\" + 0.023*\"language\" + 0.023*\"relationship\" + 0.023*\"sentiment\"\n",
      "2022-02-24 15:25:29,049 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,049 : INFO : topic diff=0.002802, rho=0.267261\n",
      "2022-02-24 15:25:29,052 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,053 : INFO : PROGRESS: pass 13, at document #10/10\n",
      "2022-02-24 15:25:29,054 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"language\" + 0.023*\"text\" + 0.023*\"ukasz\"\n",
      "2022-02-24 15:25:29,055 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.055*\"deep\" + 0.055*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"ai\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"younes\"\n",
      "2022-02-24 15:25:29,056 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,056 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"use\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"sentiment\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,058 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"vector\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,058 : INFO : topic diff=0.001993, rho=0.258199\n",
      "2022-02-24 15:25:29,064 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,065 : INFO : PROGRESS: pass 14, at document #10/10\n",
      "2022-02-24 15:25:29,068 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"language\" + 0.023*\"ukasz\" + 0.023*\"text\"\n",
      "2022-02-24 15:25:29,069 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,069 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,070 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"sentiment\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,071 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,071 : INFO : topic diff=0.001436, rho=0.250000\n",
      "2022-02-24 15:25:29,074 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,075 : INFO : PROGRESS: pass 15, at document #10/10\n",
      "2022-02-24 15:25:29,077 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,078 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,078 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,078 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"sentiment\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,079 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,080 : INFO : topic diff=0.001047, rho=0.242536\n",
      "2022-02-24 15:25:29,083 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,083 : INFO : PROGRESS: pass 16, at document #10/10\n",
      "2022-02-24 15:25:29,085 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,086 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,086 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,087 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"sentiment\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,088 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"vector\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,088 : INFO : topic diff=0.000772, rho=0.235702\n",
      "2022-02-24 15:25:29,091 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,092 : INFO : PROGRESS: pass 17, at document #10/10\n",
      "2022-02-24 15:25:29,094 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,094 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,095 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"language\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,096 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"analysis\"\n",
      "2022-02-24 15:25:29,096 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"vector\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,097 : INFO : topic diff=0.000575, rho=0.229416\n",
      "2022-02-24 15:25:29,101 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,102 : INFO : PROGRESS: pass 18, at document #10/10\n",
      "2022-02-24 15:25:29,105 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,105 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"stanford\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,106 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"specialization\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"language\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,106 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"analysis\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,106 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,106 : INFO : topic diff=0.000432, rho=0.223607\n",
      "2022-02-24 15:25:29,109 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,111 : INFO : PROGRESS: pass 19, at document #10/10\n",
      "2022-02-24 15:25:29,113 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,114 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,114 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"specialization\" + 0.009*\"vector\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"language\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,115 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,115 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,116 : INFO : topic diff=0.000328, rho=0.218218\n",
      "2022-02-24 15:25:29,121 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,122 : INFO : PROGRESS: pass 20, at document #10/10\n",
      "2022-02-24 15:25:29,124 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,125 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,125 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"specialization\" + 0.009*\"vector\" + 0.009*\"space\" + 0.009*\"language\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,126 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,126 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,127 : INFO : topic diff=0.000250, rho=0.213201\n",
      "2022-02-24 15:25:29,132 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,133 : INFO : PROGRESS: pass 21, at document #10/10\n",
      "2022-02-24 15:25:29,137 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,137 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,138 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"specialization\" + 0.009*\"vector\" + 0.009*\"space\" + 0.009*\"language\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,138 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,139 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"vector\"\n",
      "2022-02-24 15:25:29,139 : INFO : topic diff=0.000193, rho=0.208514\n",
      "2022-02-24 15:25:29,143 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,144 : INFO : PROGRESS: pass 22, at document #10/10\n",
      "2022-02-24 15:25:29,146 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"perform\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,147 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,148 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"specialization\" + 0.009*\"vector\" + 0.009*\"space\" + 0.009*\"language\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,148 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,149 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,149 : INFO : topic diff=0.000150, rho=0.204124\n",
      "2022-02-24 15:25:29,153 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,153 : INFO : PROGRESS: pass 23, at document #10/10\n",
      "2022-02-24 15:25:29,156 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"perform\" + 0.023*\"trax\" + 0.023*\"google\"\n",
      "2022-02-24 15:25:29,156 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,156 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"course\" + 0.009*\"specialization\" + 0.009*\"vector\" + 0.009*\"space\" + 0.009*\"language\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,157 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,157 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,158 : INFO : topic diff=0.000117, rho=0.200000\n",
      "2022-02-24 15:25:29,162 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,162 : INFO : PROGRESS: pass 24, at document #10/10\n",
      "2022-02-24 15:25:29,164 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,165 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,166 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"language\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"word\"\n",
      "2022-02-24 15:25:29,166 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,167 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,167 : INFO : topic diff=0.000092, rho=0.196116\n",
      "2022-02-24 15:25:29,170 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,171 : INFO : PROGRESS: pass 25, at document #10/10\n",
      "2022-02-24 15:25:29,173 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,173 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,174 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"language\" + 0.009*\"space\" + 0.009*\"natural\" + 0.009*\"relationship\" + 0.009*\"analysis\"\n",
      "2022-02-24 15:25:29,174 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,175 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,175 : INFO : topic diff=0.000072, rho=0.192450\n",
      "2022-02-24 15:25:29,179 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,179 : INFO : PROGRESS: pass 26, at document #10/10\n",
      "2022-02-24 15:25:29,182 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,182 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,183 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"language\" + 0.009*\"space\" + 0.009*\"analysis\" + 0.009*\"natural\" + 0.009*\"regression\"\n",
      "2022-02-24 15:25:29,183 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,184 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,184 : INFO : topic diff=0.000057, rho=0.188982\n",
      "2022-02-24 15:25:29,187 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,187 : INFO : PROGRESS: pass 27, at document #10/10\n",
      "2022-02-24 15:25:29,190 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,191 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,191 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"language\" + 0.009*\"analysis\" + 0.009*\"space\" + 0.009*\"regression\" + 0.009*\"sentiment\"\n",
      "2022-02-24 15:25:29,192 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,192 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,192 : INFO : topic diff=0.000046, rho=0.185695\n",
      "2022-02-24 15:25:29,197 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,197 : INFO : PROGRESS: pass 28, at document #10/10\n",
      "2022-02-24 15:25:29,200 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,200 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,201 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"language\" + 0.009*\"analysis\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"space\"\n",
      "2022-02-24 15:25:29,201 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,202 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,203 : INFO : topic diff=0.000037, rho=0.182574\n",
      "2022-02-24 15:25:29,206 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,207 : INFO : PROGRESS: pass 29, at document #10/10\n",
      "2022-02-24 15:25:29,209 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"chatbot\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,209 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,210 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"course\" + 0.009*\"vector\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"language\" + 0.009*\"sentiment\" + 0.009*\"model\"\n",
      "2022-02-24 15:25:29,210 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,210 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,211 : INFO : topic diff=0.000029, rho=0.179605\n",
      "2022-02-24 15:25:29,215 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,215 : INFO : PROGRESS: pass 30, at document #10/10\n",
      "2022-02-24 15:25:29,218 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"chatbot\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,218 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,219 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"language\" + 0.009*\"vector\" + 0.009*\"course\" + 0.009*\"sentiment\" + 0.009*\"model\"\n",
      "2022-02-24 15:25:29,219 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,220 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,220 : INFO : topic diff=0.000024, rho=0.176777\n",
      "2022-02-24 15:25:29,224 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,224 : INFO : PROGRESS: pass 31, at document #10/10\n",
      "2022-02-24 15:25:29,227 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"tool\" + 0.023*\"application\" + 0.023*\"end\" + 0.023*\"chatbot\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,228 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,229 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"language\" + 0.009*\"nlp\" + 0.009*\"model\" + 0.009*\"vector\"\n",
      "2022-02-24 15:25:29,229 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,229 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,230 : INFO : topic diff=0.000019, rho=0.174078\n",
      "2022-02-24 15:25:29,233 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,233 : INFO : PROGRESS: pass 32, at document #10/10\n",
      "2022-02-24 15:25:29,236 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,237 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,237 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"nlp\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"machine\" + 0.009*\"language\"\n",
      "2022-02-24 15:25:29,237 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,238 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,238 : INFO : topic diff=0.000016, rho=0.171499\n",
      "2022-02-24 15:25:29,242 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,243 : INFO : PROGRESS: pass 33, at document #10/10\n",
      "2022-02-24 15:25:29,245 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"end\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,246 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,246 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"nlp\" + 0.009*\"sentiment\" + 0.009*\"machine\" + 0.009*\"model\" + 0.009*\"language\"\n",
      "2022-02-24 15:25:29,247 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,247 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,247 : INFO : topic diff=0.000013, rho=0.169031\n",
      "2022-02-24 15:25:29,250 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,252 : INFO : PROGRESS: pass 34, at document #10/10\n",
      "2022-02-24 15:25:29,254 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,254 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,255 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"nlp\" + 0.009*\"analysis\" + 0.009*\"machine\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"language\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,255 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,255 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,256 : INFO : topic diff=0.000011, rho=0.166667\n",
      "2022-02-24 15:25:29,259 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,259 : INFO : PROGRESS: pass 35, at document #10/10\n",
      "2022-02-24 15:25:29,263 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,263 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,263 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"nlp\" + 0.009*\"analysis\" + 0.009*\"machine\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"language\"\n",
      "2022-02-24 15:25:29,264 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,264 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,265 : INFO : topic diff=0.000009, rho=0.164399\n",
      "2022-02-24 15:25:29,268 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,268 : INFO : PROGRESS: pass 36, at document #10/10\n",
      "2022-02-24 15:25:29,272 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,272 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,272 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"nlp\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"developer\"\n",
      "2022-02-24 15:25:29,273 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,273 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,274 : INFO : topic diff=0.000007, rho=0.162221\n",
      "2022-02-24 15:25:29,277 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,278 : INFO : PROGRESS: pass 37, at document #10/10\n",
      "2022-02-24 15:25:29,280 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,281 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,281 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"nlp\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"developer\"\n",
      "2022-02-24 15:25:29,282 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,282 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,282 : INFO : topic diff=0.000006, rho=0.160128\n",
      "2022-02-24 15:25:29,286 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,286 : INFO : PROGRESS: pass 38, at document #10/10\n",
      "2022-02-24 15:25:29,288 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,289 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,289 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"developer\"\n",
      "2022-02-24 15:25:29,290 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,290 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,290 : INFO : topic diff=0.000005, rho=0.158114\n",
      "2022-02-24 15:25:29,294 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,294 : INFO : PROGRESS: pass 39, at document #10/10\n",
      "2022-02-24 15:25:29,297 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,297 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,298 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,298 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,299 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,299 : INFO : topic diff=0.000004, rho=0.156174\n",
      "2022-02-24 15:25:29,303 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,303 : INFO : PROGRESS: pass 40, at document #10/10\n",
      "2022-02-24 15:25:29,306 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"scientist\" + 0.023*\"google\" + 0.023*\"trax\"\n",
      "2022-02-24 15:25:29,306 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,307 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,308 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,308 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,308 : INFO : topic diff=0.000003, rho=0.154303\n",
      "2022-02-24 15:25:29,311 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,312 : INFO : PROGRESS: pass 41, at document #10/10\n",
      "2022-02-24 15:25:29,315 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,315 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,316 : INFO : topic #2 (0.200): 0.056*\"informative\" + 0.056*\"fun\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,316 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,317 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,318 : INFO : topic diff=0.000003, rho=0.152499\n",
      "2022-02-24 15:25:29,321 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,321 : INFO : PROGRESS: pass 42, at document #10/10\n",
      "2022-02-24 15:25:29,323 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,323 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,324 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,325 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,325 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,326 : INFO : topic diff=0.000002, rho=0.150756\n",
      "2022-02-24 15:25:29,328 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,329 : INFO : PROGRESS: pass 43, at document #10/10\n",
      "2022-02-24 15:25:29,331 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,331 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,332 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,332 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,333 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,333 : INFO : topic diff=0.000002, rho=0.149071\n",
      "2022-02-24 15:25:29,337 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,337 : INFO : PROGRESS: pass 44, at document #10/10\n",
      "2022-02-24 15:25:29,340 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,341 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,341 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,341 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,342 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"extract\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,343 : INFO : topic diff=0.000002, rho=0.147442\n",
      "2022-02-24 15:25:29,346 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,346 : INFO : PROGRESS: pass 45, at document #10/10\n",
      "2022-02-24 15:25:29,348 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,349 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,349 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,350 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,350 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"extract\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,351 : INFO : topic diff=0.000001, rho=0.145865\n",
      "2022-02-24 15:25:29,354 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,354 : INFO : PROGRESS: pass 46, at document #10/10\n",
      "2022-02-24 15:25:29,357 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"google\" + 0.023*\"scientist\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,358 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,358 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,359 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,359 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"numerical\" + 0.034*\"extract\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,359 : INFO : topic diff=0.000001, rho=0.144338\n",
      "2022-02-24 15:25:29,363 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,363 : INFO : PROGRESS: pass 47, at document #10/10\n",
      "2022-02-24 15:25:29,366 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"translate\" + 0.023*\"chatbot\" + 0.023*\"tool\" + 0.023*\"end\" + 0.023*\"trax\" + 0.023*\"ukasz\" + 0.023*\"application\" + 0.023*\"google\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,366 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,367 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,367 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,368 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,368 : INFO : topic diff=0.000001, rho=0.142857\n",
      "2022-02-24 15:25:29,371 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,372 : INFO : PROGRESS: pass 48, at document #10/10\n",
      "2022-02-24 15:25:29,374 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"google\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,374 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,375 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,375 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,376 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,377 : INFO : topic diff=0.000001, rho=0.141421\n",
      "2022-02-24 15:25:29,380 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,381 : INFO : PROGRESS: pass 49, at document #10/10\n",
      "2022-02-24 15:25:29,384 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"google\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,384 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,384 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,385 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,385 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,386 : INFO : topic diff=0.000001, rho=0.140028\n",
      "2022-02-24 15:25:29,389 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,389 : INFO : PROGRESS: pass 50, at document #10/10\n",
      "2022-02-24 15:25:29,392 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tool\" + 0.023*\"chatbot\" + 0.023*\"google\" + 0.023*\"end\" + 0.023*\"application\" + 0.023*\"translate\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,392 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,393 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"specialization\" + 0.009*\"nlp\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,393 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,394 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,394 : INFO : topic diff=0.000001, rho=0.138675\n",
      "2022-02-24 15:25:29,398 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,398 : INFO : PROGRESS: pass 51, at document #10/10\n",
      "2022-02-24 15:25:29,400 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"scientist\" + 0.023*\"google\" + 0.023*\"tensorflow\" + 0.023*\"research\" + 0.023*\"paper\" + 0.023*\"staff\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,401 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,401 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,402 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,402 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,403 : INFO : topic diff=0.000001, rho=0.137361\n",
      "2022-02-24 15:25:29,406 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,406 : INFO : PROGRESS: pass 52, at document #10/10\n",
      "2022-02-24 15:25:29,409 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"research\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,409 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,410 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,410 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,411 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 15:25:29,412 : INFO : topic diff=0.000001, rho=0.136083\n",
      "2022-02-24 15:25:29,415 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,416 : INFO : PROGRESS: pass 53, at document #10/10\n",
      "2022-02-24 15:25:29,418 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"scientist\" + 0.023*\"google\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"coauthor\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,418 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,419 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,419 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,420 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,421 : INFO : topic diff=0.000000, rho=0.134840\n",
      "2022-02-24 15:25:29,424 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,425 : INFO : PROGRESS: pass 54, at document #10/10\n",
      "2022-02-24 15:25:29,427 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"research\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,428 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,428 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,429 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,430 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,430 : INFO : topic diff=0.000000, rho=0.133631\n",
      "2022-02-24 15:25:29,433 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,434 : INFO : PROGRESS: pass 55, at document #10/10\n",
      "2022-02-24 15:25:29,436 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"research\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,436 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,437 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,437 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,438 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,438 : INFO : topic diff=0.000000, rho=0.132453\n",
      "2022-02-24 15:25:29,441 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,442 : INFO : PROGRESS: pass 56, at document #10/10\n",
      "2022-02-24 15:25:29,445 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"research\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,445 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,446 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"regression\" + 0.009*\"machine\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"curriculum\"\n",
      "2022-02-24 15:25:29,446 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"natural\" + 0.023*\"use\"\n",
      "2022-02-24 15:25:29,447 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n",
      "2022-02-24 15:25:29,447 : INFO : topic diff=0.000000, rho=0.131306\n",
      "2022-02-24 15:25:29,450 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,451 : INFO : PROGRESS: pass 57, at document #10/10\n",
      "2022-02-24 15:25:29,454 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,454 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,455 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,455 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,456 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,456 : INFO : topic diff=0.000000, rho=0.130189\n",
      "2022-02-24 15:25:29,460 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,461 : INFO : PROGRESS: pass 58, at document #10/10\n",
      "2022-02-24 15:25:29,463 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"staff\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"research\" + 0.023*\"paper\"\n",
      "2022-02-24 15:25:29,463 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,464 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,464 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,464 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,465 : INFO : topic diff=0.000000, rho=0.129099\n",
      "2022-02-24 15:25:29,468 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,469 : INFO : PROGRESS: pass 59, at document #10/10\n",
      "2022-02-24 15:25:29,471 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,471 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,472 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,472 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"natural\" + 0.023*\"use\"\n",
      "2022-02-24 15:25:29,473 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,473 : INFO : topic diff=0.000000, rho=0.128037\n",
      "2022-02-24 15:25:29,476 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,477 : INFO : PROGRESS: pass 60, at document #10/10\n",
      "2022-02-24 15:25:29,479 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,480 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,480 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,480 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,480 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,481 : INFO : topic diff=0.000000, rho=0.127000\n",
      "2022-02-24 15:25:29,484 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,484 : INFO : PROGRESS: pass 61, at document #10/10\n",
      "2022-02-24 15:25:29,486 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,487 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,488 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,488 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,489 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,489 : INFO : topic diff=0.000000, rho=0.125988\n",
      "2022-02-24 15:25:29,492 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,493 : INFO : PROGRESS: pass 62, at document #10/10\n",
      "2022-02-24 15:25:29,495 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,496 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,496 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,496 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"natural\"\n",
      "2022-02-24 15:25:29,497 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,498 : INFO : topic diff=0.000000, rho=0.125000\n",
      "2022-02-24 15:25:29,501 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,501 : INFO : PROGRESS: pass 63, at document #10/10\n",
      "2022-02-24 15:25:29,504 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"ukasz\" + 0.023*\"trax\" + 0.023*\"tensorflow\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\"\n",
      "2022-02-24 15:25:29,504 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,505 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,505 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"natural\" + 0.023*\"use\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,506 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,506 : INFO : topic diff=0.000000, rho=0.124035\n",
      "2022-02-24 15:25:29,510 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,510 : INFO : PROGRESS: pass 64, at document #10/10\n",
      "2022-02-24 15:25:29,513 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,513 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,514 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,514 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,515 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,515 : INFO : topic diff=0.000000, rho=0.123091\n",
      "2022-02-24 15:25:29,518 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,519 : INFO : PROGRESS: pass 65, at document #10/10\n",
      "2022-02-24 15:25:29,521 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,521 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,522 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,522 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 15:25:29,522 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,523 : INFO : topic diff=0.000000, rho=0.122169\n",
      "2022-02-24 15:25:29,526 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,526 : INFO : PROGRESS: pass 66, at document #10/10\n",
      "2022-02-24 15:25:29,530 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,530 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,531 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,531 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"sentiment\" + 0.023*\"analysis\" + 0.023*\"natural\" + 0.023*\"use\"\n",
      "2022-02-24 15:25:29,532 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,532 : INFO : topic diff=0.000000, rho=0.121268\n",
      "2022-02-24 15:25:29,536 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,536 : INFO : PROGRESS: pass 67, at document #10/10\n",
      "2022-02-24 15:25:29,539 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,539 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,539 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,540 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,540 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,541 : INFO : topic diff=0.000000, rho=0.120386\n",
      "2022-02-24 15:25:29,544 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,545 : INFO : PROGRESS: pass 68, at document #10/10\n",
      "2022-02-24 15:25:29,547 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,547 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,548 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,548 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,548 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,549 : INFO : topic diff=0.000000, rho=0.119523\n",
      "2022-02-24 15:25:29,552 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,553 : INFO : PROGRESS: pass 69, at document #10/10\n",
      "2022-02-24 15:25:29,556 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,556 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,556 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,557 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,557 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,558 : INFO : topic diff=0.000000, rho=0.118678\n",
      "2022-02-24 15:25:29,561 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,561 : INFO : PROGRESS: pass 70, at document #10/10\n",
      "2022-02-24 15:25:29,564 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,564 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,565 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,565 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,567 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,567 : INFO : topic diff=0.000000, rho=0.117851\n",
      "2022-02-24 15:25:29,570 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,571 : INFO : PROGRESS: pass 71, at document #10/10\n",
      "2022-02-24 15:25:29,573 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,574 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,574 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,575 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,575 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,575 : INFO : topic diff=0.000000, rho=0.117041\n",
      "2022-02-24 15:25:29,579 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,580 : INFO : PROGRESS: pass 72, at document #10/10\n",
      "2022-02-24 15:25:29,583 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,583 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,583 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,584 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,585 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,585 : INFO : topic diff=0.000000, rho=0.116248\n",
      "2022-02-24 15:25:29,590 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,590 : INFO : PROGRESS: pass 73, at document #10/10\n",
      "2022-02-24 15:25:29,592 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,593 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,593 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,594 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,594 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,595 : INFO : topic diff=0.000000, rho=0.115470\n",
      "2022-02-24 15:25:29,598 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,599 : INFO : PROGRESS: pass 74, at document #10/10\n",
      "2022-02-24 15:25:29,601 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,601 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,602 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,602 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,602 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,603 : INFO : topic diff=0.000000, rho=0.114708\n",
      "2022-02-24 15:25:29,606 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,607 : INFO : PROGRESS: pass 75, at document #10/10\n",
      "2022-02-24 15:25:29,609 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,609 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:25:29,610 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,610 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,611 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,611 : INFO : topic diff=0.000000, rho=0.113961\n",
      "2022-02-24 15:25:29,614 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,615 : INFO : PROGRESS: pass 76, at document #10/10\n",
      "2022-02-24 15:25:29,618 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,618 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,619 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,619 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,620 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,620 : INFO : topic diff=0.000000, rho=0.113228\n",
      "2022-02-24 15:25:29,623 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,624 : INFO : PROGRESS: pass 77, at document #10/10\n",
      "2022-02-24 15:25:29,626 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,626 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,627 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,627 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,628 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,629 : INFO : topic diff=0.000000, rho=0.112509\n",
      "2022-02-24 15:25:29,632 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,632 : INFO : PROGRESS: pass 78, at document #10/10\n",
      "2022-02-24 15:25:29,634 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,635 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,635 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,635 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,636 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,636 : INFO : topic diff=0.000000, rho=0.111803\n",
      "2022-02-24 15:25:29,640 : INFO : -5.101 per-word bound, 34.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 15:25:29,640 : INFO : PROGRESS: pass 79, at document #10/10\n",
      "2022-02-24 15:25:29,642 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,643 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,643 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,644 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,645 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n",
      "2022-02-24 15:25:29,645 : INFO : topic diff=0.000000, rho=0.111111\n",
      "2022-02-24 15:25:29,646 : INFO : topic #0 (0.200): 0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"\n",
      "2022-02-24 15:25:29,646 : INFO : topic #1 (0.200): 0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"\n",
      "2022-02-24 15:25:29,647 : INFO : topic #2 (0.200): 0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"\n",
      "2022-02-24 15:25:29,648 : INFO : topic #3 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"\n",
      "2022-02-24 15:25:29,648 : INFO : topic #4 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"nlp\" + 0.023*\"library\" + 0.023*\"brain\" + 0.023*\"research\" + 0.023*\"staff\" + 0.023*\"paper\" + 0.023*\"google\" + 0.023*\"coauthor\" + 0.023*\"tensorflow\" + 0.023*\"scientist\"'),\n",
       " (1,\n",
       "  '0.081*\"instructor\" + 0.056*\"specialization\" + 0.056*\"deep\" + 0.056*\"learning\" + 0.030*\"machine\" + 0.030*\"younes\" + 0.030*\"university\" + 0.030*\"stanford\" + 0.030*\"mourri\" + 0.030*\"ai\"'),\n",
       " (2,\n",
       "  '0.056*\"fun\" + 0.056*\"informative\" + 0.009*\"nlp\" + 0.009*\"specialization\" + 0.009*\"machine\" + 0.009*\"regression\" + 0.009*\"sentiment\" + 0.009*\"analysis\" + 0.009*\"model\" + 0.009*\"senior\"'),\n",
       " (3,\n",
       "  '0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"analysis\" + 0.023*\"sentiment\" + 0.023*\"use\" + 0.023*\"processing\"'),\n",
       " (4,\n",
       "  '0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"text\" + 0.034*\"tweet\" + 0.034*\"classifier\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"extract\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bec4a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(3, 0.98423713)], 0),\n",
       " ([(0, 0.93794733),\n",
       "   (1, 0.015538729),\n",
       "   (2, 0.015391254),\n",
       "   (3, 0.015581874),\n",
       "   (4, 0.01554082)],\n",
       "  1),\n",
       " ([(0, 0.028716104),\n",
       "   (1, 0.8854514),\n",
       "   (2, 0.02857835),\n",
       "   (3, 0.028678542),\n",
       "   (4, 0.028575635)],\n",
       "  2),\n",
       " ([(0, 0.020021288),\n",
       "   (1, 0.91994584),\n",
       "   (2, 0.02000486),\n",
       "   (3, 0.020025084),\n",
       "   (4, 0.020002956)],\n",
       "  3),\n",
       " ([(0, 0.9384454),\n",
       "   (1, 0.015388263),\n",
       "   (2, 0.015391453),\n",
       "   (3, 0.015386183),\n",
       "   (4, 0.01538868)],\n",
       "  4),\n",
       " ([(3, 0.97030735)], 5),\n",
       " ([(0, 0.91074777),\n",
       "   (1, 0.022227537),\n",
       "   (2, 0.022231929),\n",
       "   (3, 0.02230375),\n",
       "   (4, 0.022489058)],\n",
       "  6),\n",
       " ([(0, 0.06667232),\n",
       "   (1, 0.06667399),\n",
       "   (2, 0.73330903),\n",
       "   (3, 0.0666698),\n",
       "   (4, 0.0666748)],\n",
       "  7),\n",
       " ([(0, 0.011816977),\n",
       "   (1, 0.011766334),\n",
       "   (2, 0.011767677),\n",
       "   (3, 0.0118445745),\n",
       "   (4, 0.95280445)],\n",
       "  8),\n",
       " ([(0, 0.033336755),\n",
       "   (1, 0.8666483),\n",
       "   (2, 0.033341426),\n",
       "   (3, 0.03333524),\n",
       "   (4, 0.03333825)],\n",
       "  9)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e124ed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dom_Topic  Topic_Contri                                           Keywords\n",
      "0        3.0        0.9842  course, word, vector, space, specialization, l...\n",
      "1        0.0        0.9379  nlp, library, brain, research, staff, paper, g...\n",
      "2        1.0        0.8855  instructor, specialization, deep, learning, ma...\n",
      "3        1.0        0.9199  instructor, specialization, deep, learning, ma...\n",
      "4        0.0        0.9384  nlp, library, brain, research, staff, paper, g...\n",
      "5        3.0        0.9703  course, word, vector, space, specialization, l...\n",
      "6        0.0        0.9107  nlp, library, brain, research, staff, paper, g...\n",
      "7        2.0        0.7333  fun, informative, nlp, specialization, machine...\n",
      "8        4.0        0.9528  regression, logistic, sentiment, analysis, tex...\n",
      "9        1.0        0.8666  instructor, specialization, deep, learning, ma...\n"
     ]
    }
   ],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(ldana[corpusna]):\n",
    "        row = row_list[0] if ldana.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldana.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "sent_topics_df.columns = ['Dom_Topic', 'Topic_Contri', 'Keywords']\n",
    "print(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e550428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
