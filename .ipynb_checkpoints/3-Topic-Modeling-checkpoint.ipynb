{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7084085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>attented</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>...</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>waiting</th>\n",
       "      <th>wasnnto</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>write</th>\n",
       "      <th>younes</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  algorithm  analysis  application  approximate  assignment  attented  \\\n",
       "0   0          1         1            0            1           0         0   \n",
       "1   0          0         1            1            0           0         0   \n",
       "2   0          0         0            0            0           0         0   \n",
       "3   1          0         0            0            0           0         0   \n",
       "4   0          0         0            0            0           0         0   \n",
       "5   0          0         1            0            0           1         1   \n",
       "6   0          0         0            0            0           0         0   \n",
       "7   0          0         0            0            0           0         0   \n",
       "8   0          0         1            0            0           0         0   \n",
       "9   0          0         0            0            0           0         0   \n",
       "\n",
       "   aug  awesome  bayes  ...  vector  video  visualize  waiting  wasnnto  week  \\\n",
       "0    0        0      1  ...       2      1          1        0        0     0   \n",
       "1    0        0      0  ...       0      0          0        0        0     0   \n",
       "2    0        0      0  ...       0      0          0        0        0     0   \n",
       "3    0        0      0  ...       0      0          0        0        0     0   \n",
       "4    0        0      0  ...       0      0          0        0        0     0   \n",
       "5    2        1      0  ...       1      0          0        1        1     1   \n",
       "6    0        0      0  ...       0      0          0        0        0     0   \n",
       "7    0        0      0  ...       0      0          0        0        0     0   \n",
       "8    0        0      0  ...       1      0          0        0        0     0   \n",
       "9    0        0      0  ...       0      0          0        0        0     0   \n",
       "\n",
       "   word  write  younes  youtube  \n",
       "0     3      1       0        0  \n",
       "1     0      0       0        0  \n",
       "2     0      0       0        0  \n",
       "3     0      0       1        0  \n",
       "4     0      0       0        0  \n",
       "5     1      0       0        0  \n",
       "6     0      0       0        1  \n",
       "7     0      0       0        0  \n",
       "8     0      0       0        0  \n",
       "9     0      0       0        0  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef8f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d76bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approximate</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2  3  4  5  6  7  8  9\n",
       "ai           0  0  0  1  0  0  0  0  0  0\n",
       "algorithm    1  0  0  0  0  0  0  0  0  0\n",
       "analysis     1  1  0  0  0  1  0  0  1  0\n",
       "application  0  1  0  0  0  0  0  0  0  0\n",
       "approximate  1  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aeee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22350489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:43:55,832 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-24 21:43:55,834 : INFO : using symmetric eta at 0.5\n",
      "2022-02-24 21:43:55,834 : INFO : using serial LDA version on this node\n",
      "2022-02-24 21:43:55,835 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 21:43:55,847 : INFO : -5.480 per-word bound, 44.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,848 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 21:43:55,855 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.023*\"learning\" + 0.021*\"word\" + 0.020*\"analysis\" + 0.020*\"course\" + 0.020*\"vector\" + 0.019*\"logistic\" + 0.018*\"language\" + 0.016*\"deep\" + 0.016*\"using\"\n",
      "2022-02-24 21:43:55,856 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.023*\"instructor\" + 0.020*\"vector\" + 0.020*\"aug\" + 0.020*\"analysis\" + 0.019*\"word\" + 0.016*\"using\" + 0.015*\"localitysensitive\" + 0.015*\"specialization\" + 0.015*\"embeddings\"\n",
      "2022-02-24 21:43:55,857 : INFO : topic diff=0.576360, rho=1.000000\n",
      "2022-02-24 21:43:55,863 : INFO : -5.132 per-word bound, 35.1 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,864 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 21:43:55,868 : INFO : topic #0 (0.500): 0.034*\"specialization\" + 0.023*\"learning\" + 0.022*\"word\" + 0.021*\"vector\" + 0.021*\"analysis\" + 0.021*\"course\" + 0.021*\"logistic\" + 0.021*\"language\" + 0.020*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,868 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.025*\"instructor\" + 0.023*\"aug\" + 0.018*\"analysis\" + 0.018*\"vector\" + 0.017*\"word\" + 0.016*\"localitysensitive\" + 0.015*\"embeddings\" + 0.015*\"hashing\" + 0.015*\"translation\"\n",
      "2022-02-24 21:43:55,869 : INFO : topic diff=0.210680, rho=0.577350\n",
      "2022-02-24 21:43:55,872 : INFO : -5.053 per-word bound, 33.2 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,873 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 21:43:55,877 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.023*\"learning\" + 0.022*\"word\" + 0.022*\"vector\" + 0.022*\"analysis\" + 0.022*\"logistic\" + 0.022*\"course\" + 0.022*\"language\" + 0.021*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,877 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.026*\"instructor\" + 0.025*\"aug\" + 0.017*\"analysis\" + 0.017*\"vector\" + 0.017*\"word\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\" + 0.016*\"hashing\" + 0.015*\"machine\"\n",
      "2022-02-24 21:43:55,877 : INFO : topic diff=0.105233, rho=0.500000\n",
      "2022-02-24 21:43:55,882 : INFO : -5.032 per-word bound, 32.7 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,882 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 21:43:55,884 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"learning\" + 0.022*\"word\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"analysis\" + 0.022*\"course\" + 0.022*\"language\" + 0.022*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,886 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.026*\"aug\" + 0.017*\"analysis\" + 0.017*\"vector\" + 0.016*\"word\" + 0.016*\"localitysensitive\" + 0.016*\"machine\" + 0.016*\"embeddings\" + 0.016*\"hashing\"\n",
      "2022-02-24 21:43:55,886 : INFO : topic diff=0.056013, rho=0.447214\n",
      "2022-02-24 21:43:55,890 : INFO : -5.025 per-word bound, 32.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,890 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 21:43:55,893 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"learning\" + 0.022*\"word\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"analysis\" + 0.022*\"course\" + 0.022*\"language\" + 0.022*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,893 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.026*\"aug\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"word\" + 0.016*\"machine\" + 0.016*\"localitysensitive\" + 0.016*\"helped\" + 0.016*\"embeddings\"\n",
      "2022-02-24 21:43:55,894 : INFO : topic diff=0.031447, rho=0.408248\n",
      "2022-02-24 21:43:55,898 : INFO : -5.023 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,899 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 21:43:55,902 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"learning\" + 0.022*\"word\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"analysis\" + 0.022*\"language\" + 0.022*\"course\" + 0.022*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,903 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.026*\"aug\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"machine\" + 0.016*\"word\" + 0.016*\"helped\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\"\n",
      "2022-02-24 21:43:55,903 : INFO : topic diff=0.018422, rho=0.377964\n",
      "2022-02-24 21:43:55,909 : INFO : -5.022 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,910 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 21:43:55,912 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"analysis\" + 0.022*\"language\" + 0.022*\"course\" + 0.022*\"using\" + 0.016*\"deep\"\n",
      "2022-02-24 21:43:55,913 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.026*\"aug\" + 0.016*\"machine\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"helped\" + 0.016*\"word\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\"\n",
      "2022-02-24 21:43:55,913 : INFO : topic diff=0.011197, rho=0.353553\n",
      "2022-02-24 21:43:55,917 : INFO : -5.022 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,917 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 21:43:55,919 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"analysis\" + 0.022*\"language\" + 0.022*\"using\" + 0.022*\"course\" + 0.016*\"natural\"\n",
      "2022-02-24 21:43:55,920 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.026*\"aug\" + 0.016*\"machine\" + 0.016*\"helped\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"word\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\"\n",
      "2022-02-24 21:43:55,920 : INFO : topic diff=0.007031, rho=0.333333\n",
      "2022-02-24 21:43:55,926 : INFO : -5.022 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,927 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 21:43:55,929 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"language\" + 0.022*\"analysis\" + 0.022*\"using\" + 0.022*\"course\" + 0.016*\"natural\"\n",
      "2022-02-24 21:43:55,930 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.027*\"aug\" + 0.016*\"machine\" + 0.016*\"helped\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\" + 0.016*\"hashing\"\n",
      "2022-02-24 21:43:55,930 : INFO : topic diff=0.004543, rho=0.316228\n",
      "2022-02-24 21:43:55,934 : INFO : -5.022 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,934 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 21:43:55,937 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"language\" + 0.022*\"analysis\" + 0.022*\"using\" + 0.022*\"course\" + 0.016*\"natural\"\n",
      "2022-02-24 21:43:55,938 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.027*\"aug\" + 0.016*\"machine\" + 0.016*\"helped\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\" + 0.016*\"hashing\"\n",
      "2022-02-24 21:43:55,938 : INFO : topic diff=0.003010, rho=0.301511\n",
      "2022-02-24 21:43:55,940 : INFO : topic #0 (0.500): 0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"language\" + 0.022*\"analysis\" + 0.022*\"using\" + 0.022*\"course\" + 0.016*\"natural\"\n",
      "2022-02-24 21:43:55,941 : INFO : topic #1 (0.500): 0.037*\"course\" + 0.027*\"instructor\" + 0.027*\"aug\" + 0.016*\"machine\" + 0.016*\"helped\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\" + 0.016*\"hashing\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"specialization\" + 0.022*\"word\" + 0.022*\"learning\" + 0.022*\"vector\" + 0.022*\"logistic\" + 0.022*\"language\" + 0.022*\"analysis\" + 0.022*\"using\" + 0.022*\"course\" + 0.016*\"natural\"'),\n",
       " (1,\n",
       "  '0.037*\"course\" + 0.027*\"instructor\" + 0.027*\"aug\" + 0.016*\"machine\" + 0.016*\"helped\" + 0.016*\"analysis\" + 0.016*\"vector\" + 0.016*\"localitysensitive\" + 0.016*\"embeddings\" + 0.016*\"hashing\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fe173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:43:55,957 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 21:43:55,957 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 21:43:55,958 : INFO : using serial LDA version on this node\n",
      "2022-02-24 21:43:55,959 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 21:43:55,965 : INFO : -6.827 per-word bound, 113.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,966 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 21:43:55,971 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.030*\"machine\" + 0.027*\"deep\" + 0.027*\"specialization\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.020*\"youtube\"\n",
      "2022-02-24 21:43:55,971 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:55,973 : INFO : topic #2 (0.250): 0.047*\"instructor\" + 0.041*\"course\" + 0.026*\"curriculum\" + 0.026*\"developer\" + 0.022*\"vector\" + 0.020*\"aug\" + 0.018*\"word\" + 0.018*\"embeddings\" + 0.016*\"analysis\" + 0.015*\"localitysensitive\"\n",
      "2022-02-24 21:43:55,973 : INFO : topic #3 (0.250): 0.035*\"course\" + 0.033*\"specialization\" + 0.030*\"analysis\" + 0.029*\"word\" + 0.028*\"vector\" + 0.026*\"logistic\" + 0.024*\"using\" + 0.023*\"language\" + 0.019*\"build\" + 0.019*\"text\"\n",
      "2022-02-24 21:43:55,974 : INFO : topic diff=1.870460, rho=1.000000\n",
      "2022-02-24 21:43:55,977 : INFO : -5.384 per-word bound, 41.8 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,978 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 21:43:55,980 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.028*\"machine\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"specialization\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.024*\"youtube\"\n",
      "2022-02-24 21:43:55,981 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:55,982 : INFO : topic #2 (0.250): 0.050*\"course\" + 0.043*\"instructor\" + 0.032*\"aug\" + 0.024*\"curriculum\" + 0.024*\"developer\" + 0.021*\"vector\" + 0.020*\"embeddings\" + 0.020*\"word\" + 0.019*\"wasnnto\" + 0.019*\"good\"\n",
      "2022-02-24 21:43:55,982 : INFO : topic #3 (0.250): 0.036*\"specialization\" + 0.032*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.029*\"vector\" + 0.028*\"logistic\" + 0.027*\"using\" + 0.027*\"language\" + 0.020*\"build\" + 0.020*\"text\"\n",
      "2022-02-24 21:43:55,983 : INFO : topic diff=0.256473, rho=0.577350\n",
      "2022-02-24 21:43:55,988 : INFO : -5.210 per-word bound, 37.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:55,989 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 21:43:55,991 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.028*\"machine\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"specialization\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.026*\"youtube\"\n",
      "2022-02-24 21:43:55,992 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:55,993 : INFO : topic #2 (0.250): 0.053*\"course\" + 0.041*\"instructor\" + 0.036*\"aug\" + 0.023*\"curriculum\" + 0.023*\"developer\" + 0.021*\"vector\" + 0.021*\"embeddings\" + 0.021*\"word\" + 0.020*\"wasnnto\" + 0.020*\"good\"\n",
      "2022-02-24 21:43:55,993 : INFO : topic #3 (0.250): 0.037*\"specialization\" + 0.031*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.029*\"vector\" + 0.029*\"logistic\" + 0.028*\"using\" + 0.028*\"language\" + 0.020*\"build\" + 0.020*\"text\"\n",
      "2022-02-24 21:43:55,994 : INFO : topic diff=0.130330, rho=0.500000\n",
      "2022-02-24 21:43:55,999 : INFO : -5.170 per-word bound, 36.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,000 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 21:43:56,002 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.028*\"machine\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"specialization\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.026*\"youtube\"\n",
      "2022-02-24 21:43:56,003 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,003 : INFO : topic #2 (0.250): 0.055*\"course\" + 0.040*\"instructor\" + 0.037*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.021*\"embeddings\" + 0.021*\"word\" + 0.021*\"wasnnto\" + 0.021*\"good\"\n",
      "2022-02-24 21:43:56,004 : INFO : topic #3 (0.250): 0.038*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.029*\"logistic\" + 0.029*\"using\" + 0.029*\"language\" + 0.020*\"build\" + 0.020*\"text\"\n",
      "2022-02-24 21:43:56,005 : INFO : topic diff=0.071635, rho=0.447214\n",
      "2022-02-24 21:43:56,011 : INFO : -5.158 per-word bound, 35.7 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,012 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 21:43:56,015 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"machine\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\" + 0.027*\"youtube\"\n",
      "2022-02-24 21:43:56,016 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,017 : INFO : topic #2 (0.250): 0.055*\"course\" + 0.040*\"instructor\" + 0.038*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.021*\"embeddings\" + 0.021*\"word\" + 0.021*\"wasnnto\" + 0.021*\"good\"\n",
      "2022-02-24 21:43:56,017 : INFO : topic #3 (0.250): 0.038*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.029*\"logistic\" + 0.029*\"using\" + 0.029*\"language\" + 0.020*\"build\" + 0.020*\"text\"\n",
      "2022-02-24 21:43:56,018 : INFO : topic diff=0.041130, rho=0.408248\n",
      "2022-02-24 21:43:56,021 : INFO : -5.154 per-word bound, 35.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,021 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 21:43:56,027 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"machine\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\" + 0.027*\"helped\"\n",
      "2022-02-24 21:43:56,028 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,028 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.038*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.021*\"word\" + 0.021*\"wasnnto\" + 0.021*\"good\"\n",
      "2022-02-24 21:43:56,029 : INFO : topic #3 (0.250): 0.038*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.029*\"logistic\" + 0.029*\"using\" + 0.029*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n",
      "2022-02-24 21:43:56,030 : INFO : topic diff=0.024382, rho=0.377964\n",
      "2022-02-24 21:43:56,033 : INFO : -5.153 per-word bound, 35.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,033 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 21:43:56,035 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"machine\" + 0.027*\"designed\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\" + 0.027*\"helped\"\n",
      "2022-02-24 21:43:56,036 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:43:56,036 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.021*\"wasnnto\" + 0.021*\"good\"\n",
      "2022-02-24 21:43:56,037 : INFO : topic #3 (0.250): 0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.029*\"logistic\" + 0.029*\"using\" + 0.029*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n",
      "2022-02-24 21:43:56,037 : INFO : topic diff=0.014893, rho=0.353553\n",
      "2022-02-24 21:43:56,043 : INFO : -5.152 per-word bound, 35.6 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,043 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 21:43:56,046 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"machine\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\" + 0.027*\"helped\"\n",
      "2022-02-24 21:43:56,046 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,047 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.022*\"wasnnto\" + 0.022*\"localitysensitive\"\n",
      "2022-02-24 21:43:56,048 : INFO : topic #3 (0.250): 0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.030*\"logistic\" + 0.030*\"using\" + 0.029*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n",
      "2022-02-24 21:43:56,048 : INFO : topic diff=0.009358, rho=0.333333\n",
      "2022-02-24 21:43:56,051 : INFO : -5.152 per-word bound, 35.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,052 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 21:43:56,054 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"machine\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"helped\" + 0.027*\"specialization\"\n",
      "2022-02-24 21:43:56,055 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"paper\" + 0.034*\"tensorflow\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,056 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.022*\"wasnnto\" + 0.022*\"localitysensitive\"\n",
      "2022-02-24 21:43:56,057 : INFO : topic #3 (0.250): 0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.030*\"logistic\" + 0.030*\"using\" + 0.030*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n",
      "2022-02-24 21:43:56,057 : INFO : topic diff=0.006036, rho=0.316228\n",
      "2022-02-24 21:43:56,062 : INFO : -5.152 per-word bound, 35.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 21:43:56,063 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 21:43:56,065 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"machine\" + 0.027*\"helped\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\"\n",
      "2022-02-24 21:43:56,065 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,066 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.022*\"localitysensitive\" + 0.022*\"hashing\"\n",
      "2022-02-24 21:43:56,066 : INFO : topic #3 (0.250): 0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.030*\"logistic\" + 0.030*\"using\" + 0.030*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n",
      "2022-02-24 21:43:56,067 : INFO : topic diff=0.003986, rho=0.301511\n",
      "2022-02-24 21:43:56,068 : INFO : topic #0 (0.250): 0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"machine\" + 0.027*\"helped\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\"\n",
      "2022-02-24 21:43:56,068 : INFO : topic #1 (0.250): 0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"\n",
      "2022-02-24 21:43:56,069 : INFO : topic #2 (0.250): 0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.022*\"localitysensitive\" + 0.022*\"hashing\"\n",
      "2022-02-24 21:43:56,070 : INFO : topic #3 (0.250): 0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.030*\"logistic\" + 0.030*\"using\" + 0.030*\"language\" + 0.020*\"text\" + 0.020*\"build\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"learning\" + 0.027*\"deep\" + 0.027*\"designed\" + 0.027*\"machine\" + 0.027*\"helped\" + 0.027*\"taught\" + 0.027*\"expert\" + 0.027*\"informative\" + 0.027*\"fun\" + 0.027*\"specialization\"'),\n",
       " (1,\n",
       "  '0.034*\"google\" + 0.034*\"brain\" + 0.034*\"coauthor\" + 0.034*\"library\" + 0.034*\"ukasz\" + 0.034*\"staff\" + 0.034*\"tensorflow\" + 0.034*\"paper\" + 0.034*\"trax\" + 0.034*\"kaiser\"'),\n",
       " (2,\n",
       "  '0.056*\"course\" + 0.039*\"instructor\" + 0.039*\"aug\" + 0.022*\"curriculum\" + 0.022*\"developer\" + 0.022*\"vector\" + 0.022*\"embeddings\" + 0.022*\"word\" + 0.022*\"localitysensitive\" + 0.022*\"hashing\"'),\n",
       " (3,\n",
       "  '0.039*\"specialization\" + 0.030*\"course\" + 0.030*\"analysis\" + 0.030*\"word\" + 0.030*\"vector\" + 0.030*\"logistic\" + 0.030*\"using\" + 0.030*\"language\" + 0.020*\"text\" + 0.020*\"build\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b6dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77935955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  locality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0     rating  â â  students enrolled course   nat...\n",
       "1  by end specialization  designed nlp applicatio...\n",
       "2  this specialization designed taught two expert...\n",
       "3  younes bensouda mourri instructor ai stanford ...\n",
       "4  åukasz kaiser staff research scientist google...\n",
       "5  machine translation  word embeddings  locality...\n",
       "6  the lecture exciting detailed  though little h...\n",
       "7                          other  i informative fun \n",
       "8  from lesson sentiment analysis logistic regres...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b8af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â students course language processing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>specialization application perform sentiment a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes instructor ai stanford university learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture straight regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentiment analysis regression learn feature ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â students course language processing s...\n",
       "1  specialization application perform sentiment a...\n",
       "2                    specialization machine learning\n",
       "3  younes instructor ai stanford university learn...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings sentiment ...\n",
       "6                  lecture straight regression model\n",
       "7                                                fun\n",
       "8  sentiment analysis regression learn feature ve...\n",
       "9         instructor instructor curriculum developer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009ca2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>brain</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>coauthor</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  assignment  aug  awesome  bayes  brain  chatbot  \\\n",
       "0   0         1            0           0    0        0      1      0        0   \n",
       "1   0         1            1           0    0        0      0      0        1   \n",
       "2   0         0            0           0    0        0      0      0        0   \n",
       "3   1         0            0           0    0        0      0      0        0   \n",
       "4   0         0            0           0    0        0      0      1        0   \n",
       "5   0         1            0           1    1        1      0      0        0   \n",
       "6   0         0            0           0    0        0      0      0        0   \n",
       "7   0         0            0           0    0        0      0      0        0   \n",
       "8   0         1            0           0    0        0      0      0        0   \n",
       "9   0         0            0           0    0        0      0      0        0   \n",
       "\n",
       "   coauthor  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0         0  ...      1      0           0    2       2      1          1   \n",
       "1         0  ...      0      0           0    0       0      0          0   \n",
       "2         0  ...      0      0           0    0       0      0          0   \n",
       "3         0  ...      0      0           1    0       0      0          0   \n",
       "4         1  ...      0      1           0    0       0      0          0   \n",
       "5         0  ...      0      0           0    0       1      0          0   \n",
       "6         0  ...      0      0           0    0       0      0          0   \n",
       "7         0  ...      0      0           0    0       0      0          0   \n",
       "8         0  ...      1      0           0    0       1      0          0   \n",
       "9         0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e9ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32882e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:43:56,678 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 21:43:56,679 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 21:43:56,679 : INFO : using serial LDA version on this node\n",
      "2022-02-24 21:43:56,680 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 21:43:56,687 : INFO : -6.257 per-word bound, 76.5 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,688 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 21:43:56,694 : INFO : topic #0 (0.250): 0.041*\"instructor\" + 0.035*\"course\" + 0.035*\"word\" + 0.028*\"relationship\" + 0.028*\"vector\" + 0.028*\"processing\" + 0.027*\"space\" + 0.026*\"language\" + 0.025*\"specialization\" + 0.023*\"google\"\n",
      "2022-02-24 21:43:56,695 : INFO : topic #1 (0.250): 0.078*\"regression\" + 0.056*\"sentiment\" + 0.056*\"analysis\" + 0.033*\"straight\" + 0.033*\"model\" + 0.033*\"lecture\" + 0.032*\"vector\" + 0.031*\"learn\" + 0.030*\"tweet\" + 0.030*\"feature\"\n",
      "2022-02-24 21:43:56,695 : INFO : topic #2 (0.250): 0.077*\"course\" + 0.045*\"machine\" + 0.039*\"word\" + 0.035*\"specialization\" + 0.034*\"space\" + 0.033*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.030*\"sentiment\" + 0.030*\"analysis\"\n",
      "2022-02-24 21:43:56,696 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.036*\"instructor\" + 0.036*\"learning\" + 0.036*\"university\" + 0.036*\"stanford\" + 0.036*\"ai\" + 0.036*\"younes\" + 0.029*\"word\" + 0.028*\"course\" + 0.027*\"language\"\n",
      "2022-02-24 21:43:56,696 : INFO : topic diff=1.825686, rho=1.000000\n",
      "2022-02-24 21:43:56,700 : INFO : -4.966 per-word bound, 31.2 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,701 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 21:43:56,703 : INFO : topic #0 (0.250): 0.042*\"course\" + 0.042*\"word\" + 0.035*\"instructor\" + 0.031*\"relationship\" + 0.030*\"vector\" + 0.030*\"processing\" + 0.030*\"space\" + 0.030*\"language\" + 0.029*\"specialization\" + 0.029*\"use\"\n",
      "2022-02-24 21:43:56,703 : INFO : topic #1 (0.250): 0.082*\"regression\" + 0.057*\"sentiment\" + 0.057*\"analysis\" + 0.033*\"straight\" + 0.033*\"model\" + 0.033*\"lecture\" + 0.032*\"vector\" + 0.032*\"learn\" + 0.031*\"tweet\" + 0.031*\"feature\"\n",
      "2022-02-24 21:43:56,704 : INFO : topic #2 (0.250): 0.079*\"course\" + 0.051*\"machine\" + 0.035*\"word\" + 0.032*\"specialization\" + 0.032*\"space\" + 0.032*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.030*\"analysis\"\n",
      "2022-02-24 21:43:56,704 : INFO : topic #3 (0.250): 0.055*\"specialization\" + 0.044*\"learning\" + 0.044*\"instructor\" + 0.044*\"university\" + 0.044*\"stanford\" + 0.044*\"ai\" + 0.044*\"younes\" + 0.020*\"word\" + 0.020*\"course\" + 0.019*\"language\"\n",
      "2022-02-24 21:43:56,705 : INFO : topic diff=0.295810, rho=0.577350\n",
      "2022-02-24 21:43:56,709 : INFO : -4.844 per-word bound, 28.7 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,710 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 21:43:56,712 : INFO : topic #0 (0.250): 0.044*\"course\" + 0.044*\"word\" + 0.033*\"instructor\" + 0.031*\"relationship\" + 0.031*\"vector\" + 0.031*\"processing\" + 0.031*\"space\" + 0.031*\"language\" + 0.031*\"specialization\" + 0.030*\"use\"\n",
      "2022-02-24 21:43:56,713 : INFO : topic #1 (0.250): 0.083*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.033*\"straight\" + 0.033*\"model\" + 0.033*\"lecture\" + 0.032*\"vector\" + 0.032*\"learn\" + 0.032*\"tweet\" + 0.032*\"feature\"\n",
      "2022-02-24 21:43:56,714 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.053*\"machine\" + 0.033*\"word\" + 0.032*\"space\" + 0.031*\"specialization\" + 0.031*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,714 : INFO : topic #3 (0.250): 0.055*\"specialization\" + 0.049*\"learning\" + 0.048*\"instructor\" + 0.048*\"university\" + 0.048*\"stanford\" + 0.048*\"ai\" + 0.048*\"younes\" + 0.016*\"word\" + 0.016*\"course\" + 0.015*\"language\"\n",
      "2022-02-24 21:43:56,715 : INFO : topic diff=0.150439, rho=0.500000\n",
      "2022-02-24 21:43:56,719 : INFO : -4.815 per-word bound, 28.1 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,720 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 21:43:56,723 : INFO : topic #0 (0.250): 0.045*\"course\" + 0.045*\"word\" + 0.033*\"instructor\" + 0.032*\"relationship\" + 0.032*\"vector\" + 0.032*\"processing\" + 0.031*\"space\" + 0.031*\"language\" + 0.031*\"specialization\" + 0.031*\"use\"\n",
      "2022-02-24 21:43:56,724 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.033*\"straight\" + 0.033*\"lecture\" + 0.033*\"model\" + 0.032*\"vector\" + 0.032*\"learn\" + 0.032*\"tweet\" + 0.032*\"feature\"\n",
      "2022-02-24 21:43:56,724 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.054*\"machine\" + 0.032*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.031*\"specialization\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,725 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.051*\"learning\" + 0.051*\"instructor\" + 0.050*\"university\" + 0.050*\"stanford\" + 0.050*\"ai\" + 0.050*\"younes\" + 0.014*\"word\" + 0.013*\"course\" + 0.013*\"language\"\n",
      "2022-02-24 21:43:56,725 : INFO : topic diff=0.082441, rho=0.447214\n",
      "2022-02-24 21:43:56,730 : INFO : -4.805 per-word bound, 28.0 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,731 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 21:43:56,733 : INFO : topic #0 (0.250): 0.045*\"course\" + 0.045*\"word\" + 0.032*\"instructor\" + 0.032*\"relationship\" + 0.032*\"vector\" + 0.032*\"processing\" + 0.032*\"space\" + 0.032*\"specialization\" + 0.032*\"language\" + 0.031*\"use\"\n",
      "2022-02-24 21:43:56,734 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.033*\"straight\" + 0.033*\"lecture\" + 0.032*\"model\" + 0.032*\"vector\" + 0.032*\"learn\" + 0.032*\"tweet\" + 0.032*\"feature\"\n",
      "2022-02-24 21:43:56,735 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.032*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.031*\"embeddings\"\n",
      "2022-02-24 21:43:56,735 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.052*\"learning\" + 0.052*\"instructor\" + 0.051*\"university\" + 0.051*\"stanford\" + 0.051*\"ai\" + 0.051*\"younes\" + 0.012*\"word\" + 0.012*\"course\" + 0.012*\"language\"\n",
      "2022-02-24 21:43:56,736 : INFO : topic diff=0.046861, rho=0.408248\n",
      "2022-02-24 21:43:56,741 : INFO : -4.802 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,741 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 21:43:56,744 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"instructor\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"specialization\" + 0.032*\"processing\" + 0.032*\"space\" + 0.032*\"language\" + 0.032*\"use\"\n",
      "2022-02-24 21:43:56,744 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"model\" + 0.032*\"learn\" + 0.032*\"vector\" + 0.032*\"feature\" + 0.032*\"tweet\"\n",
      "2022-02-24 21:43:56,745 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,746 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.052*\"instructor\" + 0.052*\"university\" + 0.052*\"stanford\" + 0.052*\"ai\" + 0.052*\"younes\" + 0.012*\"word\" + 0.012*\"course\" + 0.012*\"language\"\n",
      "2022-02-24 21:43:56,746 : INFO : topic diff=0.027549, rho=0.377964\n",
      "2022-02-24 21:43:56,753 : INFO : -4.801 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,753 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 21:43:56,756 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"instructor\" + 0.032*\"specialization\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\" + 0.032*\"space\" + 0.032*\"language\" + 0.032*\"use\"\n",
      "2022-02-24 21:43:56,756 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"model\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"tweet\" + 0.032*\"specialization\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:43:56,758 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,759 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.052*\"university\" + 0.052*\"stanford\" + 0.052*\"ai\" + 0.052*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"\n",
      "2022-02-24 21:43:56,760 : INFO : topic diff=0.016731, rho=0.353553\n",
      "2022-02-24 21:43:56,764 : INFO : -4.801 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,765 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 21:43:56,768 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"specialization\" + 0.032*\"instructor\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"processing\" + 0.032*\"space\" + 0.032*\"language\" + 0.032*\"use\"\n",
      "2022-02-24 21:43:56,768 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"model\" + 0.032*\"specialization\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"tweet\"\n",
      "2022-02-24 21:43:56,769 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"space\" + 0.031*\"vector\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,769 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.053*\"university\" + 0.053*\"stanford\" + 0.053*\"ai\" + 0.053*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"\n",
      "2022-02-24 21:43:56,770 : INFO : topic diff=0.010473, rho=0.333333\n",
      "2022-02-24 21:43:56,776 : INFO : -4.801 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,777 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 21:43:56,780 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"specialization\" + 0.032*\"vector\" + 0.032*\"relationship\" + 0.032*\"language\" + 0.032*\"processing\" + 0.032*\"space\" + 0.032*\"instructor\" + 0.032*\"use\"\n",
      "2022-02-24 21:43:56,781 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"specialization\" + 0.032*\"learn\" + 0.032*\"model\" + 0.032*\"feature\" + 0.032*\"tweet\"\n",
      "2022-02-24 21:43:56,782 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"vector\" + 0.031*\"space\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,782 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.053*\"university\" + 0.053*\"stanford\" + 0.053*\"ai\" + 0.053*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"\n",
      "2022-02-24 21:43:56,783 : INFO : topic diff=0.006739, rho=0.316228\n",
      "2022-02-24 21:43:56,786 : INFO : -4.800 per-word bound, 27.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 21:43:56,786 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 21:43:56,792 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"specialization\" + 0.032*\"vector\" + 0.032*\"language\" + 0.032*\"relationship\" + 0.032*\"space\" + 0.032*\"processing\" + 0.032*\"use\" + 0.032*\"instructor\"\n",
      "2022-02-24 21:43:56,793 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"specialization\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"text\" + 0.032*\"tweet\"\n",
      "2022-02-24 21:43:56,794 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"vector\" + 0.031*\"space\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,795 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.053*\"university\" + 0.053*\"stanford\" + 0.053*\"ai\" + 0.053*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"\n",
      "2022-02-24 21:43:56,796 : INFO : topic diff=0.004443, rho=0.301511\n",
      "2022-02-24 21:43:56,796 : INFO : topic #0 (0.250): 0.046*\"word\" + 0.046*\"course\" + 0.032*\"specialization\" + 0.032*\"vector\" + 0.032*\"language\" + 0.032*\"relationship\" + 0.032*\"space\" + 0.032*\"processing\" + 0.032*\"use\" + 0.032*\"instructor\"\n",
      "2022-02-24 21:43:56,797 : INFO : topic #1 (0.250): 0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"specialization\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"text\" + 0.032*\"tweet\"\n",
      "2022-02-24 21:43:56,797 : INFO : topic #2 (0.250): 0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"vector\" + 0.031*\"space\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"\n",
      "2022-02-24 21:43:56,798 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.053*\"university\" + 0.053*\"stanford\" + 0.053*\"ai\" + 0.053*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.046*\"word\" + 0.046*\"course\" + 0.032*\"specialization\" + 0.032*\"vector\" + 0.032*\"language\" + 0.032*\"relationship\" + 0.032*\"space\" + 0.032*\"processing\" + 0.032*\"use\" + 0.032*\"instructor\"'),\n",
       " (1,\n",
       "  '0.084*\"regression\" + 0.058*\"sentiment\" + 0.058*\"analysis\" + 0.032*\"specialization\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"text\" + 0.032*\"tweet\"'),\n",
       " (2,\n",
       "  '0.080*\"course\" + 0.055*\"machine\" + 0.031*\"word\" + 0.031*\"vector\" + 0.031*\"space\" + 0.031*\"translation\" + 0.031*\"rating\" + 0.031*\"sentiment\" + 0.031*\"embeddings\" + 0.031*\"analysis\"'),\n",
       " (3,\n",
       "  '0.054*\"specialization\" + 0.053*\"learning\" + 0.053*\"instructor\" + 0.053*\"university\" + 0.053*\"stanford\" + 0.053*\"ai\" + 0.053*\"younes\" + 0.011*\"word\" + 0.011*\"course\" + 0.011*\"language\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d643340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924e6236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â â students course natural language pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end specialization nlp application perform sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization expert nlp machine deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes mourri instructor ai stanford universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings localityse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture detailed little hard straight helped r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lesson sentiment analysis logistic regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â â students course natural language pr...\n",
       "1  end specialization nlp application perform sen...\n",
       "2    specialization expert nlp machine deep learning\n",
       "3  younes mourri instructor ai stanford universit...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings localityse...\n",
       "6  lecture detailed little hard straight helped r...\n",
       "7                            other i informative fun\n",
       "8  lesson sentiment analysis logistic regression ...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d43240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>best</th>\n",
       "      <th>binary</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  approximate  assignment  aug  awesome  bayes  \\\n",
       "0   0         1            0            1           0    0        0      1   \n",
       "1   0         1            1            0           0    0        0      0   \n",
       "2   0         0            0            0           0    0        0      0   \n",
       "3   1         0            0            0           0    0        0      0   \n",
       "4   0         0            0            0           0    0        0      0   \n",
       "5   0         1            0            0           1    1        1      0   \n",
       "6   0         0            0            0           0    0        0      0   \n",
       "7   0         0            0            0           0    0        0      0   \n",
       "8   0         1            0            0           0    0        0      0   \n",
       "9   0         0            0            0           0    0        0      0   \n",
       "\n",
       "   best  binary  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0     0       0  ...      1      0           0    2       2      1          1   \n",
       "1     0       0  ...      0      0           0    0       0      0          0   \n",
       "2     0       0  ...      0      0           0    0       0      0          0   \n",
       "3     0       0  ...      0      0           1    0       0      0          0   \n",
       "4     0       0  ...      0      1           0    0       0      0          0   \n",
       "5     1       0  ...      0      0           0    0       1      0          0   \n",
       "6     0       0  ...      0      0           0    0       0      0          0   \n",
       "7     0       0  ...      0      0           0    0       0      0          0   \n",
       "8     0       1  ...      1      0           0    0       1      0          0   \n",
       "9     0       0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 98 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4daee25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,600 : INFO : using symmetric alpha at 0.2\n",
      "2022-02-24 21:44:00,602 : INFO : using symmetric eta at 0.2\n",
      "2022-02-24 21:44:00,602 : INFO : using serial LDA version on this node\n",
      "2022-02-24 21:44:00,603 : INFO : running online (multi-pass) LDA training, 5 topics, 80 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 21:44:00,610 : INFO : -7.529 per-word bound, 184.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,611 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 21:44:00,616 : INFO : topic #0 (0.200): 0.054*\"specialization\" + 0.041*\"nlp\" + 0.035*\"language\" + 0.029*\"perform\" + 0.029*\"analysis\" + 0.029*\"sentiment\" + 0.022*\"text\" + 0.022*\"tool\" + 0.022*\"translate\" + 0.022*\"application\"\n",
      "2022-02-24 21:44:00,617 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"specialization\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"regression\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\"\n",
      "2022-02-24 21:44:00,618 : INFO : topic #2 (0.200): 0.035*\"google\" + 0.035*\"scientist\" + 0.035*\"staff\" + 0.035*\"ukasz\" + 0.035*\"tensorflow\" + 0.035*\"library\" + 0.035*\"brain\" + 0.035*\"coauthor\" + 0.035*\"research\" + 0.035*\"transformer\"\n",
      "2022-02-24 21:44:00,619 : INFO : topic #3 (0.200): 0.051*\"regression\" + 0.050*\"logistic\" + 0.036*\"vector\" + 0.030*\"tweet\" + 0.029*\"analysis\" + 0.029*\"sentiment\" + 0.025*\"text\" + 0.025*\"lesson\" + 0.025*\"learn\" + 0.025*\"binary\"\n",
      "2022-02-24 21:44:00,620 : INFO : topic #4 (0.200): 0.071*\"course\" + 0.037*\"word\" + 0.033*\"space\" + 0.031*\"vector\" + 0.025*\"sentiment\" + 0.025*\"analysis\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,620 : INFO : topic diff=2.785831, rho=1.000000\n",
      "2022-02-24 21:44:00,624 : INFO : -5.342 per-word bound, 40.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,625 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 21:44:00,629 : INFO : topic #0 (0.200): 0.064*\"specialization\" + 0.046*\"nlp\" + 0.039*\"language\" + 0.034*\"perform\" + 0.030*\"analysis\" + 0.030*\"sentiment\" + 0.025*\"tool\" + 0.025*\"translate\" + 0.025*\"text\" + 0.025*\"application\"\n",
      "2022-02-24 21:44:00,631 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"model\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,632 : INFO : topic #2 (0.200): 0.035*\"google\" + 0.035*\"scientist\" + 0.035*\"staff\" + 0.035*\"ukasz\" + 0.035*\"tensorflow\" + 0.035*\"library\" + 0.035*\"brain\" + 0.035*\"coauthor\" + 0.035*\"research\" + 0.035*\"transformer\"\n",
      "2022-02-24 21:44:00,633 : INFO : topic #3 (0.200): 0.056*\"regression\" + 0.056*\"logistic\" + 0.035*\"vector\" + 0.032*\"tweet\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.029*\"text\" + 0.029*\"lesson\" + 0.029*\"learn\" + 0.029*\"binary\"\n",
      "2022-02-24 21:44:00,634 : INFO : topic #4 (0.200): 0.070*\"course\" + 0.042*\"word\" + 0.035*\"space\" + 0.034*\"vector\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"sentiment\" + 0.024*\"analysis\"\n",
      "2022-02-24 21:44:00,634 : INFO : topic diff=0.229288, rho=0.577350\n",
      "2022-02-24 21:44:00,640 : INFO : -5.179 per-word bound, 36.2 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,641 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 21:44:00,643 : INFO : topic #0 (0.200): 0.061*\"specialization\" + 0.051*\"nlp\" + 0.036*\"language\" + 0.033*\"perform\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.028*\"tool\" + 0.028*\"translate\" + 0.028*\"application\" + 0.028*\"end\"\n",
      "2022-02-24 21:44:00,644 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"model\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,645 : INFO : topic #2 (0.200): 0.035*\"google\" + 0.035*\"scientist\" + 0.035*\"staff\" + 0.035*\"ukasz\" + 0.035*\"tensorflow\" + 0.035*\"library\" + 0.035*\"brain\" + 0.035*\"coauthor\" + 0.035*\"research\" + 0.035*\"transformer\"\n",
      "2022-02-24 21:44:00,646 : INFO : topic #3 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.034*\"vector\" + 0.033*\"tweet\" + 0.033*\"analysis\" + 0.032*\"sentiment\" + 0.031*\"text\" + 0.031*\"lesson\" + 0.031*\"learn\" + 0.031*\"binary\"\n",
      "2022-02-24 21:44:00,646 : INFO : topic #4 (0.200): 0.067*\"course\" + 0.043*\"word\" + 0.034*\"space\" + 0.034*\"vector\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,646 : INFO : topic diff=0.163019, rho=0.500000\n",
      "2022-02-24 21:44:00,651 : INFO : -5.110 per-word bound, 34.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,653 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 21:44:00,657 : INFO : topic #0 (0.200): 0.060*\"specialization\" + 0.054*\"nlp\" + 0.034*\"language\" + 0.033*\"perform\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.030*\"tool\" + 0.030*\"translate\" + 0.030*\"application\" + 0.030*\"end\"\n",
      "2022-02-24 21:44:00,658 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"model\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,659 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,659 : INFO : topic #3 (0.200): 0.060*\"regression\" + 0.060*\"logistic\" + 0.034*\"vector\" + 0.033*\"tweet\" + 0.033*\"analysis\" + 0.033*\"sentiment\" + 0.032*\"text\" + 0.032*\"lesson\" + 0.032*\"learn\" + 0.032*\"binary\"\n",
      "2022-02-24 21:44:00,660 : INFO : topic #4 (0.200): 0.066*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.034*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,661 : INFO : topic diff=0.097678, rho=0.447214\n",
      "2022-02-24 21:44:00,666 : INFO : -5.086 per-word bound, 34.0 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,667 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 21:44:00,670 : INFO : topic #0 (0.200): 0.060*\"specialization\" + 0.056*\"nlp\" + 0.033*\"language\" + 0.032*\"perform\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"tool\" + 0.031*\"translate\" + 0.031*\"application\" + 0.031*\"end\"\n",
      "2022-02-24 21:44:00,672 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"detailed\" + 0.029*\"model\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,673 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,674 : INFO : topic #3 (0.200): 0.061*\"regression\" + 0.061*\"logistic\" + 0.034*\"vector\" + 0.033*\"tweet\" + 0.033*\"analysis\" + 0.033*\"sentiment\" + 0.033*\"text\" + 0.033*\"lesson\" + 0.033*\"learn\" + 0.033*\"binary\"\n",
      "2022-02-24 21:44:00,675 : INFO : topic #4 (0.200): 0.066*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.034*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,675 : INFO : topic diff=0.060054, rho=0.408248\n",
      "2022-02-24 21:44:00,678 : INFO : -5.078 per-word bound, 33.8 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,679 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 21:44:00,681 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.057*\"nlp\" + 0.033*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.031*\"tool\" + 0.031*\"translate\" + 0.031*\"application\" + 0.031*\"end\"\n",
      "2022-02-24 21:44:00,681 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\" + 0.029*\"little\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,682 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,682 : INFO : topic #3 (0.200): 0.061*\"regression\" + 0.061*\"logistic\" + 0.034*\"vector\" + 0.033*\"tweet\" + 0.033*\"analysis\" + 0.033*\"sentiment\" + 0.033*\"text\" + 0.033*\"lesson\" + 0.033*\"learn\" + 0.033*\"binary\"\n",
      "2022-02-24 21:44:00,683 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,683 : INFO : topic diff=0.037354, rho=0.377964\n",
      "2022-02-24 21:44:00,688 : INFO : -5.074 per-word bound, 33.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,689 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 21:44:00,691 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.057*\"nlp\" + 0.033*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.031*\"tool\" + 0.031*\"translate\" + 0.031*\"application\" + 0.031*\"machine\"\n",
      "2022-02-24 21:44:00,692 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,693 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,693 : INFO : topic #3 (0.200): 0.061*\"regression\" + 0.061*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.033*\"text\" + 0.033*\"lesson\" + 0.033*\"learn\" + 0.033*\"binary\"\n",
      "2022-02-24 21:44:00,694 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,694 : INFO : topic diff=0.023572, rho=0.353553\n",
      "2022-02-24 21:44:00,697 : INFO : -5.073 per-word bound, 33.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,698 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 21:44:00,700 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,701 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,701 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,702 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.033*\"text\" + 0.033*\"lesson\" + 0.033*\"learn\" + 0.033*\"extract\"\n",
      "2022-02-24 21:44:00,703 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,704 : INFO : topic diff=0.015138, rho=0.333333\n",
      "2022-02-24 21:44:00,708 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,709 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 21:44:00,711 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,711 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"detailed\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"straight\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,712 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,713 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:00,713 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.034*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,714 : INFO : topic diff=0.009907, rho=0.316228\n",
      "2022-02-24 21:44:00,717 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,718 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 21:44:00,721 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,721 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"straight\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,722 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"ukasz\" + 0.036*\"staff\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,723 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,723 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,724 : INFO : topic diff=0.006607, rho=0.301511\n",
      "2022-02-24 21:44:00,728 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,728 : INFO : PROGRESS: pass 10, at document #10/10\n",
      "2022-02-24 21:44:00,731 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,731 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"little\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,732 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,732 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,733 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,733 : INFO : topic diff=0.004486, rho=0.288675\n",
      "2022-02-24 21:44:00,737 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,738 : INFO : PROGRESS: pass 11, at document #10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,740 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,740 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"mourri\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,741 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"brain\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,741 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,742 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,742 : INFO : topic diff=0.003098, rho=0.277350\n",
      "2022-02-24 21:44:00,746 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,746 : INFO : PROGRESS: pass 12, at document #10/10\n",
      "2022-02-24 21:44:00,748 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,749 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,749 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"research\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,750 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,750 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,751 : INFO : topic diff=0.002174, rho=0.267261\n",
      "2022-02-24 21:44:00,755 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,755 : INFO : PROGRESS: pass 13, at document #10/10\n",
      "2022-02-24 21:44:00,757 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,758 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,758 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"library\" + 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,759 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,760 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,760 : INFO : topic diff=0.001547, rho=0.258199\n",
      "2022-02-24 21:44:00,763 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,764 : INFO : PROGRESS: pass 14, at document #10/10\n",
      "2022-02-24 21:44:00,766 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,766 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,767 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"ukasz\" + 0.036*\"staff\" + 0.036*\"library\" + 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,767 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"sentiment\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,767 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,768 : INFO : topic diff=0.001115, rho=0.250000\n",
      "2022-02-24 21:44:00,772 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,772 : INFO : PROGRESS: pass 15, at document #10/10\n",
      "2022-02-24 21:44:00,774 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,776 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,776 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"coauthor\" + 0.036*\"brain\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,777 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"vector\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,777 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,778 : INFO : topic diff=0.000813, rho=0.242536\n",
      "2022-02-24 21:44:00,781 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,781 : INFO : PROGRESS: pass 16, at document #10/10\n",
      "2022-02-24 21:44:00,783 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,784 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"learning\" + 0.029*\"deep\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,784 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"ukasz\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,785 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,786 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"space\" + 0.033*\"vector\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,786 : INFO : topic diff=0.000600, rho=0.235702\n",
      "2022-02-24 21:44:00,789 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,789 : INFO : PROGRESS: pass 17, at document #10/10\n",
      "2022-02-24 21:44:00,792 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,792 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,793 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"ukasz\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,793 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"extract\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:00,794 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,794 : INFO : topic diff=0.000447, rho=0.229416\n",
      "2022-02-24 21:44:00,798 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,798 : INFO : PROGRESS: pass 18, at document #10/10\n",
      "2022-02-24 21:44:00,800 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,801 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,801 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"ukasz\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"transformer\"\n",
      "2022-02-24 21:44:00,802 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"vector\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,803 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,803 : INFO : topic diff=0.000336, rho=0.223607\n",
      "2022-02-24 21:44:00,806 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,807 : INFO : PROGRESS: pass 19, at document #10/10\n",
      "2022-02-24 21:44:00,809 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"language\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"tool\"\n",
      "2022-02-24 21:44:00,810 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"straight\" + 0.029*\"little\"\n",
      "2022-02-24 21:44:00,810 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"staff\" + 0.036*\"scientist\" + 0.036*\"library\" + 0.036*\"tensorflow\" + 0.036*\"coauthor\" + 0.036*\"transformer\" + 0.036*\"trax\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,810 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"vector\" + 0.034*\"lesson\" + 0.034*\"learn\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,811 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,811 : INFO : topic diff=0.000255, rho=0.218218\n",
      "2022-02-24 21:44:00,815 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,816 : INFO : PROGRESS: pass 20, at document #10/10\n",
      "2022-02-24 21:44:00,818 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"perform\" + 0.032*\"language\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\" + 0.032*\"end\" + 0.032*\"chatbot\"\n",
      "2022-02-24 21:44:00,818 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,819 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"staff\" + 0.036*\"scientist\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"library\" + 0.036*\"trax\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,819 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"vector\" + 0.034*\"feature\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,820 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,820 : INFO : topic diff=0.000195, rho=0.213201\n",
      "2022-02-24 21:44:00,824 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,824 : INFO : PROGRESS: pass 21, at document #10/10\n",
      "2022-02-24 21:44:00,827 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.058*\"nlp\" + 0.032*\"perform\" + 0.032*\"language\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"end\"\n",
      "2022-02-24 21:44:00,827 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"helped\"\n",
      "2022-02-24 21:44:00,828 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"library\" + 0.036*\"coauthor\" + 0.036*\"brain\" + 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,828 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"vector\" + 0.034*\"feature\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,829 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,830 : INFO : topic diff=0.000150, rho=0.208514\n",
      "2022-02-24 21:44:00,833 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,834 : INFO : PROGRESS: pass 22, at document #10/10\n",
      "2022-02-24 21:44:00,836 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"language\" + 0.032*\"machine\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"end\"\n",
      "2022-02-24 21:44:00,836 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"ai\" + 0.029*\"mourri\" + 0.029*\"detailed\" + 0.029*\"helped\"\n",
      "2022-02-24 21:44:00,837 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"staff\" + 0.036*\"scientist\" + 0.036*\"ukasz\" + 0.036*\"paper\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"trax\"\n",
      "2022-02-24 21:44:00,837 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"learn\" + 0.034*\"feature\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,838 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,838 : INFO : topic diff=0.000116, rho=0.204124\n",
      "2022-02-24 21:44:00,842 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,842 : INFO : PROGRESS: pass 23, at document #10/10\n",
      "2022-02-24 21:44:00,844 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"language\" + 0.032*\"machine\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\" + 0.032*\"end\" + 0.032*\"chatbot\"\n",
      "2022-02-24 21:44:00,845 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,845 : INFO : topic #2 (0.200): 0.036*\"staff\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"paper\" + 0.036*\"library\" + 0.036*\"brain\" + 0.036*\"coauthor\" + 0.036*\"research\" + 0.036*\"trax\"\n",
      "2022-02-24 21:44:00,846 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"learn\"\n",
      "2022-02-24 21:44:00,847 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\" + 0.023*\"analysis\"\n",
      "2022-02-24 21:44:00,847 : INFO : topic diff=0.000091, rho=0.200000\n",
      "2022-02-24 21:44:00,851 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,851 : INFO : PROGRESS: pass 24, at document #10/10\n",
      "2022-02-24 21:44:00,854 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"language\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"chatbot\"\n",
      "2022-02-24 21:44:00,854 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,855 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"tensorflow\" + 0.036*\"trax\" + 0.036*\"brain\" + 0.036*\"library\" + 0.036*\"transformer\" + 0.036*\"scientist\" + 0.036*\"paper\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,855 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"learn\"\n",
      "2022-02-24 21:44:00,856 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\"\n",
      "2022-02-24 21:44:00,857 : INFO : topic diff=0.000071, rho=0.196116\n",
      "2022-02-24 21:44:00,860 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,861 : INFO : PROGRESS: pass 25, at document #10/10\n",
      "2022-02-24 21:44:00,863 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"language\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,864 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"straight\"\n",
      "2022-02-24 21:44:00,864 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"trax\" + 0.036*\"ukasz\" + 0.036*\"brain\" + 0.036*\"staff\" + 0.036*\"transformer\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"research\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,864 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"learn\"\n",
      "2022-02-24 21:44:00,865 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"sentiment\"\n",
      "2022-02-24 21:44:00,866 : INFO : topic diff=0.000056, rho=0.192450\n",
      "2022-02-24 21:44:00,869 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,870 : INFO : PROGRESS: pass 26, at document #10/10\n",
      "2022-02-24 21:44:00,872 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"language\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\"\n",
      "2022-02-24 21:44:00,872 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"helped\"\n",
      "2022-02-24 21:44:00,873 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"trax\" + 0.036*\"scientist\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"tensorflow\" + 0.036*\"staff\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 21:44:00,873 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,874 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"translation\" + 0.023*\"language\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,874 : INFO : topic diff=0.000045, rho=0.188982\n",
      "2022-02-24 21:44:00,878 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,878 : INFO : PROGRESS: pass 27, at document #10/10\n",
      "2022-02-24 21:44:00,880 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"tool\" + 0.032*\"application\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,880 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"helped\"\n",
      "2022-02-24 21:44:00,881 : INFO : topic #2 (0.200): 0.036*\"google\" + 0.036*\"transformer\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"tensorflow\" + 0.036*\"brain\" + 0.036*\"staff\" + 0.036*\"trax\" + 0.036*\"ukasz\"\n",
      "2022-02-24 21:44:00,881 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,882 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,882 : INFO : topic diff=0.000035, rho=0.185695\n",
      "2022-02-24 21:44:00,885 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,886 : INFO : PROGRESS: pass 28, at document #10/10\n",
      "2022-02-24 21:44:00,889 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"perform\" + 0.032*\"machine\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,889 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"detailed\" + 0.029*\"helped\"\n",
      "2022-02-24 21:44:00,890 : INFO : topic #2 (0.200): 0.036*\"trax\" + 0.036*\"transformer\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"tensorflow\" + 0.036*\"brain\" + 0.036*\"staff\" + 0.036*\"ukasz\" + 0.036*\"library\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,890 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"learn\"\n",
      "2022-02-24 21:44:00,891 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,891 : INFO : topic diff=0.000028, rho=0.182574\n",
      "2022-02-24 21:44:00,896 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,896 : INFO : PROGRESS: pass 29, at document #10/10\n",
      "2022-02-24 21:44:00,899 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,900 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"detailed\"\n",
      "2022-02-24 21:44:00,900 : INFO : topic #2 (0.200): 0.036*\"transformer\" + 0.036*\"paper\" + 0.036*\"scientist\" + 0.036*\"tensorflow\" + 0.036*\"brain\" + 0.036*\"research\" + 0.036*\"staff\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"coauthor\"\n",
      "2022-02-24 21:44:00,901 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"feature\" + 0.034*\"classifier\" + 0.034*\"lesson\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:00,902 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,902 : INFO : topic diff=0.000023, rho=0.179605\n",
      "2022-02-24 21:44:00,906 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,906 : INFO : PROGRESS: pass 30, at document #10/10\n",
      "2022-02-24 21:44:00,908 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,909 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"helped\" + 0.029*\"detailed\"\n",
      "2022-02-24 21:44:00,910 : INFO : topic #2 (0.200): 0.036*\"transformer\" + 0.036*\"trax\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"tensorflow\" + 0.036*\"scientist\" + 0.036*\"paper\" + 0.036*\"library\" + 0.036*\"coauthor\" + 0.036*\"google\"\n",
      "2022-02-24 21:44:00,910 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"classifier\" + 0.034*\"learn\"\n",
      "2022-02-24 21:44:00,911 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,911 : INFO : topic diff=0.000018, rho=0.176777\n",
      "2022-02-24 21:44:00,914 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,915 : INFO : PROGRESS: pass 31, at document #10/10\n",
      "2022-02-24 21:44:00,917 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,918 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"helped\" + 0.029*\"detailed\"\n",
      "2022-02-24 21:44:00,918 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"library\" + 0.036*\"ukasz\"\n",
      "2022-02-24 21:44:00,918 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:00,919 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,919 : INFO : topic diff=0.000015, rho=0.174078\n",
      "2022-02-24 21:44:00,924 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,924 : INFO : PROGRESS: pass 32, at document #10/10\n",
      "2022-02-24 21:44:00,927 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,928 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"helped\" + 0.029*\"detailed\"\n",
      "2022-02-24 21:44:00,928 : INFO : topic #2 (0.200): 0.036*\"transformer\" + 0.036*\"tensorflow\" + 0.036*\"staff\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"library\" + 0.036*\"ukasz\" + 0.036*\"trax\"\n",
      "2022-02-24 21:44:00,929 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"classifier\" + 0.034*\"numerical\" + 0.034*\"lesson\"\n",
      "2022-02-24 21:44:00,930 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,930 : INFO : topic diff=0.000012, rho=0.171499\n",
      "2022-02-24 21:44:00,933 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,934 : INFO : PROGRESS: pass 33, at document #10/10\n",
      "2022-02-24 21:44:00,936 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,937 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"university\" + 0.029*\"younes\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"helped\" + 0.029*\"detailed\"\n",
      "2022-02-24 21:44:00,937 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,938 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"extract\" + 0.034*\"feature\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,939 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"embeddings\" + 0.023*\"rating\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,939 : INFO : topic diff=0.000010, rho=0.169031\n",
      "2022-02-24 21:44:00,942 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,943 : INFO : PROGRESS: pass 34, at document #10/10\n",
      "2022-02-24 21:44:00,945 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,945 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,946 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,946 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"feature\" + 0.034*\"learn\" + 0.034*\"lesson\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,947 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,947 : INFO : topic diff=0.000008, rho=0.166667\n",
      "2022-02-24 21:44:00,950 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,951 : INFO : PROGRESS: pass 35, at document #10/10\n",
      "2022-02-24 21:44:00,953 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,954 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,954 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,955 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"feature\" + 0.034*\"classifier\" + 0.034*\"learn\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:00,955 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"embeddings\" + 0.023*\"rating\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,956 : INFO : topic diff=0.000007, rho=0.164399\n",
      "2022-02-24 21:44:00,958 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,959 : INFO : PROGRESS: pass 36, at document #10/10\n",
      "2022-02-24 21:44:00,962 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,963 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,963 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,963 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"feature\" + 0.034*\"classifier\" + 0.034*\"learn\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:00,964 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,965 : INFO : topic diff=0.000005, rho=0.162221\n",
      "2022-02-24 21:44:00,968 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,968 : INFO : PROGRESS: pass 37, at document #10/10\n",
      "2022-02-24 21:44:00,971 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,971 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,972 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,972 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"classifier\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 21:44:00,973 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,973 : INFO : topic diff=0.000005, rho=0.160128\n",
      "2022-02-24 21:44:00,976 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,977 : INFO : PROGRESS: pass 38, at document #10/10\n",
      "2022-02-24 21:44:00,979 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,980 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,980 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"research\" + 0.036*\"paper\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,981 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,981 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,982 : INFO : topic diff=0.000004, rho=0.158114\n",
      "2022-02-24 21:44:00,984 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,985 : INFO : PROGRESS: pass 39, at document #10/10\n",
      "2022-02-24 21:44:00,987 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:00,987 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,988 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,988 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"analysis\" + 0.034*\"sentiment\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"numerical\" + 0.034*\"extract\" + 0.034*\"feature\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:00,989 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:00,989 : INFO : topic diff=0.000003, rho=0.156174\n",
      "2022-02-24 21:44:00,994 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:00,995 : INFO : PROGRESS: pass 40, at document #10/10\n",
      "2022-02-24 21:44:00,998 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:00,998 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:00,999 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:00,999 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"classifier\" + 0.034*\"lesson\" + 0.034*\"extract\" + 0.034*\"feature\"\n",
      "2022-02-24 21:44:01,000 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"embeddings\" + 0.023*\"rating\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,001 : INFO : topic diff=0.000003, rho=0.154303\n",
      "2022-02-24 21:44:01,004 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,005 : INFO : PROGRESS: pass 41, at document #10/10\n",
      "2022-02-24 21:44:01,007 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,007 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,008 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,008 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"classifier\" + 0.034*\"extract\" + 0.034*\"lesson\" + 0.034*\"numerical\"\n",
      "2022-02-24 21:44:01,009 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,009 : INFO : topic diff=0.000002, rho=0.152499\n",
      "2022-02-24 21:44:01,013 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,014 : INFO : PROGRESS: pass 42, at document #10/10\n",
      "2022-02-24 21:44:01,017 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,017 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,018 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,018 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"classifier\" + 0.034*\"extract\" + 0.034*\"feature\" + 0.034*\"numerical\"\n",
      "2022-02-24 21:44:01,019 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"translation\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,020 : INFO : topic diff=0.000002, rho=0.150756\n",
      "2022-02-24 21:44:01,023 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,024 : INFO : PROGRESS: pass 43, at document #10/10\n",
      "2022-02-24 21:44:01,027 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"chatbot\" + 0.032*\"application\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,027 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,028 : INFO : topic #2 (0.200): 0.036*\"paper\" + 0.036*\"brain\" + 0.036*\"google\" + 0.036*\"scientist\" + 0.036*\"coauthor\" + 0.036*\"staff\" + 0.036*\"research\" + 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"ukasz\"\n",
      "2022-02-24 21:44:01,028 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"feature\" + 0.034*\"lesson\" + 0.034*\"classifier\" + 0.034*\"numerical\"\n",
      "2022-02-24 21:44:01,028 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"translation\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,029 : INFO : topic diff=0.000002, rho=0.149071\n",
      "2022-02-24 21:44:01,033 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,033 : INFO : PROGRESS: pass 44, at document #10/10\n",
      "2022-02-24 21:44:01,035 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,036 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,036 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,037 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"classifier\" + 0.034*\"lesson\" + 0.034*\"binary\"\n",
      "2022-02-24 21:44:01,037 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"translation\" + 0.023*\"rating\" + 0.023*\"embeddings\" + 0.023*\"natural\"\n",
      "2022-02-24 21:44:01,038 : INFO : topic diff=0.000001, rho=0.147442\n",
      "2022-02-24 21:44:01,041 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,041 : INFO : PROGRESS: pass 45, at document #10/10\n",
      "2022-02-24 21:44:01,044 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,045 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,045 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,046 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"extract\" + 0.034*\"numerical\" + 0.034*\"feature\" + 0.034*\"lesson\"\n",
      "2022-02-24 21:44:01,046 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"translation\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,047 : INFO : topic diff=0.000001, rho=0.145865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,050 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,050 : INFO : PROGRESS: pass 46, at document #10/10\n",
      "2022-02-24 21:44:01,054 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"translate\" + 0.032*\"chatbot\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,054 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,055 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,055 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"numerical\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:01,056 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"translation\" + 0.023*\"processing\" + 0.023*\"use\"\n",
      "2022-02-24 21:44:01,057 : INFO : topic diff=0.000001, rho=0.144338\n",
      "2022-02-24 21:44:01,061 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,062 : INFO : PROGRESS: pass 47, at document #10/10\n",
      "2022-02-24 21:44:01,064 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"chatbot\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,064 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,065 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,065 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"numerical\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:01,066 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"localitysensitive\"\n",
      "2022-02-24 21:44:01,066 : INFO : topic diff=0.000001, rho=0.142857\n",
      "2022-02-24 21:44:01,070 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,070 : INFO : PROGRESS: pass 48, at document #10/10\n",
      "2022-02-24 21:44:01,073 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,074 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,074 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,074 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"numerical\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:01,075 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,075 : INFO : topic diff=0.000001, rho=0.141421\n",
      "2022-02-24 21:44:01,079 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,079 : INFO : PROGRESS: pass 49, at document #10/10\n",
      "2022-02-24 21:44:01,082 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"translate\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,082 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,083 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,083 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"numerical\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:01,084 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,085 : INFO : topic diff=0.000001, rho=0.140028\n",
      "2022-02-24 21:44:01,088 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,088 : INFO : PROGRESS: pass 50, at document #10/10\n",
      "2022-02-24 21:44:01,091 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"end\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,091 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,092 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,092 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"numerical\" + 0.034*\"learn\" + 0.034*\"extract\"\n",
      "2022-02-24 21:44:01,093 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,093 : INFO : topic diff=0.000001, rho=0.138675\n",
      "2022-02-24 21:44:01,097 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,097 : INFO : PROGRESS: pass 51, at document #10/10\n",
      "2022-02-24 21:44:01,100 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"chatbot\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,100 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,101 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,101 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,102 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,103 : INFO : topic diff=0.000001, rho=0.137361\n",
      "2022-02-24 21:44:01,108 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,108 : INFO : PROGRESS: pass 52, at document #10/10\n",
      "2022-02-24 21:44:01,111 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"chatbot\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,111 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,112 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,112 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n",
      "2022-02-24 21:44:01,113 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,113 : INFO : topic diff=0.000000, rho=0.136083\n",
      "2022-02-24 21:44:01,116 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,117 : INFO : PROGRESS: pass 53, at document #10/10\n",
      "2022-02-24 21:44:01,120 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"chatbot\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,120 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,121 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,121 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n",
      "2022-02-24 21:44:01,122 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"processing\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,122 : INFO : topic diff=0.000000, rho=0.134840\n",
      "2022-02-24 21:44:01,126 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,126 : INFO : PROGRESS: pass 54, at document #10/10\n",
      "2022-02-24 21:44:01,128 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"chatbot\" + 0.032*\"tool\" + 0.032*\"end\" + 0.032*\"translate\" + 0.032*\"application\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,129 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,129 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,130 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"learn\" + 0.034*\"numerical\" + 0.034*\"lesson\" + 0.034*\"feature\"\n",
      "2022-02-24 21:44:01,131 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"processing\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,131 : INFO : topic diff=0.000000, rho=0.133631\n",
      "2022-02-24 21:44:01,134 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,134 : INFO : PROGRESS: pass 55, at document #10/10\n",
      "2022-02-24 21:44:01,138 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,139 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,139 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,140 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,140 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,141 : INFO : topic diff=0.000000, rho=0.132453\n",
      "2022-02-24 21:44:01,146 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,146 : INFO : PROGRESS: pass 56, at document #10/10\n",
      "2022-02-24 21:44:01,148 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,149 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,149 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,150 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,150 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"processing\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,151 : INFO : topic diff=0.000000, rho=0.131306\n",
      "2022-02-24 21:44:01,154 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,155 : INFO : PROGRESS: pass 57, at document #10/10\n",
      "2022-02-24 21:44:01,157 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,159 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,160 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,160 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,160 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"use\" + 0.023*\"processing\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,161 : INFO : topic diff=0.000000, rho=0.130189\n",
      "2022-02-24 21:44:01,165 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,165 : INFO : PROGRESS: pass 58, at document #10/10\n",
      "2022-02-24 21:44:01,168 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,169 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,169 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,170 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,170 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"natural\" + 0.023*\"processing\" + 0.023*\"relationship\" + 0.023*\"use\"\n",
      "2022-02-24 21:44:01,171 : INFO : topic diff=0.000000, rho=0.129099\n",
      "2022-02-24 21:44:01,175 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,175 : INFO : PROGRESS: pass 59, at document #10/10\n",
      "2022-02-24 21:44:01,177 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,178 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,178 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,179 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,179 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,180 : INFO : topic diff=0.000000, rho=0.128037\n",
      "2022-02-24 21:44:01,184 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,184 : INFO : PROGRESS: pass 60, at document #10/10\n",
      "2022-02-24 21:44:01,187 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,187 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,188 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,188 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,189 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,190 : INFO : topic diff=0.000000, rho=0.127000\n",
      "2022-02-24 21:44:01,194 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,194 : INFO : PROGRESS: pass 61, at document #10/10\n",
      "2022-02-24 21:44:01,197 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,198 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,198 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,198 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,199 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,200 : INFO : topic diff=0.000000, rho=0.125988\n",
      "2022-02-24 21:44:01,203 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,204 : INFO : PROGRESS: pass 62, at document #10/10\n",
      "2022-02-24 21:44:01,206 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,206 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,207 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,207 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,208 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,208 : INFO : topic diff=0.000000, rho=0.125000\n",
      "2022-02-24 21:44:01,212 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,212 : INFO : PROGRESS: pass 63, at document #10/10\n",
      "2022-02-24 21:44:01,214 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,214 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,215 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,215 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,216 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,216 : INFO : topic diff=0.000000, rho=0.124035\n",
      "2022-02-24 21:44:01,220 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,221 : INFO : PROGRESS: pass 64, at document #10/10\n",
      "2022-02-24 21:44:01,223 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,223 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,224 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,224 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,224 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,225 : INFO : topic diff=0.000000, rho=0.123091\n",
      "2022-02-24 21:44:01,228 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,228 : INFO : PROGRESS: pass 65, at document #10/10\n",
      "2022-02-24 21:44:01,231 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,232 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,233 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,234 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,234 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,235 : INFO : topic diff=0.000000, rho=0.122169\n",
      "2022-02-24 21:44:01,238 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,238 : INFO : PROGRESS: pass 66, at document #10/10\n",
      "2022-02-24 21:44:01,240 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,241 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,241 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,242 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,242 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,243 : INFO : topic diff=0.000000, rho=0.121268\n",
      "2022-02-24 21:44:01,246 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,246 : INFO : PROGRESS: pass 67, at document #10/10\n",
      "2022-02-24 21:44:01,249 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,250 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,251 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,251 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,251 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,252 : INFO : topic diff=0.000000, rho=0.120386\n",
      "2022-02-24 21:44:01,255 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,256 : INFO : PROGRESS: pass 68, at document #10/10\n",
      "2022-02-24 21:44:01,259 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,259 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,260 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,260 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,261 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,261 : INFO : topic diff=0.000000, rho=0.119523\n",
      "2022-02-24 21:44:01,265 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,265 : INFO : PROGRESS: pass 69, at document #10/10\n",
      "2022-02-24 21:44:01,268 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,268 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,269 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,270 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,270 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,271 : INFO : topic diff=0.000000, rho=0.118678\n",
      "2022-02-24 21:44:01,274 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,274 : INFO : PROGRESS: pass 70, at document #10/10\n",
      "2022-02-24 21:44:01,276 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,277 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,277 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,278 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,278 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,279 : INFO : topic diff=0.000000, rho=0.117851\n",
      "2022-02-24 21:44:01,282 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,283 : INFO : PROGRESS: pass 71, at document #10/10\n",
      "2022-02-24 21:44:01,285 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,286 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,286 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,286 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,287 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,287 : INFO : topic diff=0.000000, rho=0.117041\n",
      "2022-02-24 21:44:01,291 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,291 : INFO : PROGRESS: pass 72, at document #10/10\n",
      "2022-02-24 21:44:01,294 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,295 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,295 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,296 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,296 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,297 : INFO : topic diff=0.000000, rho=0.116248\n",
      "2022-02-24 21:44:01,300 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,300 : INFO : PROGRESS: pass 73, at document #10/10\n",
      "2022-02-24 21:44:01,303 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,304 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,305 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,306 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,306 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,306 : INFO : topic diff=0.000000, rho=0.115470\n",
      "2022-02-24 21:44:01,310 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,312 : INFO : PROGRESS: pass 74, at document #10/10\n",
      "2022-02-24 21:44:01,317 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,318 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,318 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,319 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,320 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,321 : INFO : topic diff=0.000000, rho=0.114708\n",
      "2022-02-24 21:44:01,323 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:44:01,324 : INFO : PROGRESS: pass 75, at document #10/10\n",
      "2022-02-24 21:44:01,327 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,327 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,328 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,328 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,329 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,329 : INFO : topic diff=0.000000, rho=0.113961\n",
      "2022-02-24 21:44:01,333 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,334 : INFO : PROGRESS: pass 76, at document #10/10\n",
      "2022-02-24 21:44:01,338 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,338 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,338 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,339 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,339 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,340 : INFO : topic diff=0.000000, rho=0.113228\n",
      "2022-02-24 21:44:01,343 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,343 : INFO : PROGRESS: pass 77, at document #10/10\n",
      "2022-02-24 21:44:01,346 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,346 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,347 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,347 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,348 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,348 : INFO : topic diff=0.000000, rho=0.112509\n",
      "2022-02-24 21:44:01,352 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,353 : INFO : PROGRESS: pass 78, at document #10/10\n",
      "2022-02-24 21:44:01,355 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,356 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,356 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,356 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,356 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,357 : INFO : topic diff=0.000000, rho=0.111803\n",
      "2022-02-24 21:44:01,360 : INFO : -5.072 per-word bound, 33.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 21:44:01,361 : INFO : PROGRESS: pass 79, at document #10/10\n",
      "2022-02-24 21:44:01,363 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,363 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,364 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,364 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,364 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n",
      "2022-02-24 21:44:01,365 : INFO : topic diff=0.000000, rho=0.111111\n",
      "2022-02-24 21:44:01,365 : INFO : topic #0 (0.200): 0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"\n",
      "2022-02-24 21:44:01,366 : INFO : topic #1 (0.200): 0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"\n",
      "2022-02-24 21:44:01,366 : INFO : topic #2 (0.200): 0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"\n",
      "2022-02-24 21:44:01,367 : INFO : topic #3 (0.200): 0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"\n",
      "2022-02-24 21:44:01,367 : INFO : topic #4 (0.200): 0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"specialization\" + 0.059*\"nlp\" + 0.032*\"machine\" + 0.032*\"perform\" + 0.032*\"end\" + 0.032*\"application\" + 0.032*\"translate\" + 0.032*\"tool\" + 0.032*\"chatbot\" + 0.032*\"expert\"'),\n",
       " (1,\n",
       "  '0.077*\"instructor\" + 0.029*\"deep\" + 0.029*\"learning\" + 0.029*\"younes\" + 0.029*\"university\" + 0.029*\"stanford\" + 0.029*\"mourri\" + 0.029*\"ai\" + 0.029*\"hard\" + 0.029*\"lecture\"'),\n",
       " (2,\n",
       "  '0.036*\"tensorflow\" + 0.036*\"transformer\" + 0.036*\"brain\" + 0.036*\"scientist\" + 0.036*\"research\" + 0.036*\"coauthor\" + 0.036*\"paper\" + 0.036*\"google\" + 0.036*\"ukasz\" + 0.036*\"library\"'),\n",
       " (3,\n",
       "  '0.062*\"regression\" + 0.062*\"logistic\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.034*\"tweet\" + 0.034*\"text\" + 0.034*\"lesson\" + 0.034*\"feature\" + 0.034*\"numerical\" + 0.034*\"classifier\"'),\n",
       " (4,\n",
       "  '0.065*\"course\" + 0.044*\"word\" + 0.033*\"vector\" + 0.033*\"space\" + 0.023*\"specialization\" + 0.023*\"language\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"natural\" + 0.023*\"relationship\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bec4a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(4, 0.98423046)], 0),\n",
       " ([(0, 0.93821067),\n",
       "   (1, 0.015399863),\n",
       "   (2, 0.015387423),\n",
       "   (3, 0.015496068),\n",
       "   (4, 0.015505998)],\n",
       "  1),\n",
       " ([(0, 0.8853018),\n",
       "   (1, 0.028872894),\n",
       "   (2, 0.028576214),\n",
       "   (3, 0.02857595),\n",
       "   (4, 0.028673114)],\n",
       "  2),\n",
       " ([(0, 0.020304438),\n",
       "   (1, 0.91962755),\n",
       "   (2, 0.020004138),\n",
       "   (3, 0.020003907),\n",
       "   (4, 0.020059945)],\n",
       "  3),\n",
       " ([(0, 0.01538711),\n",
       "   (1, 0.01538687),\n",
       "   (2, 0.93845314),\n",
       "   (3, 0.015387241),\n",
       "   (4, 0.015385632)],\n",
       "  4),\n",
       " ([(4, 0.9703012)], 5),\n",
       " ([(0, 0.022226714),\n",
       "   (1, 0.9108184),\n",
       "   (2, 0.02222724),\n",
       "   (3, 0.022440098),\n",
       "   (4, 0.022287546)],\n",
       "  6),\n",
       " ([(0, 0.06667876),\n",
       "   (1, 0.06667762),\n",
       "   (2, 0.7332926),\n",
       "   (3, 0.06667943),\n",
       "   (4, 0.06667159)],\n",
       "  7),\n",
       " ([(0, 0.011821607),\n",
       "   (1, 0.011780262),\n",
       "   (2, 0.01176662),\n",
       "   (3, 0.95278245),\n",
       "   (4, 0.011849039)],\n",
       "  8),\n",
       " ([(0, 0.033338226),\n",
       "   (1, 0.8666491),\n",
       "   (2, 0.0333388),\n",
       "   (3, 0.033338495),\n",
       "   (4, 0.033335328)],\n",
       "  9)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "137cda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dom_Topic  Topic_Contri                                           Keywords\n",
      "0        4.0        0.9842  course, word, vector, space, specialization, l...\n",
      "1        0.0        0.9382  specialization, nlp, machine, perform, end, ap...\n",
      "2        0.0        0.8853  specialization, nlp, machine, perform, end, ap...\n",
      "3        1.0        0.9196  instructor, deep, learning, younes, university...\n",
      "4        2.0        0.9385  tensorflow, transformer, brain, scientist, res...\n",
      "5        4.0        0.9703  course, word, vector, space, specialization, l...\n",
      "6        1.0        0.9108  instructor, deep, learning, younes, university...\n",
      "7        2.0        0.7333  tensorflow, transformer, brain, scientist, res...\n",
      "8        3.0        0.9528  regression, logistic, sentiment, analysis, twe...\n",
      "9        1.0        0.8666  instructor, deep, learning, younes, university...\n"
     ]
    }
   ],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(ldana[corpusna]):\n",
    "        row = row_list[0] if ldana.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldana.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "sent_topics_df.columns = ['Dom_Topic', 'Topic_Contri', 'Keywords']\n",
    "print(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd582911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '4.6 ( 3,347 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .',\n",
       " 1: 'By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !This Specialization designed taught two expert NLP , machine learning , deep learning .',\n",
       " 2: 'Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .',\n",
       " 3: 'Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .',\n",
       " 4: 'Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,347 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .',\n",
       " 5: 'The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .',\n",
       " 6: 'Other , I informative fun .',\n",
       " 7: 'From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !',\n",
       " 8: 'Instructor Instructor Senior Curriculum Developer'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "sentences = \"\"\n",
    "corpus = pd.read_pickle(\"corpus.pkl\")\n",
    "corpus\n",
    "len(sent_topics_df)\n",
    "i=0\n",
    "a=0\n",
    "while(a<len(sent_topics_df)-1):\n",
    "    sentences = corpus.loc[a].at['transcript']\n",
    "    if(sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"]):\n",
    "        while((a<len(sent_topics_df)-1) and (sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"])):\n",
    "            sentences += corpus.loc[a+1].at['transcript']\n",
    "            a+=1\n",
    "    data[i] = sentences\n",
    "    i+=1\n",
    "    a+=1\n",
    "data[i] = sentences = corpus.loc[a].at['transcript']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26546255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: sentence_id, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c085e483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['4.6 ( 3,347 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .'],\n",
       " 1: ['By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !This Specialization designed taught two expert NLP , machine learning , deep learning .'],\n",
       " 2: ['Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .'],\n",
       " 3: ['Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .'],\n",
       " 4: ['Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,347 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .'],\n",
       " 5: ['The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .'],\n",
       " 6: ['Other , I informative fun .'],\n",
       " 7: ['From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !'],\n",
       " 8: ['Instructor Instructor Senior Curriculum Developer']}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ec4ab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6 ( 3,347 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,347 rating ) HA Aug 9 , 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Other , I informative fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Instructor Instructor Senior Curriculum Developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              transcript\n",
       "0  4.6 ( 3,347 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...\n",
       "1  By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...\n",
       "2                                              Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .\n",
       "3                           Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .\n",
       "4  Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,347 rating ) HA Aug 9 , 2020...\n",
       "5                                      The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .\n",
       "6                                                                                                                            Other , I informative fun .\n",
       "7  From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...\n",
       "8                                                                                                      Instructor Instructor Senior Curriculum Developer"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "combined_sent = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "combined_sent.columns = ['transcript']\n",
    "combined_sent = combined_sent.sort_index()\n",
    "combined_sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
