{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7084085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>analysis</th>\n",
       "      <th>appears</th>\n",
       "      <th>application</th>\n",
       "      <th>applies</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>associated</th>\n",
       "      <th>attented</th>\n",
       "      <th>aug</th>\n",
       "      <th>...</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>waiting</th>\n",
       "      <th>want</th>\n",
       "      <th>wasnnto</th>\n",
       "      <th>week</th>\n",
       "      <th>write</th>\n",
       "      <th>younes</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    algorithm  analysis  appears  application  applies  approximate  \\\n",
       "0           1         1        0            0        0            1   \n",
       "1           0         1        0            1        0            0   \n",
       "2           0         0        0            0        0            0   \n",
       "3           0         0        0            0        0            0   \n",
       "4           0         0        0            0        0            0   \n",
       "5           0         1        0            0        0            0   \n",
       "6           0         0        0            0        0            0   \n",
       "7           0         0        0            0        0            0   \n",
       "8           0         1        0            0        0            0   \n",
       "9           0         0        0            0        0            0   \n",
       "10          0         0        0            0        0            0   \n",
       "11          0         0        0            0        0            0   \n",
       "12          0         0        0            0        0            0   \n",
       "13          0         0        0            0        0            0   \n",
       "14          0         0        0            0        0            0   \n",
       "15          0         0        0            0        0            0   \n",
       "16          0         0        0            0        0            0   \n",
       "17          0         0        0            0        0            0   \n",
       "18          0         1        0            0        0            0   \n",
       "19          0         0        0            0        0            0   \n",
       "20          0         0        0            0        0            0   \n",
       "21          0         0        0            0        0            0   \n",
       "22          0         0        0            0        0            0   \n",
       "23          0         0        1            0        0            0   \n",
       "24          0         0        1            0        0            0   \n",
       "25          0         0        0            0        0            0   \n",
       "26          0         0        0            0        0            0   \n",
       "27          0         0        0            0        0            0   \n",
       "28          0         0        0            0        1            0   \n",
       "29          0         0        1            0        0            0   \n",
       "30          0         0        0            0        0            0   \n",
       "31          0         0        0            0        0            0   \n",
       "32          0         0        0            0        0            0   \n",
       "33          0         0        0            0        0            0   \n",
       "34          0         0        0            0        0            0   \n",
       "35          0         0        0            0        0            0   \n",
       "36          0         0        0            0        0            0   \n",
       "\n",
       "    assignment  associated  attented  aug  ...  video  visualize  vocabulary  \\\n",
       "0            0           0         0    0  ...      1          1           0   \n",
       "1            0           0         0    0  ...      0          0           0   \n",
       "2            0           0         0    0  ...      0          0           0   \n",
       "3            0           0         0    0  ...      0          0           0   \n",
       "4            0           0         0    0  ...      0          0           0   \n",
       "5            1           0         1    2  ...      0          0           0   \n",
       "6            0           0         0    0  ...      0          0           0   \n",
       "7            0           0         0    0  ...      0          0           0   \n",
       "8            0           0         0    0  ...      0          0           0   \n",
       "9            0           0         0    0  ...      0          0           0   \n",
       "10           0           0         0    0  ...      0          0           0   \n",
       "11           0           0         0    0  ...      0          0           0   \n",
       "12           0           0         0    0  ...      0          0           0   \n",
       "13           0           0         0    0  ...      0          0           0   \n",
       "14           0           0         0    0  ...      0          0           0   \n",
       "15           0           0         0    0  ...      0          0           0   \n",
       "16           0           1         0    0  ...      0          0           1   \n",
       "17           0           0         0    0  ...      0          0           1   \n",
       "18           0           0         0    0  ...      0          0           0   \n",
       "19           0           1         0    0  ...      0          0           0   \n",
       "20           0           0         0    0  ...      0          0           0   \n",
       "21           0           0         0    0  ...      0          0           0   \n",
       "22           0           0         0    0  ...      0          0           1   \n",
       "23           0           0         0    0  ...      0          0           1   \n",
       "24           0           0         0    0  ...      0          0           0   \n",
       "25           0           0         0    0  ...      0          0           0   \n",
       "26           0           0         0    0  ...      0          0           0   \n",
       "27           0           0         0    0  ...      0          0           0   \n",
       "28           0           0         0    0  ...      0          0           0   \n",
       "29           0           0         0    0  ...      0          0           0   \n",
       "30           0           0         0    0  ...      0          0           0   \n",
       "31           0           0         0    0  ...      0          0           0   \n",
       "32           0           0         0    0  ...      0          0           0   \n",
       "33           0           0         0    0  ...      0          0           0   \n",
       "34           0           0         0    0  ...      0          0           0   \n",
       "35           0           0         0    0  ...      0          0           0   \n",
       "36           0           0         0    0  ...      1          0           0   \n",
       "\n",
       "    waiting  want  wasnnto  week  write  younes  youtube  \n",
       "0         0     0        0     0      1       0        0  \n",
       "1         0     0        0     0      0       0        0  \n",
       "2         0     0        0     0      0       0        0  \n",
       "3         0     0        0     0      0       1        0  \n",
       "4         0     0        0     0      0       0        0  \n",
       "5         1     0        1     1      0       0        0  \n",
       "6         0     0        0     0      0       0        1  \n",
       "7         0     0        0     0      0       0        0  \n",
       "8         0     0        0     0      0       0        0  \n",
       "9         0     0        0     0      0       0        0  \n",
       "10        0     1        0     0      0       0        0  \n",
       "11        0     1        0     0      0       0        0  \n",
       "12        0     0        0     0      0       0        0  \n",
       "13        0     0        0     0      0       0        0  \n",
       "14        0     0        0     0      0       0        0  \n",
       "15        0     0        0     0      0       0        0  \n",
       "16        0     0        0     0      0       0        0  \n",
       "17        0     0        0     0      0       0        0  \n",
       "18        0     0        0     0      0       0        0  \n",
       "19        0     0        0     0      0       0        0  \n",
       "20        0     0        0     0      0       0        0  \n",
       "21        0     0        0     0      0       0        0  \n",
       "22        0     0        0     0      0       0        0  \n",
       "23        0     0        0     0      0       0        0  \n",
       "24        0     0        0     0      0       0        0  \n",
       "25        0     0        0     0      0       0        0  \n",
       "26        0     0        0     0      0       0        0  \n",
       "27        0     0        0     0      0       0        0  \n",
       "28        0     0        0     0      0       0        0  \n",
       "29        0     0        0     0      0       0        0  \n",
       "30        0     0        0     0      0       0        0  \n",
       "31        0     0        0     0      0       0        0  \n",
       "32        0     0        0     0      0       0        0  \n",
       "33        0     0        0     0      0       0        0  \n",
       "34        0     0        0     0      0       0        0  \n",
       "35        0     0        0     0      0       0        0  \n",
       "36        0     0        0     0      0       0        0  \n",
       "\n",
       "[37 rows x 143 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef8f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d76bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appears</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applies</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  \\\n",
       "algorithm     1   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "analysis      1   1   0   0   0   1   0   0   1   0  ...   0   0   0   0   0   \n",
       "appears       0   0   0   0   0   0   0   0   0   0  ...   0   0   1   0   0   \n",
       "application   0   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n",
       "applies       0   0   0   0   0   0   0   0   0   0  ...   0   1   0   0   0   \n",
       "\n",
       "             32  33  34  35  36  \n",
       "algorithm     0   0   0   0   0  \n",
       "analysis      0   0   0   0   0  \n",
       "appears       0   0   0   0   0  \n",
       "application   0   0   0   0   0  \n",
       "applies       0   0   0   0   0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22350489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:50:08,762 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-25 11:50:08,765 : INFO : using symmetric eta at 0.5\n",
      "2022-02-25 11:50:08,766 : INFO : using serial LDA version on this node\n",
      "2022-02-25 11:50:08,772 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 37 documents, updating model once every 37 documents, evaluating perplexity every 37 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-25 11:50:08,810 : INFO : -5.824 per-word bound, 56.6 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,811 : INFO : PROGRESS: pass 0, at document #37/37\n",
      "2022-02-25 11:50:08,838 : INFO : topic #0 (0.500): 0.021*\"vocabulary\" + 0.019*\"appears\" + 0.018*\"specialization\" + 0.018*\"corpus\" + 0.017*\"instructor\" + 0.016*\"example\" + 0.015*\"number\" + 0.014*\"dictionary\" + 0.014*\"use\" + 0.013*\"count\"\n",
      "2022-02-25 11:50:08,839 : INFO : topic #1 (0.500): 0.028*\"course\" + 0.021*\"using\" + 0.020*\"vector\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.017*\"feature\" + 0.016*\"table\" + 0.015*\"space\" + 0.014*\"specialization\"\n",
      "2022-02-25 11:50:08,840 : INFO : topic diff=0.651104, rho=1.000000\n",
      "2022-02-25 11:50:08,852 : INFO : -5.393 per-word bound, 42.0 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,852 : INFO : PROGRESS: pass 1, at document #37/37\n",
      "2022-02-25 11:50:08,859 : INFO : topic #0 (0.500): 0.023*\"vocabulary\" + 0.019*\"appears\" + 0.019*\"specialization\" + 0.019*\"instructor\" + 0.018*\"corpus\" + 0.017*\"example\" + 0.017*\"learning\" + 0.015*\"number\" + 0.014*\"dictionary\" + 0.014*\"count\"\n",
      "2022-02-25 11:50:08,860 : INFO : topic #1 (0.500): 0.031*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"table\" + 0.018*\"feature\" + 0.017*\"space\" + 0.013*\"specialization\"\n",
      "2022-02-25 11:50:08,861 : INFO : topic diff=0.205709, rho=0.577350\n",
      "2022-02-25 11:50:08,871 : INFO : -5.338 per-word bound, 40.4 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,872 : INFO : PROGRESS: pass 2, at document #37/37\n",
      "2022-02-25 11:50:08,884 : INFO : topic #0 (0.500): 0.024*\"vocabulary\" + 0.020*\"appears\" + 0.019*\"specialization\" + 0.019*\"instructor\" + 0.018*\"example\" + 0.018*\"learning\" + 0.018*\"corpus\" + 0.015*\"dictionary\" + 0.015*\"number\" + 0.014*\"count\"\n",
      "2022-02-25 11:50:08,886 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.020*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.014*\"entire\"\n",
      "2022-02-25 11:50:08,886 : INFO : topic diff=0.100317, rho=0.500000\n",
      "2022-02-25 11:50:08,897 : INFO : -5.320 per-word bound, 40.0 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,898 : INFO : PROGRESS: pass 3, at document #37/37\n",
      "2022-02-25 11:50:08,903 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.019*\"specialization\" + 0.019*\"example\" + 0.019*\"learning\" + 0.017*\"corpus\" + 0.015*\"dictionary\" + 0.014*\"number\" + 0.014*\"count\"\n",
      "2022-02-25 11:50:08,904 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.021*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.016*\"entire\"\n",
      "2022-02-25 11:50:08,905 : INFO : topic diff=0.053817, rho=0.447214\n",
      "2022-02-25 11:50:08,914 : INFO : -5.314 per-word bound, 39.8 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,915 : INFO : PROGRESS: pass 4, at document #37/37\n",
      "2022-02-25 11:50:08,920 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.020*\"specialization\" + 0.019*\"learning\" + 0.019*\"example\" + 0.016*\"corpus\" + 0.015*\"dictionary\" + 0.014*\"number\" + 0.014*\"count\"\n",
      "2022-02-25 11:50:08,921 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.021*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.016*\"entire\"\n",
      "2022-02-25 11:50:08,922 : INFO : topic diff=0.030573, rho=0.408248\n",
      "2022-02-25 11:50:08,931 : INFO : -5.312 per-word bound, 39.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,931 : INFO : PROGRESS: pass 5, at document #37/37\n",
      "2022-02-25 11:50:08,937 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.020*\"specialization\" + 0.020*\"learning\" + 0.019*\"example\" + 0.016*\"corpus\" + 0.015*\"dictionary\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:08,938 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n",
      "2022-02-25 11:50:08,938 : INFO : topic diff=0.018198, rho=0.377964\n",
      "2022-02-25 11:50:08,949 : INFO : -5.311 per-word bound, 39.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,949 : INFO : PROGRESS: pass 6, at document #37/37\n",
      "2022-02-25 11:50:08,955 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.020*\"specialization\" + 0.020*\"learning\" + 0.020*\"example\" + 0.015*\"corpus\" + 0.015*\"dictionary\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:08,955 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n",
      "2022-02-25 11:50:08,956 : INFO : topic diff=0.011252, rho=0.353553\n",
      "2022-02-25 11:50:08,964 : INFO : -5.310 per-word bound, 39.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,965 : INFO : PROGRESS: pass 7, at document #37/37\n",
      "2022-02-25 11:50:08,970 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.020*\"specialization\" + 0.020*\"learning\" + 0.020*\"example\" + 0.015*\"corpus\" + 0.015*\"dictionary\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:08,971 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n",
      "2022-02-25 11:50:08,971 : INFO : topic diff=0.007183, rho=0.333333\n",
      "2022-02-25 11:50:08,983 : INFO : -5.310 per-word bound, 39.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:08,983 : INFO : PROGRESS: pass 8, at document #37/37\n",
      "2022-02-25 11:50:08,991 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"appears\" + 0.020*\"instructor\" + 0.020*\"learning\" + 0.020*\"specialization\" + 0.020*\"example\" + 0.015*\"dictionary\" + 0.015*\"corpus\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:08,992 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n",
      "2022-02-25 11:50:08,993 : INFO : topic diff=0.004711, rho=0.316228\n",
      "2022-02-25 11:50:09,001 : INFO : -5.310 per-word bound, 39.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:09,001 : INFO : PROGRESS: pass 9, at document #37/37\n",
      "2022-02-25 11:50:09,011 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"instructor\" + 0.020*\"appears\" + 0.020*\"learning\" + 0.020*\"specialization\" + 0.020*\"example\" + 0.015*\"dictionary\" + 0.015*\"corpus\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:09,012 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n",
      "2022-02-25 11:50:09,013 : INFO : topic diff=0.003166, rho=0.301511\n",
      "2022-02-25 11:50:09,014 : INFO : topic #0 (0.500): 0.025*\"vocabulary\" + 0.020*\"instructor\" + 0.020*\"appears\" + 0.020*\"learning\" + 0.020*\"specialization\" + 0.020*\"example\" + 0.015*\"dictionary\" + 0.015*\"corpus\" + 0.014*\"count\" + 0.014*\"number\"\n",
      "2022-02-25 11:50:09,014 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"vocabulary\" + 0.020*\"instructor\" + 0.020*\"appears\" + 0.020*\"learning\" + 0.020*\"specialization\" + 0.020*\"example\" + 0.015*\"dictionary\" + 0.015*\"corpus\" + 0.014*\"count\" + 0.014*\"number\"'),\n",
       " (1,\n",
       "  '0.032*\"course\" + 0.022*\"using\" + 0.022*\"vector\" + 0.022*\"table\" + 0.019*\"analysis\" + 0.018*\"set\" + 0.018*\"use\" + 0.018*\"feature\" + 0.017*\"space\" + 0.017*\"entire\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fe173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:50:14,744 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-25 11:50:14,746 : INFO : using symmetric eta at 0.25\n",
      "2022-02-25 11:50:14,746 : INFO : using serial LDA version on this node\n",
      "2022-02-25 11:50:14,747 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 37 documents, updating model once every 37 documents, evaluating perplexity every 37 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-25 11:50:14,764 : INFO : -7.106 per-word bound, 137.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,765 : INFO : PROGRESS: pass 0, at document #37/37\n",
      "2022-02-25 11:50:14,778 : INFO : topic #0 (0.250): 0.036*\"feature\" + 0.028*\"using\" + 0.026*\"use\" + 0.025*\"classifier\" + 0.025*\"extract\" + 0.025*\"example\" + 0.025*\"appears\" + 0.016*\"vector\" + 0.015*\"analysis\" + 0.015*\"video\"\n",
      "2022-02-25 11:50:14,779 : INFO : topic #1 (0.250): 0.058*\"number\" + 0.031*\"instructor\" + 0.031*\"count\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.030*\"vocabulary\" + 0.018*\"use\"\n",
      "2022-02-25 11:50:14,779 : INFO : topic #2 (0.250): 0.030*\"analysis\" + 0.027*\"specialization\" + 0.027*\"table\" + 0.027*\"corpus\" + 0.027*\"set\" + 0.026*\"let\" + 0.017*\"nlp\" + 0.017*\"entire\" + 0.015*\"vocabulary\" + 0.015*\"associated\"\n",
      "2022-02-25 11:50:14,780 : INFO : topic #3 (0.250): 0.040*\"course\" + 0.024*\"specialization\" + 0.022*\"space\" + 0.021*\"vector\" + 0.017*\"set\" + 0.017*\"learning\" + 0.017*\"belong\" + 0.017*\"table\" + 0.016*\"model\" + 0.016*\"use\"\n",
      "2022-02-25 11:50:14,781 : INFO : topic diff=2.256894, rho=1.000000\n",
      "2022-02-25 11:50:14,790 : INFO : -5.579 per-word bound, 47.8 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,791 : INFO : PROGRESS: pass 1, at document #37/37\n",
      "2022-02-25 11:50:14,797 : INFO : topic #0 (0.250): 0.037*\"feature\" + 0.027*\"using\" + 0.026*\"use\" + 0.026*\"example\" + 0.026*\"classifier\" + 0.026*\"extract\" + 0.026*\"appears\" + 0.015*\"vector\" + 0.015*\"analysis\" + 0.015*\"video\"\n",
      "2022-02-25 11:50:14,798 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"instructor\" + 0.031*\"count\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.031*\"vocabulary\" + 0.017*\"use\"\n",
      "2022-02-25 11:50:14,798 : INFO : topic #2 (0.250): 0.029*\"analysis\" + 0.028*\"corpus\" + 0.028*\"table\" + 0.028*\"specialization\" + 0.028*\"set\" + 0.028*\"let\" + 0.023*\"associated\" + 0.017*\"nlp\" + 0.017*\"entire\" + 0.016*\"vocabulary\"\n",
      "2022-02-25 11:50:14,798 : INFO : topic #3 (0.250): 0.043*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.022*\"vector\" + 0.016*\"learning\" + 0.016*\"set\" + 0.016*\"belong\" + 0.016*\"table\" + 0.016*\"model\" + 0.016*\"use\"\n",
      "2022-02-25 11:50:14,799 : INFO : topic diff=0.104081, rho=0.577350\n",
      "2022-02-25 11:50:14,816 : INFO : -5.553 per-word bound, 47.0 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,816 : INFO : PROGRESS: pass 2, at document #37/37\n",
      "2022-02-25 11:50:14,827 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"using\" + 0.026*\"use\" + 0.026*\"example\" + 0.026*\"classifier\" + 0.026*\"extract\" + 0.026*\"appears\" + 0.015*\"vector\" + 0.015*\"analysis\" + 0.015*\"video\"\n",
      "2022-02-25 11:50:14,828 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"instructor\" + 0.031*\"count\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.031*\"vocabulary\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,829 : INFO : topic #2 (0.250): 0.029*\"analysis\" + 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"specialization\" + 0.029*\"set\" + 0.028*\"let\" + 0.026*\"associated\" + 0.016*\"nlp\" + 0.016*\"entire\" + 0.016*\"vocabulary\"\n",
      "2022-02-25 11:50:14,830 : INFO : topic #3 (0.250): 0.043*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"belong\" + 0.016*\"learning\" + 0.016*\"set\" + 0.016*\"table\" + 0.016*\"use\" + 0.016*\"model\"\n",
      "2022-02-25 11:50:14,830 : INFO : topic diff=0.048755, rho=0.500000\n",
      "2022-02-25 11:50:14,841 : INFO : -5.547 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,841 : INFO : PROGRESS: pass 3, at document #37/37\n",
      "2022-02-25 11:50:14,847 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"using\" + 0.027*\"example\" + 0.026*\"use\" + 0.026*\"appears\" + 0.026*\"classifier\" + 0.026*\"extract\" + 0.015*\"vector\" + 0.015*\"analysis\" + 0.015*\"video\"\n",
      "2022-02-25 11:50:14,848 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"vocabulary\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,849 : INFO : topic #2 (0.250): 0.029*\"analysis\" + 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.027*\"associated\" + 0.016*\"nlp\" + 0.016*\"entire\" + 0.016*\"vocabulary\"\n",
      "2022-02-25 11:50:14,849 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"belong\" + 0.016*\"learning\" + 0.016*\"set\" + 0.016*\"use\" + 0.016*\"table\" + 0.016*\"model\"\n",
      "2022-02-25 11:50:14,850 : INFO : topic diff=0.025144, rho=0.447214\n",
      "2022-02-25 11:50:14,859 : INFO : -5.545 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,860 : INFO : PROGRESS: pass 4, at document #37/37\n",
      "2022-02-25 11:50:14,867 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"using\" + 0.026*\"appears\" + 0.026*\"use\" + 0.026*\"classifier\" + 0.026*\"extract\" + 0.015*\"vector\" + 0.015*\"analysis\" + 0.015*\"video\"\n",
      "2022-02-25 11:50:14,869 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"vocabulary\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,870 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"analysis\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.028*\"associated\" + 0.016*\"nlp\" + 0.016*\"entire\" + 0.016*\"vocabulary\"\n",
      "2022-02-25 11:50:14,870 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"belong\" + 0.016*\"learning\" + 0.016*\"use\" + 0.016*\"model\" + 0.016*\"set\" + 0.016*\"entire\"\n",
      "2022-02-25 11:50:14,871 : INFO : topic diff=0.013808, rho=0.408248\n",
      "2022-02-25 11:50:14,881 : INFO : -5.544 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,882 : INFO : PROGRESS: pass 5, at document #37/37\n",
      "2022-02-25 11:50:14,889 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"using\" + 0.027*\"appears\" + 0.026*\"use\" + 0.026*\"classifier\" + 0.026*\"extract\" + 0.015*\"vector\" + 0.015*\"analysis\" + 0.015*\"feel\"\n",
      "2022-02-25 11:50:14,890 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"count\" + 0.031*\"vocabulary\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"map\" + 0.031*\"track\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,890 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"analysis\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.028*\"associated\" + 0.016*\"nlp\" + 0.016*\"vocabulary\" + 0.016*\"unique\"\n",
      "2022-02-25 11:50:14,890 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"belong\" + 0.016*\"use\" + 0.016*\"learning\" + 0.016*\"model\" + 0.016*\"entire\" + 0.016*\"embeddings\"\n",
      "2022-02-25 11:50:14,891 : INFO : topic diff=0.007964, rho=0.377964\n",
      "2022-02-25 11:50:14,900 : INFO : -5.544 per-word bound, 46.6 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,901 : INFO : PROGRESS: pass 6, at document #37/37\n",
      "2022-02-25 11:50:14,908 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"using\" + 0.027*\"appears\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.026*\"use\" + 0.015*\"analysis\" + 0.015*\"vector\" + 0.015*\"feel\"\n",
      "2022-02-25 11:50:14,909 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,910 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"analysis\" + 0.029*\"let\" + 0.028*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"nlp\" + 0.016*\"unique\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:50:14,910 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"use\" + 0.016*\"belong\" + 0.016*\"entire\" + 0.016*\"learning\" + 0.016*\"model\" + 0.016*\"analysis\"\n",
      "2022-02-25 11:50:14,911 : INFO : topic diff=0.004785, rho=0.353553\n",
      "2022-02-25 11:50:14,921 : INFO : -5.544 per-word bound, 46.6 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,921 : INFO : PROGRESS: pass 7, at document #37/37\n",
      "2022-02-25 11:50:14,927 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"appears\" + 0.027*\"using\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.027*\"use\" + 0.015*\"analysis\" + 0.015*\"feel\" + 0.015*\"check\"\n",
      "2022-02-25 11:50:14,927 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,928 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.029*\"analysis\" + 0.029*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"unique\" + 0.016*\"nlp\"\n",
      "2022-02-25 11:50:14,928 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.023*\"vector\" + 0.016*\"use\" + 0.016*\"entire\" + 0.016*\"belong\" + 0.016*\"analysis\" + 0.016*\"model\" + 0.016*\"learning\"\n",
      "2022-02-25 11:50:14,929 : INFO : topic diff=0.002977, rho=0.333333\n",
      "2022-02-25 11:50:14,938 : INFO : -5.544 per-word bound, 46.6 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,940 : INFO : PROGRESS: pass 8, at document #37/37\n",
      "2022-02-25 11:50:14,947 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"appears\" + 0.027*\"using\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.027*\"use\" + 0.015*\"analysis\" + 0.015*\"feel\" + 0.015*\"check\"\n",
      "2022-02-25 11:50:14,947 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,948 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.029*\"analysis\" + 0.029*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"unique\" + 0.016*\"designed\"\n",
      "2022-02-25 11:50:14,948 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"specialization\" + 0.023*\"vector\" + 0.023*\"space\" + 0.016*\"entire\" + 0.016*\"use\" + 0.016*\"analysis\" + 0.016*\"belong\" + 0.016*\"model\" + 0.016*\"language\"\n",
      "2022-02-25 11:50:14,949 : INFO : topic diff=0.001908, rho=0.316228\n",
      "2022-02-25 11:50:14,959 : INFO : -5.544 per-word bound, 46.6 perplexity estimate based on a held-out corpus of 37 documents with 233 words\n",
      "2022-02-25 11:50:14,960 : INFO : PROGRESS: pass 9, at document #37/37\n",
      "2022-02-25 11:50:14,966 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"appears\" + 0.027*\"using\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.027*\"use\" + 0.015*\"analysis\" + 0.015*\"feel\" + 0.015*\"check\"\n",
      "2022-02-25 11:50:14,966 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,966 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.029*\"analysis\" + 0.029*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"unique\" + 0.016*\"designed\"\n",
      "2022-02-25 11:50:14,967 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"vector\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.016*\"entire\" + 0.016*\"use\" + 0.016*\"analysis\" + 0.016*\"belong\" + 0.016*\"model\" + 0.016*\"language\"\n",
      "2022-02-25 11:50:14,967 : INFO : topic diff=0.001255, rho=0.301511\n",
      "2022-02-25 11:50:14,970 : INFO : topic #0 (0.250): 0.038*\"feature\" + 0.027*\"example\" + 0.027*\"appears\" + 0.027*\"using\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.027*\"use\" + 0.015*\"analysis\" + 0.015*\"feel\" + 0.015*\"check\"\n",
      "2022-02-25 11:50:14,971 : INFO : topic #1 (0.250): 0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"\n",
      "2022-02-25 11:50:14,971 : INFO : topic #2 (0.250): 0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.029*\"analysis\" + 0.029*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"unique\" + 0.016*\"designed\"\n",
      "2022-02-25 11:50:14,972 : INFO : topic #3 (0.250): 0.044*\"course\" + 0.023*\"vector\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.016*\"entire\" + 0.016*\"use\" + 0.016*\"analysis\" + 0.016*\"belong\" + 0.016*\"model\" + 0.016*\"language\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.038*\"feature\" + 0.027*\"example\" + 0.027*\"appears\" + 0.027*\"using\" + 0.027*\"classifier\" + 0.027*\"extract\" + 0.027*\"use\" + 0.015*\"analysis\" + 0.015*\"feel\" + 0.015*\"check\"'),\n",
       " (1,\n",
       "  '0.059*\"number\" + 0.031*\"vocabulary\" + 0.031*\"count\" + 0.031*\"instructor\" + 0.031*\"want\" + 0.031*\"given\" + 0.031*\"track\" + 0.031*\"map\" + 0.031*\"corresponding\" + 0.017*\"dictionary\"'),\n",
       " (2,\n",
       "  '0.029*\"corpus\" + 0.029*\"table\" + 0.029*\"set\" + 0.029*\"specialization\" + 0.029*\"let\" + 0.029*\"analysis\" + 0.029*\"associated\" + 0.016*\"vocabulary\" + 0.016*\"unique\" + 0.016*\"designed\"'),\n",
       " (3,\n",
       "  '0.044*\"course\" + 0.023*\"vector\" + 0.023*\"specialization\" + 0.023*\"space\" + 0.016*\"entire\" + 0.016*\"use\" + 0.016*\"analysis\" + 0.016*\"belong\" + 0.016*\"model\" + 0.016*\"language\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b6dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77935955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  locality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specifically  given word  want keep track numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>given another word want keep track number time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>using count  extract feature use feature logis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>so let s take look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it helpful first imagine two class would look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here instance  could corpus consisting four tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>associated corpus  would set unique word  voca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>in example  vocabulary would eight unique word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for particular example sentiment analysis  two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>one class associated positive sentiment negati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>so taking corpus  d set two tweet belong posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>let s take set positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>now  take look vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>to get positive frequency word vocabulary  cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for instance  word happy appears one time firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>so s positive frequency two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the complete table look like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>feel free take pause check entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the logic applies getting negative frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>however  sake clarity  look example  word appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>so s negative frequency three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>take look entire table negative frequency feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>so entire table positive negative frequency co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>in practice coding  table dictionary mapping w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>so map word corresponding class frequency numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>you know create frequency dictionary  map word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>in next video  re going use frequency dictiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           transcript\n",
       "0      rating  â â  students enrolled course   nat...\n",
       "1   by end specialization  designed nlp applicatio...\n",
       "2   this specialization designed taught two expert...\n",
       "3   younes bensouda mourri instructor ai stanford ...\n",
       "4   åukasz kaiser staff research scientist google...\n",
       "5   machine translation  word embeddings  locality...\n",
       "6   the lecture exciting detailed  though little h...\n",
       "7                           other  i informative fun \n",
       "8   from lesson sentiment analysis logistic regres...\n",
       "9   instructor instructor senior curriculum develo...\n",
       "10  specifically  given word  want keep track numb...\n",
       "11  given another word want keep track number time...\n",
       "12  using count  extract feature use feature logis...\n",
       "13                                so let s take look \n",
       "14     it helpful first imagine two class would look \n",
       "15  here instance  could corpus consisting four tw...\n",
       "16  associated corpus  would set unique word  voca...\n",
       "17    in example  vocabulary would eight unique word \n",
       "18  for particular example sentiment analysis  two...\n",
       "19  one class associated positive sentiment negati...\n",
       "20  so taking corpus  d set two tweet belong posit...\n",
       "21                     let s take set positive tweet \n",
       "22                         now  take look vocabulary \n",
       "23  to get positive frequency word vocabulary  cou...\n",
       "24  for instance  word happy appears one time firs...\n",
       "25                       so s positive frequency two \n",
       "26                      the complete table look like \n",
       "27                  feel free take pause check entry \n",
       "28      the logic applies getting negative frequency \n",
       "29  however  sake clarity  look example  word appe...\n",
       "30                     so s negative frequency three \n",
       "31  take look entire table negative frequency feel...\n",
       "32  so entire table positive negative frequency co...\n",
       "33  in practice coding  table dictionary mapping w...\n",
       "34  so map word corresponding class frequency numb...\n",
       "35  you know create frequency dictionary  map word...\n",
       "36  in next video  re going use frequency dictiona..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b8af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â students course language processing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>specialization application perform sentiment a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes instructor ai stanford university learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture straight regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentiment analysis regression learn feature ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor curriculum developer gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word track number time show class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>word track number time word class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>count feature use feature regression classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>s look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>instance tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>corpus word vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>example vocabulary unique word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>example sentiment analysis class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>class sentiment sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>corpus d tweet class tweet class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>s tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>frequency word count time tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>instance word time tweet time tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>take pause check entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>applies frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clarity example word time tweet time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>look frequency check value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>frequency corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>practice mapping word class frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>word class frequency number time class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>frequency map word class number time word class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>video re frequency represent tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           transcript\n",
       "0   rating â students course language processing s...\n",
       "1   specialization application perform sentiment a...\n",
       "2                     specialization machine learning\n",
       "3   younes instructor ai stanford university learn...\n",
       "4   åukasz staff research scientist google brain ...\n",
       "5   machine translation word embeddings sentiment ...\n",
       "6                   lecture straight regression model\n",
       "7                                                 fun\n",
       "8   sentiment analysis regression learn feature ve...\n",
       "9   instructor instructor curriculum developer gen...\n",
       "10                  word track number time show class\n",
       "11                  word track number time word class\n",
       "12    count feature use feature regression classifier\n",
       "13                                             s look\n",
       "14                                              class\n",
       "15                                     instance tweet\n",
       "16                             corpus word vocabulary\n",
       "17                     example vocabulary unique word\n",
       "18                   example sentiment analysis class\n",
       "19                          class sentiment sentiment\n",
       "20                   corpus d tweet class tweet class\n",
       "21                                            s tweet\n",
       "22                                               look\n",
       "23                    frequency word count time tweet\n",
       "24                instance word time tweet time tweet\n",
       "25                                          frequency\n",
       "26                                               look\n",
       "27                             take pause check entry\n",
       "28                                  applies frequency\n",
       "29               clarity example word time tweet time\n",
       "30                                          frequency\n",
       "31                         look frequency check value\n",
       "32                                   frequency corpus\n",
       "33              practice mapping word class frequency\n",
       "34             word class frequency number time class\n",
       "35    frequency map word class number time word class\n",
       "36                 video re frequency represent tweet"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009ca2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>applies</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>brain</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>...</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>value</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ai  analysis  application  applies  assignment  aug  awesome  bayes  \\\n",
       "0    0         1            0        0           0    0        0      1   \n",
       "1    0         1            1        0           0    0        0      0   \n",
       "2    0         0            0        0           0    0        0      0   \n",
       "3    1         0            0        0           0    0        0      0   \n",
       "4    0         0            0        0           0    0        0      0   \n",
       "5    0         1            0        0           1    1        1      0   \n",
       "6    0         0            0        0           0    0        0      0   \n",
       "7    0         0            0        0           0    0        0      0   \n",
       "8    0         1            0        0           0    0        0      0   \n",
       "9    0         0            0        0           0    0        0      0   \n",
       "10   0         0            0        0           0    0        0      0   \n",
       "11   0         0            0        0           0    0        0      0   \n",
       "12   0         0            0        0           0    0        0      0   \n",
       "13   0         0            0        0           0    0        0      0   \n",
       "14   0         0            0        0           0    0        0      0   \n",
       "15   0         0            0        0           0    0        0      0   \n",
       "16   0         0            0        0           0    0        0      0   \n",
       "17   0         0            0        0           0    0        0      0   \n",
       "18   0         1            0        0           0    0        0      0   \n",
       "19   0         0            0        0           0    0        0      0   \n",
       "20   0         0            0        0           0    0        0      0   \n",
       "21   0         0            0        0           0    0        0      0   \n",
       "22   0         0            0        0           0    0        0      0   \n",
       "23   0         0            0        0           0    0        0      0   \n",
       "24   0         0            0        0           0    0        0      0   \n",
       "25   0         0            0        0           0    0        0      0   \n",
       "26   0         0            0        0           0    0        0      0   \n",
       "27   0         0            0        0           0    0        0      0   \n",
       "28   0         0            0        1           0    0        0      0   \n",
       "29   0         0            0        0           0    0        0      0   \n",
       "30   0         0            0        0           0    0        0      0   \n",
       "31   0         0            0        0           0    0        0      0   \n",
       "32   0         0            0        0           0    0        0      0   \n",
       "33   0         0            0        0           0    0        0      0   \n",
       "34   0         0            0        0           0    0        0      0   \n",
       "35   0         0            0        0           0    0        0      0   \n",
       "36   0         0            0        0           0    0        0      0   \n",
       "\n",
       "    brain  chatbot  ...  university  use  value  vector  video  visualize  \\\n",
       "0       0        0  ...           0    2      0       2      1          1   \n",
       "1       0        1  ...           0    0      0       0      0          0   \n",
       "2       0        0  ...           0    0      0       0      0          0   \n",
       "3       0        0  ...           1    0      0       0      0          0   \n",
       "4       1        0  ...           0    0      0       0      0          0   \n",
       "5       0        0  ...           0    0      0       1      0          0   \n",
       "6       0        0  ...           0    0      0       0      0          0   \n",
       "7       0        0  ...           0    0      0       0      0          0   \n",
       "8       0        0  ...           0    0      0       1      0          0   \n",
       "9       0        0  ...           0    0      0       0      0          0   \n",
       "10      0        0  ...           0    0      0       0      0          0   \n",
       "11      0        0  ...           0    0      0       0      0          0   \n",
       "12      0        0  ...           0    1      0       0      0          0   \n",
       "13      0        0  ...           0    0      0       0      0          0   \n",
       "14      0        0  ...           0    0      0       0      0          0   \n",
       "15      0        0  ...           0    0      0       0      0          0   \n",
       "16      0        0  ...           0    0      0       0      0          0   \n",
       "17      0        0  ...           0    0      0       0      0          0   \n",
       "18      0        0  ...           0    0      0       0      0          0   \n",
       "19      0        0  ...           0    0      0       0      0          0   \n",
       "20      0        0  ...           0    0      0       0      0          0   \n",
       "21      0        0  ...           0    0      0       0      0          0   \n",
       "22      0        0  ...           0    0      0       0      0          0   \n",
       "23      0        0  ...           0    0      0       0      0          0   \n",
       "24      0        0  ...           0    0      0       0      0          0   \n",
       "25      0        0  ...           0    0      0       0      0          0   \n",
       "26      0        0  ...           0    0      0       0      0          0   \n",
       "27      0        0  ...           0    0      0       0      0          0   \n",
       "28      0        0  ...           0    0      0       0      0          0   \n",
       "29      0        0  ...           0    0      0       0      0          0   \n",
       "30      0        0  ...           0    0      0       0      0          0   \n",
       "31      0        0  ...           0    0      1       0      0          0   \n",
       "32      0        0  ...           0    0      0       0      0          0   \n",
       "33      0        0  ...           0    0      0       0      0          0   \n",
       "34      0        0  ...           0    0      0       0      0          0   \n",
       "35      0        0  ...           0    0      0       0      0          0   \n",
       "36      0        0  ...           0    0      0       0      1          0   \n",
       "\n",
       "    vocabulary  week  word  younes  \n",
       "0            0     0     3       0  \n",
       "1            0     0     0       0  \n",
       "2            0     0     0       0  \n",
       "3            0     0     0       1  \n",
       "4            0     0     0       0  \n",
       "5            0     1     1       0  \n",
       "6            0     0     0       0  \n",
       "7            0     0     0       0  \n",
       "8            0     0     0       0  \n",
       "9            0     0     0       0  \n",
       "10           0     0     1       0  \n",
       "11           0     0     2       0  \n",
       "12           0     0     0       0  \n",
       "13           0     0     0       0  \n",
       "14           0     0     0       0  \n",
       "15           0     0     0       0  \n",
       "16           1     0     1       0  \n",
       "17           1     0     1       0  \n",
       "18           0     0     0       0  \n",
       "19           0     0     0       0  \n",
       "20           0     0     0       0  \n",
       "21           0     0     0       0  \n",
       "22           0     0     0       0  \n",
       "23           0     0     1       0  \n",
       "24           0     0     1       0  \n",
       "25           0     0     0       0  \n",
       "26           0     0     0       0  \n",
       "27           0     0     0       0  \n",
       "28           0     0     0       0  \n",
       "29           0     0     1       0  \n",
       "30           0     0     0       0  \n",
       "31           0     0     0       0  \n",
       "32           0     0     0       0  \n",
       "33           0     0     1       0  \n",
       "34           0     0     1       0  \n",
       "35           0     0     2       0  \n",
       "36           0     0     0       0  \n",
       "\n",
       "[37 rows x 89 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e9ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32882e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:50:31,763 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-25 11:50:31,764 : INFO : using symmetric eta at 0.25\n",
      "2022-02-25 11:50:31,765 : INFO : using serial LDA version on this node\n",
      "2022-02-25 11:50:31,766 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 37 documents, updating model once every 37 documents, evaluating perplexity every 37 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-25 11:50:31,782 : INFO : -6.329 per-word bound, 80.4 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,783 : INFO : PROGRESS: pass 0, at document #37/37\n",
      "2022-02-25 11:50:31,796 : INFO : topic #0 (0.250): 0.054*\"sentiment\" + 0.054*\"analysis\" + 0.054*\"example\" + 0.031*\"class\" + 0.031*\"word\" + 0.030*\"specialization\" + 0.030*\"chatbot\" + 0.030*\"perform\" + 0.030*\"application\" + 0.030*\"tool\"\n",
      "2022-02-25 11:50:31,796 : INFO : topic #1 (0.250): 0.059*\"tweet\" + 0.059*\"sentiment\" + 0.034*\"class\" + 0.034*\"word\" + 0.033*\"model\" + 0.033*\"regression\" + 0.033*\"example\" + 0.033*\"lecture\" + 0.032*\"clarity\" + 0.032*\"check\"\n",
      "2022-02-25 11:50:31,797 : INFO : topic #2 (0.250): 0.101*\"word\" + 0.101*\"class\" + 0.090*\"frequency\" + 0.061*\"tweet\" + 0.042*\"number\" + 0.032*\"feature\" + 0.032*\"corpus\" + 0.022*\"regression\" + 0.022*\"instructor\" + 0.022*\"classifier\"\n",
      "2022-02-25 11:50:31,797 : INFO : topic #3 (0.250): 0.058*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.039*\"specialization\" + 0.031*\"tweet\" + 0.030*\"sentiment\" + 0.030*\"regression\" + 0.030*\"space\" + 0.030*\"analysis\" + 0.029*\"look\"\n",
      "2022-02-25 11:50:31,798 : INFO : topic diff=2.402866, rho=1.000000\n",
      "2022-02-25 11:50:31,805 : INFO : -4.798 per-word bound, 27.8 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,806 : INFO : PROGRESS: pass 1, at document #37/37\n",
      "2022-02-25 11:50:31,812 : INFO : topic #0 (0.250): 0.054*\"sentiment\" + 0.054*\"analysis\" + 0.054*\"example\" + 0.030*\"class\" + 0.030*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\"\n",
      "2022-02-25 11:50:31,813 : INFO : topic #1 (0.250): 0.060*\"sentiment\" + 0.046*\"tweet\" + 0.034*\"model\" + 0.034*\"class\" + 0.034*\"regression\" + 0.034*\"word\" + 0.034*\"check\" + 0.034*\"example\" + 0.034*\"lecture\" + 0.034*\"clarity\"\n",
      "2022-02-25 11:50:31,813 : INFO : topic #2 (0.250): 0.100*\"word\" + 0.100*\"class\" + 0.094*\"frequency\" + 0.069*\"tweet\" + 0.041*\"number\" + 0.032*\"feature\" + 0.032*\"corpus\" + 0.022*\"instructor\" + 0.022*\"regression\" + 0.022*\"classifier\"\n",
      "2022-02-25 11:50:31,814 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"sentiment\" + 0.031*\"regression\" + 0.030*\"space\" + 0.030*\"analysis\" + 0.030*\"look\" + 0.028*\"tweet\"\n",
      "2022-02-25 11:50:31,814 : INFO : topic diff=0.050586, rho=0.577350\n",
      "2022-02-25 11:50:31,823 : INFO : -4.792 per-word bound, 27.7 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,823 : INFO : PROGRESS: pass 2, at document #37/37\n",
      "2022-02-25 11:50:31,830 : INFO : topic #0 (0.250): 0.055*\"sentiment\" + 0.054*\"example\" + 0.054*\"analysis\" + 0.030*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,830 : INFO : topic #1 (0.250): 0.061*\"sentiment\" + 0.040*\"tweet\" + 0.034*\"check\" + 0.034*\"regression\" + 0.034*\"model\" + 0.034*\"lecture\" + 0.034*\"clarity\" + 0.034*\"straight\" + 0.034*\"pause\" + 0.034*\"entry\"\n",
      "2022-02-25 11:50:31,831 : INFO : topic #2 (0.250): 0.100*\"word\" + 0.100*\"class\" + 0.096*\"frequency\" + 0.072*\"tweet\" + 0.041*\"number\" + 0.032*\"feature\" + 0.031*\"corpus\" + 0.022*\"instructor\" + 0.022*\"classifier\" + 0.022*\"count\"\n",
      "2022-02-25 11:50:31,831 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.030*\"look\" + 0.026*\"tweet\"\n",
      "2022-02-25 11:50:31,832 : INFO : topic diff=0.021683, rho=0.500000\n",
      "2022-02-25 11:50:31,840 : INFO : -4.789 per-word bound, 27.7 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,840 : INFO : PROGRESS: pass 3, at document #37/37\n",
      "2022-02-25 11:50:31,846 : INFO : topic #0 (0.250): 0.055*\"sentiment\" + 0.055*\"example\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,847 : INFO : topic #1 (0.250): 0.062*\"sentiment\" + 0.035*\"check\" + 0.035*\"regression\" + 0.035*\"model\" + 0.035*\"lecture\" + 0.035*\"straight\" + 0.035*\"pause\" + 0.035*\"clarity\" + 0.035*\"entry\" + 0.034*\"example\"\n",
      "2022-02-25 11:50:31,847 : INFO : topic #2 (0.250): 0.101*\"word\" + 0.099*\"class\" + 0.097*\"frequency\" + 0.075*\"tweet\" + 0.041*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.022*\"instructor\" + 0.022*\"classifier\" + 0.022*\"count\"\n",
      "2022-02-25 11:50:31,848 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.030*\"look\" + 0.026*\"tweet\"\n",
      "2022-02-25 11:50:31,849 : INFO : topic diff=0.013381, rho=0.447214\n",
      "2022-02-25 11:50:31,857 : INFO : -4.787 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,857 : INFO : PROGRESS: pass 4, at document #37/37\n",
      "2022-02-25 11:50:31,862 : INFO : topic #0 (0.250): 0.055*\"sentiment\" + 0.055*\"example\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,863 : INFO : topic #1 (0.250): 0.063*\"sentiment\" + 0.035*\"check\" + 0.035*\"regression\" + 0.035*\"model\" + 0.035*\"lecture\" + 0.035*\"straight\" + 0.035*\"pause\" + 0.035*\"entry\" + 0.035*\"clarity\" + 0.035*\"example\"\n",
      "2022-02-25 11:50:31,864 : INFO : topic #2 (0.250): 0.102*\"word\" + 0.099*\"class\" + 0.097*\"frequency\" + 0.078*\"tweet\" + 0.041*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.022*\"instructor\" + 0.022*\"classifier\" + 0.022*\"count\"\n",
      "2022-02-25 11:50:31,864 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"sentiment\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.031*\"look\" + 0.025*\"tweet\"\n",
      "2022-02-25 11:50:31,865 : INFO : topic diff=0.009606, rho=0.408248\n",
      "2022-02-25 11:50:31,873 : INFO : -4.785 per-word bound, 27.6 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,874 : INFO : PROGRESS: pass 5, at document #37/37\n",
      "2022-02-25 11:50:31,880 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,881 : INFO : topic #1 (0.250): 0.064*\"sentiment\" + 0.036*\"check\" + 0.036*\"regression\" + 0.035*\"model\" + 0.035*\"lecture\" + 0.035*\"straight\" + 0.035*\"pause\" + 0.035*\"entry\" + 0.035*\"clarity\" + 0.035*\"example\"\n",
      "2022-02-25 11:50:31,881 : INFO : topic #2 (0.250): 0.103*\"word\" + 0.098*\"class\" + 0.097*\"frequency\" + 0.079*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,882 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.031*\"look\" + 0.025*\"tweet\"\n",
      "2022-02-25 11:50:31,883 : INFO : topic diff=0.007381, rho=0.377964\n",
      "2022-02-25 11:50:31,890 : INFO : -4.783 per-word bound, 27.5 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,891 : INFO : PROGRESS: pass 6, at document #37/37\n",
      "2022-02-25 11:50:31,898 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,898 : INFO : topic #1 (0.250): 0.064*\"sentiment\" + 0.036*\"check\" + 0.036*\"regression\" + 0.036*\"model\" + 0.036*\"lecture\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.035*\"example\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:50:31,899 : INFO : topic #2 (0.250): 0.104*\"word\" + 0.098*\"class\" + 0.097*\"frequency\" + 0.081*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,899 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.031*\"look\" + 0.024*\"tweet\"\n",
      "2022-02-25 11:50:31,900 : INFO : topic diff=0.005848, rho=0.353553\n",
      "2022-02-25 11:50:31,911 : INFO : -4.782 per-word bound, 27.5 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,914 : INFO : PROGRESS: pass 7, at document #37/37\n",
      "2022-02-25 11:50:31,920 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,921 : INFO : topic #1 (0.250): 0.065*\"sentiment\" + 0.036*\"check\" + 0.036*\"regression\" + 0.036*\"model\" + 0.036*\"lecture\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.035*\"example\"\n",
      "2022-02-25 11:50:31,922 : INFO : topic #2 (0.250): 0.105*\"word\" + 0.098*\"class\" + 0.097*\"frequency\" + 0.082*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,922 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.031*\"look\" + 0.024*\"tweet\"\n",
      "2022-02-25 11:50:31,923 : INFO : topic diff=0.004721, rho=0.333333\n",
      "2022-02-25 11:50:31,933 : INFO : -4.781 per-word bound, 27.5 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,934 : INFO : PROGRESS: pass 8, at document #37/37\n",
      "2022-02-25 11:50:31,939 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,940 : INFO : topic #1 (0.250): 0.065*\"sentiment\" + 0.036*\"check\" + 0.036*\"regression\" + 0.036*\"model\" + 0.036*\"lecture\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.036*\"example\"\n",
      "2022-02-25 11:50:31,941 : INFO : topic #2 (0.250): 0.105*\"word\" + 0.098*\"class\" + 0.097*\"frequency\" + 0.083*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,942 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"sentiment\" + 0.031*\"analysis\" + 0.031*\"look\" + 0.024*\"tweet\"\n",
      "2022-02-25 11:50:31,942 : INFO : topic diff=0.003850, rho=0.316228\n",
      "2022-02-25 11:50:31,954 : INFO : -4.780 per-word bound, 27.5 perplexity estimate based on a held-out corpus of 37 documents with 198 words\n",
      "2022-02-25 11:50:31,954 : INFO : PROGRESS: pass 9, at document #37/37\n",
      "2022-02-25 11:50:31,960 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,961 : INFO : topic #1 (0.250): 0.065*\"sentiment\" + 0.037*\"check\" + 0.036*\"regression\" + 0.036*\"lecture\" + 0.036*\"model\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.036*\"example\"\n",
      "2022-02-25 11:50:31,962 : INFO : topic #2 (0.250): 0.105*\"word\" + 0.097*\"class\" + 0.097*\"frequency\" + 0.083*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,962 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"look\" + 0.024*\"tweet\"\n",
      "2022-02-25 11:50:31,963 : INFO : topic diff=0.003149, rho=0.301511\n",
      "2022-02-25 11:50:31,963 : INFO : topic #0 (0.250): 0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"\n",
      "2022-02-25 11:50:31,964 : INFO : topic #1 (0.250): 0.065*\"sentiment\" + 0.037*\"check\" + 0.036*\"regression\" + 0.036*\"lecture\" + 0.036*\"model\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.036*\"example\"\n",
      "2022-02-25 11:50:31,964 : INFO : topic #2 (0.250): 0.105*\"word\" + 0.097*\"class\" + 0.097*\"frequency\" + 0.083*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"\n",
      "2022-02-25 11:50:31,965 : INFO : topic #3 (0.250): 0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"look\" + 0.024*\"tweet\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.055*\"example\" + 0.055*\"sentiment\" + 0.055*\"analysis\" + 0.031*\"vocabulary\" + 0.030*\"specialization\" + 0.030*\"perform\" + 0.030*\"chatbot\" + 0.030*\"application\" + 0.030*\"tool\" + 0.030*\"translate\"'),\n",
       " (1,\n",
       "  '0.065*\"sentiment\" + 0.037*\"check\" + 0.036*\"regression\" + 0.036*\"lecture\" + 0.036*\"model\" + 0.036*\"straight\" + 0.036*\"pause\" + 0.036*\"entry\" + 0.036*\"clarity\" + 0.036*\"example\"'),\n",
       " (2,\n",
       "  '0.105*\"word\" + 0.097*\"class\" + 0.097*\"frequency\" + 0.083*\"tweet\" + 0.040*\"number\" + 0.031*\"feature\" + 0.031*\"corpus\" + 0.021*\"instructor\" + 0.021*\"classifier\" + 0.021*\"count\"'),\n",
       " (3,\n",
       "  '0.059*\"course\" + 0.040*\"word\" + 0.040*\"vector\" + 0.040*\"specialization\" + 0.031*\"regression\" + 0.031*\"space\" + 0.031*\"analysis\" + 0.031*\"sentiment\" + 0.031*\"look\" + 0.024*\"tweet\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d643340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924e6236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â â students course natural language pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end specialization nlp application perform sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization expert nlp machine deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes mourri instructor ai stanford universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings localityse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture detailed little hard straight helped r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lesson sentiment analysis logistic regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word track number time s show positive class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>word track number time word negative class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>count extract feature use feature logistic reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>let s look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>first imagine class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>instance tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>corpus unique word vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>example vocabulary unique word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>particular example sentiment analysis class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>class positive sentiment negative sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>corpus d tweet positive class tweet belong neg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>s positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>look vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>positive frequency word vocabulary count time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>instance word happy time positive tweet time s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>s positive frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>complete table look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>free take pause check entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>logic applies negative frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clarity example word time first tweet time second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>s negative frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>look entire table negative frequency free chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>entire table positive negative frequency corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>practice table dictionary mapping word class f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>map word class frequency number time s class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>create frequency dictionary map word class num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>next video re use frequency dictionary represe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           transcript\n",
       "0   rating â â students course natural language pr...\n",
       "1   end specialization nlp application perform sen...\n",
       "2     specialization expert nlp machine deep learning\n",
       "3   younes mourri instructor ai stanford universit...\n",
       "4   åukasz staff research scientist google brain ...\n",
       "5   machine translation word embeddings localityse...\n",
       "6   lecture detailed little hard straight helped r...\n",
       "7                             other i informative fun\n",
       "8   lesson sentiment analysis logistic regression ...\n",
       "9   instructor instructor senior curriculum develo...\n",
       "10       word track number time s show positive class\n",
       "11         word track number time word negative class\n",
       "12  count extract feature use feature logistic reg...\n",
       "13                                         let s look\n",
       "14                                first imagine class\n",
       "15                                     instance tweet\n",
       "16                      corpus unique word vocabulary\n",
       "17                     example vocabulary unique word\n",
       "18        particular example sentiment analysis class\n",
       "19        class positive sentiment negative sentiment\n",
       "20  corpus d tweet positive class tweet belong neg...\n",
       "21                                   s positive tweet\n",
       "22                                    look vocabulary\n",
       "23  positive frequency word vocabulary count time ...\n",
       "24  instance word happy time positive tweet time s...\n",
       "25                               s positive frequency\n",
       "26                                complete table look\n",
       "27                        free take pause check entry\n",
       "28                   logic applies negative frequency\n",
       "29  clarity example word time first tweet time second\n",
       "30                               s negative frequency\n",
       "31  look entire table negative frequency free chec...\n",
       "32    entire table positive negative frequency corpus\n",
       "33  practice table dictionary mapping word class f...\n",
       "34       map word class frequency number time s class\n",
       "35  create frequency dictionary map word class num...\n",
       "36  next video re use frequency dictionary represe..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d43240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>applies</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>belong</th>\n",
       "      <th>...</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>value</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ai  analysis  application  applies  approximate  assignment  aug  awesome  \\\n",
       "0    0         1            0        0            1           0    0        0   \n",
       "1    0         1            1        0            0           0    0        0   \n",
       "2    0         0            0        0            0           0    0        0   \n",
       "3    1         0            0        0            0           0    0        0   \n",
       "4    0         0            0        0            0           0    0        0   \n",
       "5    0         1            0        0            0           1    1        1   \n",
       "6    0         0            0        0            0           0    0        0   \n",
       "7    0         0            0        0            0           0    0        0   \n",
       "8    0         1            0        0            0           0    0        0   \n",
       "9    0         0            0        0            0           0    0        0   \n",
       "10   0         0            0        0            0           0    0        0   \n",
       "11   0         0            0        0            0           0    0        0   \n",
       "12   0         0            0        0            0           0    0        0   \n",
       "13   0         0            0        0            0           0    0        0   \n",
       "14   0         0            0        0            0           0    0        0   \n",
       "15   0         0            0        0            0           0    0        0   \n",
       "16   0         0            0        0            0           0    0        0   \n",
       "17   0         0            0        0            0           0    0        0   \n",
       "18   0         1            0        0            0           0    0        0   \n",
       "19   0         0            0        0            0           0    0        0   \n",
       "20   0         0            0        0            0           0    0        0   \n",
       "21   0         0            0        0            0           0    0        0   \n",
       "22   0         0            0        0            0           0    0        0   \n",
       "23   0         0            0        0            0           0    0        0   \n",
       "24   0         0            0        0            0           0    0        0   \n",
       "25   0         0            0        0            0           0    0        0   \n",
       "26   0         0            0        0            0           0    0        0   \n",
       "27   0         0            0        0            0           0    0        0   \n",
       "28   0         0            0        1            0           0    0        0   \n",
       "29   0         0            0        0            0           0    0        0   \n",
       "30   0         0            0        0            0           0    0        0   \n",
       "31   0         0            0        0            0           0    0        0   \n",
       "32   0         0            0        0            0           0    0        0   \n",
       "33   0         0            0        0            0           0    0        0   \n",
       "34   0         0            0        0            0           0    0        0   \n",
       "35   0         0            0        0            0           0    0        0   \n",
       "36   0         0            0        0            0           0    0        0   \n",
       "\n",
       "    bayes  belong  ...  university  use  value  vector  video  visualize  \\\n",
       "0       1       0  ...           0    2      0       2      1          1   \n",
       "1       0       0  ...           0    0      0       0      0          0   \n",
       "2       0       0  ...           0    0      0       0      0          0   \n",
       "3       0       0  ...           1    0      0       0      0          0   \n",
       "4       0       0  ...           0    0      0       0      0          0   \n",
       "5       0       0  ...           0    0      0       1      0          0   \n",
       "6       0       0  ...           0    0      0       0      0          0   \n",
       "7       0       0  ...           0    0      0       0      0          0   \n",
       "8       0       0  ...           0    0      0       1      0          0   \n",
       "9       0       0  ...           0    1      0       0      0          0   \n",
       "10      0       0  ...           0    0      0       0      0          0   \n",
       "11      0       0  ...           0    0      0       0      0          0   \n",
       "12      0       0  ...           0    1      0       0      0          0   \n",
       "13      0       0  ...           0    0      0       0      0          0   \n",
       "14      0       0  ...           0    0      0       0      0          0   \n",
       "15      0       0  ...           0    0      0       0      0          0   \n",
       "16      0       0  ...           0    0      0       0      0          0   \n",
       "17      0       0  ...           0    0      0       0      0          0   \n",
       "18      0       0  ...           0    0      0       0      0          0   \n",
       "19      0       0  ...           0    0      0       0      0          0   \n",
       "20      0       1  ...           0    0      0       0      0          0   \n",
       "21      0       0  ...           0    0      0       0      0          0   \n",
       "22      0       0  ...           0    0      0       0      0          0   \n",
       "23      0       0  ...           0    0      0       0      0          0   \n",
       "24      0       0  ...           0    0      0       0      0          0   \n",
       "25      0       0  ...           0    0      0       0      0          0   \n",
       "26      0       0  ...           0    0      0       0      0          0   \n",
       "27      0       0  ...           0    0      0       0      0          0   \n",
       "28      0       0  ...           0    0      0       0      0          0   \n",
       "29      0       0  ...           0    0      0       0      0          0   \n",
       "30      0       0  ...           0    0      0       0      0          0   \n",
       "31      0       0  ...           0    0      1       0      0          0   \n",
       "32      0       0  ...           0    0      0       0      0          0   \n",
       "33      0       0  ...           0    0      0       0      0          0   \n",
       "34      0       0  ...           0    0      0       0      0          0   \n",
       "35      0       0  ...           0    0      0       0      0          0   \n",
       "36      0       0  ...           0    1      0       0      1          0   \n",
       "\n",
       "    vocabulary  week  word  younes  \n",
       "0            0     0     3       0  \n",
       "1            0     0     0       0  \n",
       "2            0     0     0       0  \n",
       "3            0     0     0       1  \n",
       "4            0     0     0       0  \n",
       "5            0     1     1       0  \n",
       "6            0     0     0       0  \n",
       "7            0     0     0       0  \n",
       "8            0     0     0       0  \n",
       "9            0     0     0       0  \n",
       "10           0     0     1       0  \n",
       "11           0     0     2       0  \n",
       "12           0     0     0       0  \n",
       "13           0     0     0       0  \n",
       "14           0     0     0       0  \n",
       "15           0     0     0       0  \n",
       "16           1     0     1       0  \n",
       "17           1     0     1       0  \n",
       "18           0     0     0       0  \n",
       "19           0     0     0       0  \n",
       "20           0     0     0       0  \n",
       "21           0     0     0       0  \n",
       "22           1     0     0       0  \n",
       "23           1     0     1       0  \n",
       "24           0     0     1       0  \n",
       "25           0     0     0       0  \n",
       "26           0     0     0       0  \n",
       "27           0     0     0       0  \n",
       "28           0     0     0       0  \n",
       "29           0     0     1       0  \n",
       "30           0     0     0       0  \n",
       "31           0     0     0       0  \n",
       "32           0     0     0       0  \n",
       "33           0     0     1       0  \n",
       "34           0     0     1       0  \n",
       "35           0     0     2       0  \n",
       "36           0     0     0       0  \n",
       "\n",
       "[37 rows x 135 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bd222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4daee25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:12,577 : INFO : using symmetric alpha at 0.2\n",
      "2022-02-25 11:52:12,578 : INFO : using symmetric eta at 0.2\n",
      "2022-02-25 11:52:12,578 : INFO : using serial LDA version on this node\n",
      "2022-02-25 11:52:12,579 : INFO : running online (multi-pass) LDA training, 5 topics, 80 passes over the supplied corpus of 37 documents, updating model once every 37 documents, evaluating perplexity every 37 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-25 11:52:12,595 : INFO : -7.312 per-word bound, 158.9 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,596 : INFO : PROGRESS: pass 0, at document #37/37\n",
      "2022-02-25 11:52:12,609 : INFO : topic #0 (0.200): 0.041*\"sentiment\" + 0.036*\"class\" + 0.030*\"use\" + 0.028*\"tweet\" + 0.027*\"specialization\" + 0.025*\"frequency\" + 0.025*\"dictionary\" + 0.025*\"instructor\" + 0.023*\"nlp\" + 0.020*\"word\"\n",
      "2022-02-25 11:52:12,611 : INFO : topic #1 (0.200): 0.084*\"class\" + 0.084*\"word\" + 0.068*\"frequency\" + 0.051*\"number\" + 0.035*\"positive\" + 0.035*\"negative\" + 0.035*\"map\" + 0.035*\"corpus\" + 0.019*\"vocabulary\" + 0.019*\"table\"\n",
      "2022-02-25 11:52:12,612 : INFO : topic #2 (0.200): 0.026*\"feature\" + 0.025*\"free\" + 0.025*\"check\" + 0.024*\"use\" + 0.022*\"word\" + 0.022*\"course\" + 0.018*\"logistic\" + 0.018*\"regression\" + 0.016*\"entire\" + 0.014*\"classifier\"\n",
      "2022-02-25 11:52:12,612 : INFO : topic #3 (0.200): 0.060*\"positive\" + 0.059*\"tweet\" + 0.057*\"word\" + 0.032*\"frequency\" + 0.029*\"sentiment\" + 0.028*\"analysis\" + 0.025*\"course\" + 0.022*\"negative\" + 0.022*\"look\" + 0.021*\"class\"\n",
      "2022-02-25 11:52:12,613 : INFO : topic #4 (0.200): 0.032*\"tweet\" + 0.028*\"specialization\" + 0.027*\"word\" + 0.027*\"class\" + 0.021*\"regression\" + 0.021*\"course\" + 0.020*\"model\" + 0.017*\"learning\" + 0.017*\"deep\" + 0.015*\"positive\"\n",
      "2022-02-25 11:52:12,614 : INFO : topic diff=2.806172, rho=1.000000\n",
      "2022-02-25 11:52:12,624 : INFO : -5.508 per-word bound, 45.5 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,624 : INFO : PROGRESS: pass 1, at document #37/37\n",
      "2022-02-25 11:52:12,630 : INFO : topic #0 (0.200): 0.031*\"use\" + 0.030*\"specialization\" + 0.029*\"class\" + 0.029*\"sentiment\" + 0.028*\"instructor\" + 0.027*\"nlp\" + 0.026*\"dictionary\" + 0.023*\"frequency\" + 0.023*\"tweet\" + 0.018*\"logistic\"\n",
      "2022-02-25 11:52:12,631 : INFO : topic #1 (0.200): 0.098*\"word\" + 0.096*\"class\" + 0.083*\"frequency\" + 0.053*\"negative\" + 0.052*\"number\" + 0.041*\"positive\" + 0.036*\"corpus\" + 0.030*\"map\" + 0.024*\"track\" + 0.024*\"unique\"\n",
      "2022-02-25 11:52:12,631 : INFO : topic #2 (0.200): 0.031*\"feature\" + 0.031*\"free\" + 0.031*\"check\" + 0.022*\"use\" + 0.019*\"logistic\" + 0.019*\"regression\" + 0.017*\"classifier\" + 0.017*\"coauthor\" + 0.017*\"staff\" + 0.017*\"library\"\n",
      "2022-02-25 11:52:12,633 : INFO : topic #3 (0.200): 0.070*\"tweet\" + 0.060*\"positive\" + 0.048*\"word\" + 0.039*\"sentiment\" + 0.029*\"analysis\" + 0.027*\"course\" + 0.026*\"example\" + 0.023*\"vocabulary\" + 0.022*\"class\" + 0.021*\"look\"\n",
      "2022-02-25 11:52:12,634 : INFO : topic #4 (0.200): 0.031*\"specialization\" + 0.030*\"word\" + 0.028*\"course\" + 0.024*\"tweet\" + 0.022*\"regression\" + 0.021*\"model\" + 0.019*\"vector\" + 0.019*\"processing\" + 0.019*\"space\" + 0.018*\"natural\"\n",
      "2022-02-25 11:52:12,635 : INFO : topic diff=0.286608, rho=0.577350\n",
      "2022-02-25 11:52:12,646 : INFO : -5.330 per-word bound, 40.2 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,647 : INFO : PROGRESS: pass 2, at document #37/37\n",
      "2022-02-25 11:52:12,654 : INFO : topic #0 (0.200): 0.031*\"use\" + 0.031*\"specialization\" + 0.030*\"instructor\" + 0.030*\"nlp\" + 0.026*\"dictionary\" + 0.025*\"class\" + 0.023*\"sentiment\" + 0.022*\"frequency\" + 0.020*\"tweet\" + 0.018*\"logistic\"\n",
      "2022-02-25 11:52:12,654 : INFO : topic #1 (0.200): 0.103*\"class\" + 0.101*\"word\" + 0.088*\"frequency\" + 0.064*\"negative\" + 0.050*\"number\" + 0.043*\"positive\" + 0.037*\"corpus\" + 0.028*\"map\" + 0.025*\"track\" + 0.025*\"unique\"\n",
      "2022-02-25 11:52:12,655 : INFO : topic #2 (0.200): 0.034*\"feature\" + 0.034*\"free\" + 0.034*\"check\" + 0.021*\"use\" + 0.020*\"logistic\" + 0.020*\"regression\" + 0.019*\"classifier\" + 0.019*\"count\" + 0.019*\"coauthor\" + 0.019*\"staff\"\n",
      "2022-02-25 11:52:12,655 : INFO : topic #3 (0.200): 0.076*\"tweet\" + 0.060*\"positive\" + 0.044*\"word\" + 0.043*\"sentiment\" + 0.029*\"analysis\" + 0.028*\"course\" + 0.027*\"example\" + 0.024*\"vocabulary\" + 0.020*\"look\" + 0.020*\"second\"\n",
      "2022-02-25 11:52:12,656 : INFO : topic #4 (0.200): 0.032*\"specialization\" + 0.032*\"word\" + 0.030*\"course\" + 0.022*\"regression\" + 0.022*\"model\" + 0.021*\"vector\" + 0.021*\"processing\" + 0.021*\"space\" + 0.020*\"natural\" + 0.020*\"relationship\"\n",
      "2022-02-25 11:52:12,656 : INFO : topic diff=0.162060, rho=0.500000\n",
      "2022-02-25 11:52:12,670 : INFO : -5.273 per-word bound, 38.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,670 : INFO : PROGRESS: pass 3, at document #37/37\n",
      "2022-02-25 11:52:12,676 : INFO : topic #0 (0.200): 0.032*\"use\" + 0.032*\"specialization\" + 0.031*\"instructor\" + 0.031*\"nlp\" + 0.026*\"dictionary\" + 0.021*\"sentiment\" + 0.021*\"frequency\" + 0.019*\"tweet\" + 0.018*\"class\" + 0.018*\"logistic\"\n",
      "2022-02-25 11:52:12,678 : INFO : topic #1 (0.200): 0.110*\"class\" + 0.102*\"word\" + 0.091*\"frequency\" + 0.070*\"negative\" + 0.049*\"number\" + 0.045*\"positive\" + 0.036*\"corpus\" + 0.027*\"table\" + 0.026*\"map\" + 0.025*\"track\"\n",
      "2022-02-25 11:52:12,679 : INFO : topic #2 (0.200): 0.036*\"feature\" + 0.035*\"free\" + 0.035*\"check\" + 0.021*\"use\" + 0.020*\"logistic\" + 0.020*\"regression\" + 0.019*\"count\" + 0.019*\"classifier\" + 0.019*\"coauthor\" + 0.019*\"staff\"\n",
      "2022-02-25 11:52:12,679 : INFO : topic #3 (0.200): 0.079*\"tweet\" + 0.059*\"positive\" + 0.045*\"sentiment\" + 0.042*\"word\" + 0.029*\"analysis\" + 0.029*\"course\" + 0.028*\"example\" + 0.024*\"vocabulary\" + 0.020*\"look\" + 0.020*\"second\"\n",
      "2022-02-25 11:52:12,680 : INFO : topic #4 (0.200): 0.032*\"specialization\" + 0.032*\"word\" + 0.031*\"course\" + 0.022*\"regression\" + 0.022*\"model\" + 0.022*\"vector\" + 0.022*\"processing\" + 0.021*\"space\" + 0.021*\"natural\" + 0.021*\"relationship\"\n",
      "2022-02-25 11:52:12,681 : INFO : topic diff=0.103846, rho=0.447214\n",
      "2022-02-25 11:52:12,692 : INFO : -5.247 per-word bound, 38.0 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,693 : INFO : PROGRESS: pass 4, at document #37/37\n",
      "2022-02-25 11:52:12,699 : INFO : topic #0 (0.200): 0.032*\"use\" + 0.032*\"specialization\" + 0.032*\"instructor\" + 0.032*\"nlp\" + 0.025*\"dictionary\" + 0.020*\"frequency\" + 0.020*\"sentiment\" + 0.019*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,700 : INFO : topic #1 (0.200): 0.114*\"class\" + 0.102*\"word\" + 0.093*\"frequency\" + 0.073*\"negative\" + 0.048*\"number\" + 0.046*\"positive\" + 0.036*\"corpus\" + 0.029*\"table\" + 0.025*\"map\" + 0.025*\"track\"\n",
      "2022-02-25 11:52:12,701 : INFO : topic #2 (0.200): 0.037*\"feature\" + 0.036*\"free\" + 0.036*\"check\" + 0.021*\"use\" + 0.020*\"logistic\" + 0.020*\"regression\" + 0.020*\"count\" + 0.020*\"classifier\" + 0.020*\"extract\" + 0.020*\"coauthor\"\n",
      "2022-02-25 11:52:12,701 : INFO : topic #3 (0.200): 0.081*\"tweet\" + 0.058*\"positive\" + 0.047*\"sentiment\" + 0.040*\"word\" + 0.029*\"analysis\" + 0.029*\"course\" + 0.029*\"example\" + 0.024*\"vocabulary\" + 0.020*\"look\" + 0.020*\"second\"\n",
      "2022-02-25 11:52:12,702 : INFO : topic #4 (0.200): 0.032*\"specialization\" + 0.032*\"word\" + 0.032*\"course\" + 0.022*\"regression\" + 0.022*\"model\" + 0.022*\"vector\" + 0.022*\"processing\" + 0.022*\"space\" + 0.022*\"natural\" + 0.022*\"relationship\"\n",
      "2022-02-25 11:52:12,702 : INFO : topic diff=0.070239, rho=0.408248\n",
      "2022-02-25 11:52:12,711 : INFO : -5.233 per-word bound, 37.6 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,711 : INFO : PROGRESS: pass 5, at document #37/37\n",
      "2022-02-25 11:52:12,716 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.032*\"specialization\" + 0.032*\"instructor\" + 0.032*\"nlp\" + 0.025*\"dictionary\" + 0.020*\"frequency\" + 0.019*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,717 : INFO : topic #1 (0.200): 0.116*\"class\" + 0.103*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.048*\"positive\" + 0.047*\"number\" + 0.035*\"corpus\" + 0.030*\"table\" + 0.025*\"map\" + 0.024*\"track\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:12,717 : INFO : topic #2 (0.200): 0.037*\"feature\" + 0.037*\"free\" + 0.037*\"check\" + 0.021*\"use\" + 0.020*\"logistic\" + 0.020*\"regression\" + 0.020*\"count\" + 0.020*\"classifier\" + 0.020*\"extract\" + 0.020*\"coauthor\"\n",
      "2022-02-25 11:52:12,718 : INFO : topic #3 (0.200): 0.083*\"tweet\" + 0.056*\"positive\" + 0.047*\"sentiment\" + 0.039*\"word\" + 0.030*\"analysis\" + 0.029*\"course\" + 0.029*\"example\" + 0.024*\"vocabulary\" + 0.020*\"look\" + 0.020*\"second\"\n",
      "2022-02-25 11:52:12,718 : INFO : topic #4 (0.200): 0.032*\"specialization\" + 0.032*\"word\" + 0.032*\"course\" + 0.022*\"regression\" + 0.022*\"model\" + 0.022*\"vector\" + 0.022*\"processing\" + 0.022*\"space\" + 0.022*\"natural\" + 0.022*\"relationship\"\n",
      "2022-02-25 11:52:12,719 : INFO : topic diff=0.048285, rho=0.377964\n",
      "2022-02-25 11:52:12,727 : INFO : -5.225 per-word bound, 37.4 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,728 : INFO : PROGRESS: pass 6, at document #37/37\n",
      "2022-02-25 11:52:12,733 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"instructor\" + 0.033*\"nlp\" + 0.025*\"dictionary\" + 0.020*\"frequency\" + 0.019*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,734 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.075*\"negative\" + 0.049*\"positive\" + 0.046*\"number\" + 0.035*\"corpus\" + 0.031*\"table\" + 0.024*\"map\" + 0.024*\"track\"\n",
      "2022-02-25 11:52:12,735 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.037*\"free\" + 0.037*\"check\" + 0.021*\"use\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.020*\"count\" + 0.020*\"classifier\" + 0.020*\"extract\" + 0.020*\"coauthor\"\n",
      "2022-02-25 11:52:12,735 : INFO : topic #3 (0.200): 0.084*\"tweet\" + 0.055*\"positive\" + 0.048*\"sentiment\" + 0.038*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.023*\"vocabulary\" + 0.021*\"look\" + 0.020*\"second\"\n",
      "2022-02-25 11:52:12,736 : INFO : topic #4 (0.200): 0.033*\"specialization\" + 0.033*\"word\" + 0.032*\"course\" + 0.022*\"regression\" + 0.022*\"model\" + 0.022*\"vector\" + 0.022*\"processing\" + 0.022*\"space\" + 0.022*\"use\" + 0.022*\"natural\"\n",
      "2022-02-25 11:52:12,736 : INFO : topic diff=0.034851, rho=0.353553\n",
      "2022-02-25 11:52:12,746 : INFO : -5.217 per-word bound, 37.2 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,747 : INFO : PROGRESS: pass 7, at document #37/37\n",
      "2022-02-25 11:52:12,752 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"instructor\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.019*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,752 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.050*\"positive\" + 0.046*\"number\" + 0.035*\"corpus\" + 0.031*\"table\" + 0.024*\"map\" + 0.024*\"track\"\n",
      "2022-02-25 11:52:12,753 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"use\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"classifier\" + 0.021*\"extract\"\n",
      "2022-02-25 11:52:12,753 : INFO : topic #3 (0.200): 0.085*\"tweet\" + 0.054*\"positive\" + 0.048*\"sentiment\" + 0.037*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.023*\"vocabulary\" + 0.021*\"second\" + 0.021*\"look\"\n",
      "2022-02-25 11:52:12,754 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.022*\"vector\" + 0.022*\"processing\" + 0.022*\"space\" + 0.022*\"use\" + 0.022*\"natural\"\n",
      "2022-02-25 11:52:12,754 : INFO : topic diff=0.026342, rho=0.333333\n",
      "2022-02-25 11:52:12,771 : INFO : -5.211 per-word bound, 37.0 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,771 : INFO : PROGRESS: pass 8, at document #37/37\n",
      "2022-02-25 11:52:12,779 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"instructor\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,779 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.051*\"positive\" + 0.045*\"number\" + 0.034*\"corpus\" + 0.032*\"table\" + 0.024*\"map\" + 0.024*\"track\"\n",
      "2022-02-25 11:52:12,780 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"use\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\"\n",
      "2022-02-25 11:52:12,780 : INFO : topic #3 (0.200): 0.086*\"tweet\" + 0.053*\"positive\" + 0.048*\"sentiment\" + 0.036*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.022*\"vocabulary\" + 0.021*\"second\" + 0.021*\"look\"\n",
      "2022-02-25 11:52:12,781 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"vector\" + 0.022*\"processing\" + 0.022*\"space\" + 0.022*\"use\" + 0.022*\"natural\"\n",
      "2022-02-25 11:52:12,781 : INFO : topic diff=0.018949, rho=0.316228\n",
      "2022-02-25 11:52:12,791 : INFO : -5.207 per-word bound, 36.9 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,791 : INFO : PROGRESS: pass 9, at document #37/37\n",
      "2022-02-25 11:52:12,800 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"instructor\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,801 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.052*\"positive\" + 0.045*\"number\" + 0.034*\"corpus\" + 0.032*\"table\" + 0.024*\"map\" + 0.024*\"track\"\n",
      "2022-02-25 11:52:12,801 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"use\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\"\n",
      "2022-02-25 11:52:12,802 : INFO : topic #3 (0.200): 0.086*\"tweet\" + 0.052*\"positive\" + 0.049*\"sentiment\" + 0.036*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.022*\"vocabulary\" + 0.021*\"second\" + 0.021*\"regression\"\n",
      "2022-02-25 11:52:12,803 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"processing\" + 0.023*\"use\" + 0.023*\"language\"\n",
      "2022-02-25 11:52:12,803 : INFO : topic diff=0.014108, rho=0.301511\n",
      "2022-02-25 11:52:12,813 : INFO : -5.205 per-word bound, 36.9 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,813 : INFO : PROGRESS: pass 10, at document #37/37\n",
      "2022-02-25 11:52:12,819 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"instructor\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"tweet\" + 0.018*\"logistic\" + 0.018*\"regression\"\n",
      "2022-02-25 11:52:12,820 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.053*\"positive\" + 0.045*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.024*\"map\" + 0.023*\"track\"\n",
      "2022-02-25 11:52:12,820 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"use\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"extract\" + 0.021*\"classifier\"\n",
      "2022-02-25 11:52:12,821 : INFO : topic #3 (0.200): 0.086*\"tweet\" + 0.051*\"positive\" + 0.049*\"sentiment\" + 0.036*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.022*\"vocabulary\" + 0.021*\"second\" + 0.021*\"regression\"\n",
      "2022-02-25 11:52:12,821 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"use\" + 0.023*\"processing\" + 0.023*\"language\"\n",
      "2022-02-25 11:52:12,822 : INFO : topic diff=0.010730, rho=0.288675\n",
      "2022-02-25 11:52:12,831 : INFO : -5.204 per-word bound, 36.9 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,831 : INFO : PROGRESS: pass 11, at document #37/37\n",
      "2022-02-25 11:52:12,838 : INFO : topic #0 (0.200): 0.033*\"use\" + 0.033*\"instructor\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"logistic\" + 0.018*\"regression\" + 0.018*\"language\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:12,838 : INFO : topic #1 (0.200): 0.117*\"class\" + 0.103*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.054*\"positive\" + 0.045*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.023*\"map\" + 0.023*\"track\"\n",
      "2022-02-25 11:52:12,839 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"use\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"extract\" + 0.021*\"classifier\"\n",
      "2022-02-25 11:52:12,839 : INFO : topic #3 (0.200): 0.087*\"tweet\" + 0.050*\"positive\" + 0.049*\"sentiment\" + 0.035*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.021*\"vocabulary\" + 0.021*\"second\" + 0.021*\"regression\"\n",
      "2022-02-25 11:52:12,840 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"use\" + 0.023*\"space\" + 0.023*\"processing\" + 0.023*\"language\"\n",
      "2022-02-25 11:52:12,840 : INFO : topic diff=0.008627, rho=0.277350\n",
      "2022-02-25 11:52:12,851 : INFO : -5.203 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,852 : INFO : PROGRESS: pass 12, at document #37/37\n",
      "2022-02-25 11:52:12,857 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"logistic\" + 0.018*\"regression\" + 0.018*\"language\"\n",
      "2022-02-25 11:52:12,857 : INFO : topic #1 (0.200): 0.118*\"class\" + 0.102*\"word\" + 0.096*\"frequency\" + 0.076*\"negative\" + 0.054*\"positive\" + 0.044*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.023*\"map\" + 0.023*\"vocabulary\"\n",
      "2022-02-25 11:52:12,858 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"use\" + 0.021*\"logistic\" + 0.021*\"regression\" + 0.021*\"extract\" + 0.021*\"classifier\"\n",
      "2022-02-25 11:52:12,859 : INFO : topic #3 (0.200): 0.087*\"tweet\" + 0.050*\"positive\" + 0.049*\"sentiment\" + 0.035*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.021*\"second\" + 0.021*\"regression\" + 0.021*\"logistic\"\n",
      "2022-02-25 11:52:12,860 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"use\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"processing\" + 0.023*\"language\"\n",
      "2022-02-25 11:52:12,861 : INFO : topic diff=0.007099, rho=0.267261\n",
      "2022-02-25 11:52:12,870 : INFO : -5.203 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,870 : INFO : PROGRESS: pass 13, at document #37/37\n",
      "2022-02-25 11:52:12,875 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"logistic\" + 0.018*\"classifier\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:12,875 : INFO : topic #1 (0.200): 0.119*\"class\" + 0.102*\"word\" + 0.096*\"frequency\" + 0.075*\"negative\" + 0.055*\"positive\" + 0.044*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.024*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,876 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"use\" + 0.021*\"extract\" + 0.021*\"logistic\" + 0.021*\"classifier\" + 0.021*\"regression\"\n",
      "2022-02-25 11:52:12,877 : INFO : topic #3 (0.200): 0.087*\"tweet\" + 0.049*\"positive\" + 0.049*\"sentiment\" + 0.035*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.021*\"regression\" + 0.021*\"second\" + 0.021*\"logistic\"\n",
      "2022-02-25 11:52:12,878 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"model\" + 0.023*\"use\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"language\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,879 : INFO : topic diff=0.005767, rho=0.258199\n",
      "2022-02-25 11:52:12,886 : INFO : -5.202 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,887 : INFO : PROGRESS: pass 14, at document #37/37\n",
      "2022-02-25 11:52:12,892 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"specialization\" + 0.033*\"nlp\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"logistic\" + 0.018*\"classifier\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:12,893 : INFO : topic #1 (0.200): 0.120*\"class\" + 0.102*\"word\" + 0.096*\"frequency\" + 0.075*\"negative\" + 0.055*\"positive\" + 0.044*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.024*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,893 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"staff\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:12,894 : INFO : topic #3 (0.200): 0.087*\"tweet\" + 0.049*\"sentiment\" + 0.049*\"positive\" + 0.035*\"word\" + 0.030*\"analysis\" + 0.030*\"course\" + 0.030*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,894 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"use\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"language\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,895 : INFO : topic diff=0.004724, rho=0.250000\n",
      "2022-02-25 11:52:12,903 : INFO : -5.201 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,903 : INFO : PROGRESS: pass 15, at document #37/37\n",
      "2022-02-25 11:52:12,909 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:12,910 : INFO : topic #1 (0.200): 0.121*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.055*\"positive\" + 0.044*\"number\" + 0.034*\"corpus\" + 0.033*\"table\" + 0.024*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,910 : INFO : topic #2 (0.200): 0.038*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"library\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:12,911 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.049*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.030*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,911 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"use\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"language\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,912 : INFO : topic diff=0.003908, rho=0.242536\n",
      "2022-02-25 11:52:12,920 : INFO : -5.201 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,921 : INFO : PROGRESS: pass 16, at document #37/37\n",
      "2022-02-25 11:52:12,925 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:12,926 : INFO : topic #1 (0.200): 0.122*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.056*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.024*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,926 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"paper\" + 0.021*\"library\"\n",
      "2022-02-25 11:52:12,927 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.048*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,927 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"regression\" + 0.023*\"use\" + 0.023*\"model\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"language\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,928 : INFO : topic diff=0.003265, rho=0.235702\n",
      "2022-02-25 11:52:12,938 : INFO : -5.200 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:12,939 : INFO : PROGRESS: pass 17, at document #37/37\n",
      "2022-02-25 11:52:12,943 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:12,944 : INFO : topic #1 (0.200): 0.122*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.056*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.025*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,945 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"paper\" + 0.021*\"library\" + 0.021*\"transformer\"\n",
      "2022-02-25 11:52:12,946 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.048*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,946 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"space\" + 0.023*\"language\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,947 : INFO : topic diff=0.002757, rho=0.229416\n",
      "2022-02-25 11:52:12,954 : INFO : -5.200 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,954 : INFO : PROGRESS: pass 18, at document #37/37\n",
      "2022-02-25 11:52:12,960 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:12,961 : INFO : topic #1 (0.200): 0.123*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.056*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.025*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,961 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"transformer\" + 0.021*\"paper\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:12,962 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.048*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,962 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"vector\" + 0.023*\"space\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,963 : INFO : topic diff=0.002353, rho=0.223607\n",
      "2022-02-25 11:52:12,974 : INFO : -5.200 per-word bound, 36.8 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,974 : INFO : PROGRESS: pass 19, at document #37/37\n",
      "2022-02-25 11:52:12,982 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:12,983 : INFO : topic #1 (0.200): 0.123*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.057*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.025*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:12,983 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.038*\"free\" + 0.038*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"coauthor\" + 0.021*\"library\"\n",
      "2022-02-25 11:52:12,984 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:12,984 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:12,985 : INFO : topic diff=0.002026, rho=0.218218\n",
      "2022-02-25 11:52:12,994 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:12,995 : INFO : PROGRESS: pass 20, at document #37/37\n",
      "2022-02-25 11:52:13,001 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:13,002 : INFO : topic #1 (0.200): 0.124*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.057*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.025*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,003 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"staff\" + 0.021*\"transformer\"\n",
      "2022-02-25 11:52:13,003 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,004 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,004 : INFO : topic diff=0.001759, rho=0.213201\n",
      "2022-02-25 11:52:13,013 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,014 : INFO : PROGRESS: pass 21, at document #37/37\n",
      "2022-02-25 11:52:13,020 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"sentiment\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\"\n",
      "2022-02-25 11:52:13,020 : INFO : topic #1 (0.200): 0.124*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.075*\"negative\" + 0.057*\"positive\" + 0.044*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.025*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,021 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"paper\" + 0.021*\"staff\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,021 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,022 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,022 : INFO : topic diff=0.001538, rho=0.208514\n",
      "2022-02-25 11:52:13,032 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,033 : INFO : PROGRESS: pass 22, at document #37/37\n",
      "2022-02-25 11:52:13,039 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\" + 0.018*\"sentiment\"\n",
      "2022-02-25 11:52:13,040 : INFO : topic #1 (0.200): 0.124*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.074*\"negative\" + 0.057*\"positive\" + 0.043*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,040 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"paper\" + 0.021*\"staff\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,041 : INFO : topic #3 (0.200): 0.088*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,042 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,042 : INFO : topic diff=0.001352, rho=0.204124\n",
      "2022-02-25 11:52:13,051 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,051 : INFO : PROGRESS: pass 23, at document #37/37\n",
      "2022-02-25 11:52:13,056 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learn\" + 0.018*\"text\" + 0.018*\"deep\"\n",
      "2022-02-25 11:52:13,057 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.102*\"word\" + 0.095*\"frequency\" + 0.074*\"negative\" + 0.057*\"positive\" + 0.043*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,058 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"paper\" + 0.021*\"staff\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,059 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.035*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,060 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,061 : INFO : topic diff=0.001192, rho=0.200000\n",
      "2022-02-25 11:52:13,068 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,069 : INFO : PROGRESS: pass 24, at document #37/37\n",
      "2022-02-25 11:52:13,075 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learn\" + 0.018*\"learning\"\n",
      "2022-02-25 11:52:13,075 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,076 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"transformer\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:13,077 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.047*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,077 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,078 : INFO : topic diff=0.001053, rho=0.196116\n",
      "2022-02-25 11:52:13,086 : INFO : -5.199 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,086 : INFO : PROGRESS: pass 25, at document #37/37\n",
      "2022-02-25 11:52:13,091 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,092 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,093 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"google\" + 0.021*\"staff\" + 0.021*\"library\"\n",
      "2022-02-25 11:52:13,094 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,094 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,094 : INFO : topic diff=0.000932, rho=0.192450\n",
      "2022-02-25 11:52:13,102 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,103 : INFO : PROGRESS: pass 26, at document #37/37\n",
      "2022-02-25 11:52:13,108 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,109 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"corpus\" + 0.033*\"table\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,109 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"tensorflow\" + 0.021*\"transformer\" + 0.021*\"google\"\n",
      "2022-02-25 11:52:13,110 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,110 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,111 : INFO : topic diff=0.000823, rho=0.188982\n",
      "2022-02-25 11:52:13,121 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,121 : INFO : PROGRESS: pass 27, at document #37/37\n",
      "2022-02-25 11:52:13,126 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,127 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,128 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"google\" + 0.021*\"staff\" + 0.021*\"library\"\n",
      "2022-02-25 11:52:13,128 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,129 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,129 : INFO : topic diff=0.000728, rho=0.185695\n",
      "2022-02-25 11:52:13,138 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,139 : INFO : PROGRESS: pass 28, at document #37/37\n",
      "2022-02-25 11:52:13,144 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,145 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,145 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"library\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:13,146 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,147 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,147 : INFO : topic diff=0.000641, rho=0.182574\n",
      "2022-02-25 11:52:13,157 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,157 : INFO : PROGRESS: pass 29, at document #37/37\n",
      "2022-02-25 11:52:13,162 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,163 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,163 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"library\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:13,164 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,164 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,165 : INFO : topic diff=0.000565, rho=0.179605\n",
      "2022-02-25 11:52:13,175 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,176 : INFO : PROGRESS: pass 30, at document #37/37\n",
      "2022-02-25 11:52:13,181 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,181 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.026*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,182 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"library\" + 0.021*\"paper\"\n",
      "2022-02-25 11:52:13,182 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,183 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,183 : INFO : topic diff=0.000497, rho=0.176777\n",
      "2022-02-25 11:52:13,193 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,193 : INFO : PROGRESS: pass 31, at document #37/37\n",
      "2022-02-25 11:52:13,200 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,200 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,201 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"staff\" + 0.021*\"library\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,201 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,202 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,202 : INFO : topic diff=0.000437, rho=0.174078\n",
      "2022-02-25 11:52:13,212 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,212 : INFO : PROGRESS: pass 32, at document #37/37\n",
      "2022-02-25 11:52:13,218 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,218 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.058*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,219 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"google\" + 0.021*\"coauthor\" + 0.021*\"ukasz\"\n",
      "2022-02-25 11:52:13,219 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,220 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,220 : INFO : topic diff=0.000384, rho=0.171499\n",
      "2022-02-25 11:52:13,231 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,232 : INFO : PROGRESS: pass 33, at document #37/37\n",
      "2022-02-25 11:52:13,237 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,238 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,238 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"tensorflow\" + 0.021*\"google\"\n",
      "2022-02-25 11:52:13,238 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.046*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,239 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,239 : INFO : topic diff=0.000338, rho=0.169031\n",
      "2022-02-25 11:52:13,251 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,252 : INFO : PROGRESS: pass 34, at document #37/37\n",
      "2022-02-25 11:52:13,258 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,258 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,259 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"scientist\" + 0.021*\"coauthor\" + 0.021*\"research\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,260 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,261 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,261 : INFO : topic diff=0.000297, rho=0.166667\n",
      "2022-02-25 11:52:13,269 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,270 : INFO : PROGRESS: pass 35, at document #37/37\n",
      "2022-02-25 11:52:13,276 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,277 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,277 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"transformer\" + 0.021*\"coauthor\" + 0.021*\"google\"\n",
      "2022-02-25 11:52:13,278 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,279 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"model\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,279 : INFO : topic diff=0.000261, rho=0.164399\n",
      "2022-02-25 11:52:13,287 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,287 : INFO : PROGRESS: pass 36, at document #37/37\n",
      "2022-02-25 11:52:13,292 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,293 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,293 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"library\" + 0.021*\"paper\" + 0.021*\"trax\"\n",
      "2022-02-25 11:52:13,294 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,294 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"model\"\n",
      "2022-02-25 11:52:13,295 : INFO : topic diff=0.000231, rho=0.162221\n",
      "2022-02-25 11:52:13,304 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,305 : INFO : PROGRESS: pass 37, at document #37/37\n",
      "2022-02-25 11:52:13,310 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,311 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,312 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"ukasz\" + 0.021*\"library\" + 0.021*\"trax\"\n",
      "2022-02-25 11:52:13,312 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,313 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"regression\" + 0.023*\"language\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,314 : INFO : topic diff=0.000204, rho=0.160128\n",
      "2022-02-25 11:52:13,322 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,323 : INFO : PROGRESS: pass 38, at document #37/37\n",
      "2022-02-25 11:52:13,328 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,329 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,329 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"ukasz\" + 0.021*\"library\" + 0.021*\"trax\"\n",
      "2022-02-25 11:52:13,330 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,330 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,330 : INFO : topic diff=0.000179, rho=0.158114\n",
      "2022-02-25 11:52:13,339 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,339 : INFO : PROGRESS: pass 39, at document #37/37\n",
      "2022-02-25 11:52:13,345 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,345 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,346 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"ukasz\" + 0.021*\"library\" + 0.021*\"trax\"\n",
      "2022-02-25 11:52:13,347 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,347 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,348 : INFO : topic diff=0.000159, rho=0.156174\n",
      "2022-02-25 11:52:13,356 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,356 : INFO : PROGRESS: pass 40, at document #37/37\n",
      "2022-02-25 11:52:13,361 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,362 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,362 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"tensorflow\" + 0.021*\"ukasz\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,363 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,364 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,364 : INFO : topic diff=0.000142, rho=0.154303\n",
      "2022-02-25 11:52:13,372 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,372 : INFO : PROGRESS: pass 41, at document #37/37\n",
      "2022-02-25 11:52:13,378 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,378 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,379 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"tensorflow\" + 0.021*\"ukasz\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,379 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,380 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,381 : INFO : topic diff=0.000127, rho=0.152499\n",
      "2022-02-25 11:52:13,388 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,389 : INFO : PROGRESS: pass 42, at document #37/37\n",
      "2022-02-25 11:52:13,394 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,394 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,395 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"transformer\" + 0.021*\"tensorflow\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,395 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,396 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,396 : INFO : topic diff=0.000114, rho=0.150756\n",
      "2022-02-25 11:52:13,404 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,405 : INFO : PROGRESS: pass 43, at document #37/37\n",
      "2022-02-25 11:52:13,411 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,412 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,413 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"transformer\" + 0.021*\"tensorflow\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,413 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,413 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,414 : INFO : topic diff=0.000102, rho=0.149071\n",
      "2022-02-25 11:52:13,424 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,424 : INFO : PROGRESS: pass 44, at document #37/37\n",
      "2022-02-25 11:52:13,430 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,431 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,431 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"paper\" + 0.021*\"transformer\" + 0.021*\"trax\"\n",
      "2022-02-25 11:52:13,432 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,433 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,433 : INFO : topic diff=0.000091, rho=0.147442\n",
      "2022-02-25 11:52:13,440 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,441 : INFO : PROGRESS: pass 45, at document #37/37\n",
      "2022-02-25 11:52:13,451 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,452 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,452 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"google\" + 0.021*\"ukasz\" + 0.021*\"coauthor\"\n",
      "2022-02-25 11:52:13,453 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,453 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,454 : INFO : topic diff=0.000083, rho=0.145865\n",
      "2022-02-25 11:52:13,466 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,466 : INFO : PROGRESS: pass 46, at document #37/37\n",
      "2022-02-25 11:52:13,473 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,473 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,474 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"research\" + 0.021*\"google\"\n",
      "2022-02-25 11:52:13,475 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,476 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,477 : INFO : topic diff=0.000075, rho=0.144338\n",
      "2022-02-25 11:52:13,489 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,490 : INFO : PROGRESS: pass 47, at document #37/37\n",
      "2022-02-25 11:52:13,496 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,497 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,497 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"check\" + 0.039*\"free\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"google\" + 0.021*\"trax\" + 0.021*\"tensorflow\"\n",
      "2022-02-25 11:52:13,498 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,499 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,499 : INFO : topic diff=0.000068, rho=0.142857\n",
      "2022-02-25 11:52:13,507 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,507 : INFO : PROGRESS: pass 48, at document #37/37\n",
      "2022-02-25 11:52:13,512 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,513 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,514 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,515 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,515 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,516 : INFO : topic diff=0.000061, rho=0.141421\n",
      "2022-02-25 11:52:13,525 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,526 : INFO : PROGRESS: pass 49, at document #37/37\n",
      "2022-02-25 11:52:13,534 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,535 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,536 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,536 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,537 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,538 : INFO : topic diff=0.000056, rho=0.140028\n",
      "2022-02-25 11:52:13,546 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,547 : INFO : PROGRESS: pass 50, at document #37/37\n",
      "2022-02-25 11:52:13,552 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,552 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,553 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,554 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,554 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,555 : INFO : topic diff=0.000052, rho=0.138675\n",
      "2022-02-25 11:52:13,566 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,567 : INFO : PROGRESS: pass 51, at document #37/37\n",
      "2022-02-25 11:52:13,573 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,574 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,575 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,576 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,578 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,578 : INFO : topic diff=0.000047, rho=0.137361\n",
      "2022-02-25 11:52:13,595 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,595 : INFO : PROGRESS: pass 52, at document #37/37\n",
      "2022-02-25 11:52:13,603 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,604 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.027*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,605 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,606 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,606 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,607 : INFO : topic diff=0.000044, rho=0.136083\n",
      "2022-02-25 11:52:13,619 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,619 : INFO : PROGRESS: pass 53, at document #37/37\n",
      "2022-02-25 11:52:13,630 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,631 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,632 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,633 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,633 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,634 : INFO : topic diff=0.000040, rho=0.134840\n",
      "2022-02-25 11:52:13,647 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,648 : INFO : PROGRESS: pass 54, at document #37/37\n",
      "2022-02-25 11:52:13,653 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,654 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,654 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,655 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,656 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,656 : INFO : topic diff=0.000037, rho=0.133631\n",
      "2022-02-25 11:52:13,668 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,669 : INFO : PROGRESS: pass 55, at document #37/37\n",
      "2022-02-25 11:52:13,675 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,675 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.059*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,676 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,677 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,678 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"processing\" + 0.023*\"relationship\"\n",
      "2022-02-25 11:52:13,679 : INFO : topic diff=0.000035, rho=0.132453\n",
      "2022-02-25 11:52:13,694 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,695 : INFO : PROGRESS: pass 56, at document #37/37\n",
      "2022-02-25 11:52:13,705 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,705 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,706 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,706 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,707 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,707 : INFO : topic diff=0.000031, rho=0.131306\n",
      "2022-02-25 11:52:13,718 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,718 : INFO : PROGRESS: pass 57, at document #37/37\n",
      "2022-02-25 11:52:13,724 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,725 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,726 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,727 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,728 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,728 : INFO : topic diff=0.000030, rho=0.130189\n",
      "2022-02-25 11:52:13,737 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,737 : INFO : PROGRESS: pass 58, at document #37/37\n",
      "2022-02-25 11:52:13,743 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,744 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,745 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,745 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,746 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,746 : INFO : topic diff=0.000028, rho=0.129099\n",
      "2022-02-25 11:52:13,756 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,756 : INFO : PROGRESS: pass 59, at document #37/37\n",
      "2022-02-25 11:52:13,762 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,762 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,763 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,763 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,764 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"natural\"\n",
      "2022-02-25 11:52:13,764 : INFO : topic diff=0.000025, rho=0.128037\n",
      "2022-02-25 11:52:13,773 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,774 : INFO : PROGRESS: pass 60, at document #37/37\n",
      "2022-02-25 11:52:13,780 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,780 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,781 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,781 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,782 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,782 : INFO : topic diff=0.000024, rho=0.127000\n",
      "2022-02-25 11:52:13,794 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,795 : INFO : PROGRESS: pass 61, at document #37/37\n",
      "2022-02-25 11:52:13,802 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,802 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,803 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,803 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,804 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,804 : INFO : topic diff=0.000022, rho=0.125988\n",
      "2022-02-25 11:52:13,816 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,817 : INFO : PROGRESS: pass 62, at document #37/37\n",
      "2022-02-25 11:52:13,822 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,823 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,824 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,825 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,826 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,827 : INFO : topic diff=0.000021, rho=0.125000\n",
      "2022-02-25 11:52:13,841 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,841 : INFO : PROGRESS: pass 63, at document #37/37\n",
      "2022-02-25 11:52:13,849 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,850 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,851 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,852 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.045*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,853 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,854 : INFO : topic diff=0.000020, rho=0.124035\n",
      "2022-02-25 11:52:13,868 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,868 : INFO : PROGRESS: pass 64, at document #37/37\n",
      "2022-02-25 11:52:13,874 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,874 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,875 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,876 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,877 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,878 : INFO : topic diff=0.000018, rho=0.123091\n",
      "2022-02-25 11:52:13,886 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,887 : INFO : PROGRESS: pass 65, at document #37/37\n",
      "2022-02-25 11:52:13,892 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,893 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,894 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,895 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,895 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,896 : INFO : topic diff=0.000017, rho=0.122169\n",
      "2022-02-25 11:52:13,904 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,905 : INFO : PROGRESS: pass 66, at document #37/37\n",
      "2022-02-25 11:52:13,909 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,910 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,911 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,911 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,912 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,912 : INFO : topic diff=0.000017, rho=0.121268\n",
      "2022-02-25 11:52:13,925 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,926 : INFO : PROGRESS: pass 67, at document #37/37\n",
      "2022-02-25 11:52:13,935 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"deep\" + 0.018*\"learning\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,935 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,936 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,937 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,937 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,938 : INFO : topic diff=0.000016, rho=0.120386\n",
      "2022-02-25 11:52:13,948 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,949 : INFO : PROGRESS: pass 68, at document #37/37\n",
      "2022-02-25 11:52:13,953 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,954 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,954 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:13,955 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,955 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,955 : INFO : topic diff=0.000015, rho=0.119523\n",
      "2022-02-25 11:52:13,968 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,969 : INFO : PROGRESS: pass 69, at document #37/37\n",
      "2022-02-25 11:52:13,979 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:13,979 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:13,980 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:13,980 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:13,981 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:13,981 : INFO : topic diff=0.000014, rho=0.118678\n",
      "2022-02-25 11:52:13,992 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:13,993 : INFO : PROGRESS: pass 70, at document #37/37\n",
      "2022-02-25 11:52:13,999 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,000 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,000 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,001 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,002 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,002 : INFO : topic diff=0.000013, rho=0.117851\n",
      "2022-02-25 11:52:14,019 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,020 : INFO : PROGRESS: pass 71, at document #37/37\n",
      "2022-02-25 11:52:14,024 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,025 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,026 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,027 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,028 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,029 : INFO : topic diff=0.000012, rho=0.117041\n",
      "2022-02-25 11:52:14,037 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,038 : INFO : PROGRESS: pass 72, at document #37/37\n",
      "2022-02-25 11:52:14,044 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,045 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,045 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,046 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,046 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,047 : INFO : topic diff=0.000012, rho=0.116248\n",
      "2022-02-25 11:52:14,055 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,055 : INFO : PROGRESS: pass 73, at document #37/37\n",
      "2022-02-25 11:52:14,061 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,062 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,062 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,063 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,064 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,064 : INFO : topic diff=0.000011, rho=0.115470\n",
      "2022-02-25 11:52:14,071 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,072 : INFO : PROGRESS: pass 74, at document #37/37\n",
      "2022-02-25 11:52:14,077 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,078 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,079 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,079 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,080 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,080 : INFO : topic diff=0.000010, rho=0.114708\n",
      "2022-02-25 11:52:14,089 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,089 : INFO : PROGRESS: pass 75, at document #37/37\n",
      "2022-02-25 11:52:14,095 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,096 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 11:52:14,096 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,097 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,097 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,097 : INFO : topic diff=0.000009, rho=0.113961\n",
      "2022-02-25 11:52:14,106 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,106 : INFO : PROGRESS: pass 76, at document #37/37\n",
      "2022-02-25 11:52:14,112 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,113 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,113 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,114 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,114 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,115 : INFO : topic diff=0.000009, rho=0.113228\n",
      "2022-02-25 11:52:14,125 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,125 : INFO : PROGRESS: pass 77, at document #37/37\n",
      "2022-02-25 11:52:14,131 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,132 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,132 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,133 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,134 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,135 : INFO : topic diff=0.000009, rho=0.112509\n",
      "2022-02-25 11:52:14,146 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,147 : INFO : PROGRESS: pass 78, at document #37/37\n",
      "2022-02-25 11:52:14,153 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,154 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,155 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,156 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,157 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,158 : INFO : topic diff=0.000009, rho=0.111803\n",
      "2022-02-25 11:52:14,173 : INFO : -5.198 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 37 documents with 286 words\n",
      "2022-02-25 11:52:14,173 : INFO : PROGRESS: pass 79, at document #37/37\n",
      "2022-02-25 11:52:14,184 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,185 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,185 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,186 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,186 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n",
      "2022-02-25 11:52:14,187 : INFO : topic diff=0.000008, rho=0.111111\n",
      "2022-02-25 11:52:14,188 : INFO : topic #0 (0.200): 0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"\n",
      "2022-02-25 11:52:14,188 : INFO : topic #1 (0.200): 0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"\n",
      "2022-02-25 11:52:14,189 : INFO : topic #2 (0.200): 0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"\n",
      "2022-02-25 11:52:14,190 : INFO : topic #3 (0.200): 0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"\n",
      "2022-02-25 11:52:14,191 : INFO : topic #4 (0.200): 0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"instructor\" + 0.033*\"use\" + 0.033*\"nlp\" + 0.033*\"specialization\" + 0.024*\"dictionary\" + 0.019*\"frequency\" + 0.018*\"classifier\" + 0.018*\"learning\" + 0.018*\"deep\" + 0.018*\"learn\"'),\n",
       " (1,\n",
       "  '0.125*\"class\" + 0.101*\"word\" + 0.094*\"frequency\" + 0.074*\"negative\" + 0.060*\"positive\" + 0.043*\"number\" + 0.033*\"table\" + 0.033*\"corpus\" + 0.028*\"vocabulary\" + 0.023*\"map\"'),\n",
       " (2,\n",
       "  '0.039*\"feature\" + 0.039*\"free\" + 0.039*\"check\" + 0.021*\"count\" + 0.021*\"look\" + 0.021*\"extract\" + 0.021*\"classifier\" + 0.021*\"coauthor\" + 0.021*\"scientist\" + 0.021*\"research\"'),\n",
       " (3,\n",
       "  '0.089*\"tweet\" + 0.050*\"sentiment\" + 0.044*\"positive\" + 0.034*\"word\" + 0.031*\"analysis\" + 0.031*\"course\" + 0.031*\"example\" + 0.021*\"regression\" + 0.021*\"logistic\" + 0.021*\"second\"'),\n",
       " (4,\n",
       "  '0.033*\"word\" + 0.033*\"specialization\" + 0.033*\"course\" + 0.023*\"use\" + 0.023*\"language\" + 0.023*\"regression\" + 0.023*\"space\" + 0.023*\"vector\" + 0.023*\"relationship\" + 0.023*\"processing\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bec4a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(4, 0.98417056)], 0),\n",
       " ([(0, 0.9380261),\n",
       "   (1, 0.015386427),\n",
       "   (2, 0.0153875975),\n",
       "   (3, 0.015648859),\n",
       "   (4, 0.0155510735)],\n",
       "  1),\n",
       " ([(0, 0.8852855),\n",
       "   (1, 0.028574489),\n",
       "   (2, 0.028576506),\n",
       "   (3, 0.028656222),\n",
       "   (4, 0.028907303)],\n",
       "  2),\n",
       " ([(0, 0.020482922),\n",
       "   (1, 0.020003501),\n",
       "   (2, 0.020005811),\n",
       "   (3, 0.020003323),\n",
       "   (4, 0.91950446)],\n",
       "  3),\n",
       " ([(0, 0.015387056),\n",
       "   (1, 0.015386317),\n",
       "   (2, 0.9384541),\n",
       "   (3, 0.015386228),\n",
       "   (4, 0.015386323)],\n",
       "  4),\n",
       " ([(3, 0.9702059)], 5),\n",
       " ([(0, 0.022274164),\n",
       "   (1, 0.022225868),\n",
       "   (2, 0.022282915),\n",
       "   (3, 0.022296114),\n",
       "   (4, 0.9109209)],\n",
       "  6),\n",
       " ([(0, 0.066678554),\n",
       "   (1, 0.06667493),\n",
       "   (2, 0.7332971),\n",
       "   (3, 0.0666745),\n",
       "   (4, 0.06667496)],\n",
       "  7),\n",
       " ([(0, 0.011974327),\n",
       "   (1, 0.011766281),\n",
       "   (2, 0.011999774),\n",
       "   (3, 0.9524016),\n",
       "   (4, 0.011858029)],\n",
       "  8),\n",
       " ([(0, 0.9379204),\n",
       "   (1, 0.015386319),\n",
       "   (2, 0.015628453),\n",
       "   (3, 0.015556975),\n",
       "   (4, 0.015507896)],\n",
       "  9),\n",
       " ([(0, 0.033335067),\n",
       "   (1, 0.86641765),\n",
       "   (2, 0.03333533),\n",
       "   (3, 0.033525035),\n",
       "   (4, 0.033386882)],\n",
       "  10),\n",
       " ([(0, 0.02857272),\n",
       "   (1, 0.8855559),\n",
       "   (2, 0.028572923),\n",
       "   (3, 0.028651182),\n",
       "   (4, 0.0286472)],\n",
       "  11),\n",
       " ([(0, 0.022664068),\n",
       "   (1, 0.022224441),\n",
       "   (2, 0.91003597),\n",
       "   (3, 0.02259832),\n",
       "   (4, 0.022477275)],\n",
       "  12),\n",
       " ([(0, 0.06668219),\n",
       "   (1, 0.066677816),\n",
       "   (2, 0.06752316),\n",
       "   (3, 0.7319754),\n",
       "   (4, 0.06714146)],\n",
       "  13),\n",
       " ([(0, 0.3983951),\n",
       "   (1, 0.40156063),\n",
       "   (2, 0.066687025),\n",
       "   (3, 0.066678315),\n",
       "   (4, 0.066678934)],\n",
       "  14),\n",
       " ([(0, 0.06680271),\n",
       "   (1, 0.066670656),\n",
       "   (2, 0.066673175),\n",
       "   (3, 0.7330933),\n",
       "   (4, 0.06676009)],\n",
       "  15),\n",
       " ([(0, 0.040003214),\n",
       "   (1, 0.8396775),\n",
       "   (2, 0.040003717),\n",
       "   (3, 0.04023345),\n",
       "   (4, 0.04008213)],\n",
       "  16),\n",
       " ([(0, 0.0400064),\n",
       "   (1, 0.58165544),\n",
       "   (2, 0.040007405),\n",
       "   (3, 0.2982176),\n",
       "   (4, 0.040113166)],\n",
       "  17),\n",
       " ([(0, 0.033495106),\n",
       "   (1, 0.20007187),\n",
       "   (2, 0.03333919),\n",
       "   (3, 0.6996509),\n",
       "   (4, 0.03344296)],\n",
       "  18),\n",
       " ([(0, 0.03353799),\n",
       "   (1, 0.46749294),\n",
       "   (2, 0.033336118),\n",
       "   (3, 0.43216112),\n",
       "   (4, 0.033471845)],\n",
       "  19),\n",
       " ([(0, 0.022302942),\n",
       "   (1, 0.66653126),\n",
       "   (2, 0.022224886),\n",
       "   (3, 0.26666343),\n",
       "   (4, 0.022277541)],\n",
       "  20),\n",
       " ([(0, 0.0668002),\n",
       "   (1, 0.068212934),\n",
       "   (2, 0.06667003),\n",
       "   (3, 0.7315584),\n",
       "   (4, 0.06675843)],\n",
       "  21),\n",
       " ([(0, 0.066677995),\n",
       "   (1, 0.06903483),\n",
       "   (2, 0.067503214),\n",
       "   (3, 0.7296444),\n",
       "   (4, 0.06713956)],\n",
       "  22),\n",
       " ([(0, 0.025057059),\n",
       "   (1, 0.4891384),\n",
       "   (2, 0.0254224),\n",
       "   (3, 0.43531737),\n",
       "   (4, 0.025064759)],\n",
       "  23),\n",
       " ([(0, 0.022243707),\n",
       "   (1, 0.022720564),\n",
       "   (2, 0.022224708),\n",
       "   (3, 0.9105032),\n",
       "   (4, 0.022307768)],\n",
       "  24),\n",
       " ([(0, 0.06680325),\n",
       "   (1, 0.73242307),\n",
       "   (2, 0.066669375),\n",
       "   (3, 0.06743605),\n",
       "   (4, 0.06666829)],\n",
       "  25),\n",
       " ([(0, 0.050014798),\n",
       "   (1, 0.052200098),\n",
       "   (2, 0.05088856),\n",
       "   (3, 0.051142067),\n",
       "   (4, 0.7957545)],\n",
       "  26),\n",
       " ([(0, 0.04000477),\n",
       "   (1, 0.040003367),\n",
       "   (2, 0.8399854),\n",
       "   (3, 0.040003154),\n",
       "   (4, 0.040003337)],\n",
       "  27),\n",
       " ([(0, 0.04004583),\n",
       "   (1, 0.83993834),\n",
       "   (2, 0.04000728),\n",
       "   (3, 0.040004145),\n",
       "   (4, 0.040004384)],\n",
       "  28),\n",
       " ([(0, 0.033361122),\n",
       "   (1, 0.034001537),\n",
       "   (2, 0.03333803),\n",
       "   (3, 0.8657717),\n",
       "   (4, 0.03352762)],\n",
       "  29),\n",
       " ([(0, 0.06681033),\n",
       "   (1, 0.7331845),\n",
       "   (2, 0.066669084),\n",
       "   (3, 0.066668026),\n",
       "   (4, 0.06666808)],\n",
       "  30),\n",
       " ([(0, 0.022246962),\n",
       "   (1, 0.46661443),\n",
       "   (2, 0.46628752),\n",
       "   (3, 0.022494089),\n",
       "   (4, 0.022356972)],\n",
       "  31),\n",
       " ([(0, 0.028590424),\n",
       "   (1, 0.885508),\n",
       "   (2, 0.028573612),\n",
       "   (3, 0.028720642),\n",
       "   (4, 0.028607294)],\n",
       "  32),\n",
       " ([(0, 0.32684425),\n",
       "   (1, 0.59801227),\n",
       "   (2, 0.025006456),\n",
       "   (3, 0.025048913),\n",
       "   (4, 0.025088085)],\n",
       "  33),\n",
       " ([(0, 0.02858989),\n",
       "   (1, 0.8856169),\n",
       "   (2, 0.02857286),\n",
       "   (3, 0.028611073),\n",
       "   (4, 0.028609278)],\n",
       "  34),\n",
       " ([(0, 0.018247928),\n",
       "   (1, 0.92714536),\n",
       "   (2, 0.018183867),\n",
       "   (3, 0.018212076),\n",
       "   (4, 0.018210769)],\n",
       "  35),\n",
       " ([(0, 0.88288224),\n",
       "   (1, 0.029892422),\n",
       "   (2, 0.028641218),\n",
       "   (3, 0.029737717),\n",
       "   (4, 0.028846394)],\n",
       "  36)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "137cda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dom_Topic  Topic_Contri                                           Keywords\n",
      "0         4.0        0.9842  word, specialization, course, use, language, r...\n",
      "1         0.0        0.9380  instructor, use, nlp, specialization, dictiona...\n",
      "2         0.0        0.8853  instructor, use, nlp, specialization, dictiona...\n",
      "3         4.0        0.9195  word, specialization, course, use, language, r...\n",
      "4         2.0        0.9385  feature, free, check, count, look, extract, cl...\n",
      "5         3.0        0.9702  tweet, sentiment, positive, word, analysis, co...\n",
      "6         4.0        0.9109  word, specialization, course, use, language, r...\n",
      "7         2.0        0.7333  feature, free, check, count, look, extract, cl...\n",
      "8         3.0        0.9524  tweet, sentiment, positive, word, analysis, co...\n",
      "9         0.0        0.9379  instructor, use, nlp, specialization, dictiona...\n",
      "10        1.0        0.8664  class, word, frequency, negative, positive, nu...\n",
      "11        1.0        0.8856  class, word, frequency, negative, positive, nu...\n",
      "12        2.0        0.9101  feature, free, check, count, look, extract, cl...\n",
      "13        3.0        0.7320  tweet, sentiment, positive, word, analysis, co...\n",
      "14        1.0        0.4016  class, word, frequency, negative, positive, nu...\n",
      "15        3.0        0.7331  tweet, sentiment, positive, word, analysis, co...\n",
      "16        1.0        0.8397  class, word, frequency, negative, positive, nu...\n",
      "17        1.0        0.5816  class, word, frequency, negative, positive, nu...\n",
      "18        3.0        0.6997  tweet, sentiment, positive, word, analysis, co...\n",
      "19        1.0        0.4674  class, word, frequency, negative, positive, nu...\n",
      "20        1.0        0.6665  class, word, frequency, negative, positive, nu...\n",
      "21        3.0        0.7316  tweet, sentiment, positive, word, analysis, co...\n",
      "22        3.0        0.7297  tweet, sentiment, positive, word, analysis, co...\n",
      "23        1.0        0.4891  class, word, frequency, negative, positive, nu...\n",
      "24        3.0        0.9105  tweet, sentiment, positive, word, analysis, co...\n",
      "25        1.0        0.7324  class, word, frequency, negative, positive, nu...\n",
      "26        4.0        0.7958  word, specialization, course, use, language, r...\n",
      "27        2.0        0.8400  feature, free, check, count, look, extract, cl...\n",
      "28        1.0        0.8399  class, word, frequency, negative, positive, nu...\n",
      "29        3.0        0.8658  tweet, sentiment, positive, word, analysis, co...\n",
      "30        1.0        0.7332  class, word, frequency, negative, positive, nu...\n",
      "31        1.0        0.4666  class, word, frequency, negative, positive, nu...\n",
      "32        1.0        0.8855  class, word, frequency, negative, positive, nu...\n",
      "33        1.0        0.5980  class, word, frequency, negative, positive, nu...\n",
      "34        1.0        0.8856  class, word, frequency, negative, positive, nu...\n",
      "35        1.0        0.9271  class, word, frequency, negative, positive, nu...\n",
      "36        0.0        0.8829  instructor, use, nlp, specialization, dictiona...\n"
     ]
    }
   ],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(ldana[corpusna]):\n",
    "        row = row_list[0] if ldana.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldana.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "sent_topics_df.columns = ['Dom_Topic', 'Topic_Contri', 'Keywords']\n",
    "print(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd582911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .',\n",
       " 1: 'By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !This Specialization designed taught two expert NLP , machine learning , deep learning .',\n",
       " 2: 'Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .',\n",
       " 3: 'Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .',\n",
       " 4: 'Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .',\n",
       " 5: 'The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .',\n",
       " 6: 'Other , I informative fun .',\n",
       " 7: 'From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !',\n",
       " 8: \"Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\",\n",
       " 9: \"Specifically , given word , want keep track number time , 's show positive class .Given another word want keep track number time word showed negative class .\",\n",
       " 10: 'Using count , extract feature use feature logistic regression classifier .',\n",
       " 11: \"So let 's take look .\",\n",
       " 12: 'It helpful first imagine two class would look .',\n",
       " 13: 'Here instance , could corpus consisting four tweet .',\n",
       " 14: 'Associated corpus , would set unique word , vocabulary .In example , vocabulary would eight unique word .',\n",
       " 15: 'For particular example sentiment analysis , two class .',\n",
       " 16: \"One class associated positive sentiment negative sentiment .So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\",\n",
       " 17: \"Let 's take set positive tweet .Now , take look vocabulary .\",\n",
       " 18: 'To get positive frequency word vocabulary , count time appears positive tweet .',\n",
       " 19: 'For instance , word happy appears one time first positive tweet , another time second positive tweet .',\n",
       " 20: \"So 's positive frequency two .\",\n",
       " 21: 'The complete table look like .',\n",
       " 22: 'Feel free take pause check entry .',\n",
       " 23: 'The logic applies getting negative frequency .',\n",
       " 24: 'However , sake clarity , look example , word appears two time first tweet another time second one .',\n",
       " 25: \"So 's negative frequency three .Take look entire table negative frequency feel free check value .So entire table positive negative frequency corpus .In practice coding , table dictionary mapping word class frequency .So map word corresponding class frequency number time 's showed class .You know create frequency dictionary , map word class number time word showed corresponding class .\",\n",
       " 26: \"In next video , 're going use frequency dictionary represent tweet .\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "sentences = \"\"\n",
    "corpus = pd.read_pickle(\"corpus.pkl\")\n",
    "corpus\n",
    "len(sent_topics_df)\n",
    "i=0\n",
    "a=0\n",
    "while(a<len(sent_topics_df)-1):\n",
    "    sentences = corpus.loc[a].at['transcript']\n",
    "    if(sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"]):\n",
    "        while((a<len(sent_topics_df)-1) and (sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"])):\n",
    "            sentences += corpus.loc[a+1].at['transcript']\n",
    "            a+=1\n",
    "    data[i] = sentences\n",
    "    i+=1\n",
    "    a+=1\n",
    "data[i] = sentences = corpus.loc[a].at['transcript']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dd1204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: sentence_id, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eb6005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .'],\n",
       " 1: ['By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !This Specialization designed taught two expert NLP , machine learning , deep learning .'],\n",
       " 2: ['Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .'],\n",
       " 3: ['Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .'],\n",
       " 4: ['Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .'],\n",
       " 5: ['The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .'],\n",
       " 6: ['Other , I informative fun .'],\n",
       " 7: ['From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !'],\n",
       " 8: [\"Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\"],\n",
       " 9: [\"Specifically , given word , want keep track number time , 's show positive class .Given another word want keep track number time word showed negative class .\"],\n",
       " 10: ['Using count , extract feature use feature logistic regression classifier .'],\n",
       " 11: [\"So let 's take look .\"],\n",
       " 12: ['It helpful first imagine two class would look .'],\n",
       " 13: ['Here instance , could corpus consisting four tweet .'],\n",
       " 14: ['Associated corpus , would set unique word , vocabulary .In example , vocabulary would eight unique word .'],\n",
       " 15: ['For particular example sentiment analysis , two class .'],\n",
       " 16: [\"One class associated positive sentiment negative sentiment .So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\"],\n",
       " 17: [\"Let 's take set positive tweet .Now , take look vocabulary .\"],\n",
       " 18: ['To get positive frequency word vocabulary , count time appears positive tweet .'],\n",
       " 19: ['For instance , word happy appears one time first positive tweet , another time second positive tweet .'],\n",
       " 20: [\"So 's positive frequency two .\"],\n",
       " 21: ['The complete table look like .'],\n",
       " 22: ['Feel free take pause check entry .'],\n",
       " 23: ['The logic applies getting negative frequency .'],\n",
       " 24: ['However , sake clarity , look example , word appears two time first tweet another time second one .'],\n",
       " 25: [\"So 's negative frequency three .Take look entire table negative frequency feel free check value .So entire table positive negative frequency corpus .In practice coding , table dictionary mapping word class frequency .So map word corresponding class frequency number time 's showed class .You know create frequency dictionary , map word class number time word showed corresponding class .\"],\n",
       " 26: [\"In next video , 're going use frequency dictionary represent tweet .\"]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9dd9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Other , I informative fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Specifically , given word , want keep track number time , 's show positive class .Given another word want keep track number time word showed negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Using count , extract feature use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>So let 's take look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>It helpful first imagine two class would look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Here instance , could corpus consisting four tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Associated corpus , would set unique word , vocabulary .In example , vocabulary would eight unique word .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>For particular example sentiment analysis , two class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>One class associated positive sentiment negative sentiment .So taking corpus , 'd set two tweet belong positive class , set two tweet belong negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Let 's take set positive tweet .Now , take look vocabulary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To get positive frequency word vocabulary , count time appears positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For instance , word happy appears one time first positive tweet , another time second positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So 's positive frequency two .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The complete table look like .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Feel free take pause check entry .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The logic applies getting negative frequency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>However , sake clarity , look example , word appears two time first tweet another time second one .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>So 's negative frequency three .Take look entire table negative frequency feel free check value .So entire table positive negative frequency corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In next video , 're going use frequency dictionary represent tweet .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               transcript\n",
       "0   4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...\n",
       "1   By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...\n",
       "2                                               Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .\n",
       "3                            Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .\n",
       "4   Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...\n",
       "5                                       The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .\n",
       "6                                                                                                                             Other , I informative fun .\n",
       "7   From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...\n",
       "8                           Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\n",
       "9   Specifically , given word , want keep track number time , 's show positive class .Given another word want keep track number time word showed negat...\n",
       "10                                                                             Using count , extract feature use feature logistic regression classifier .\n",
       "11                                                                                                                                  So let 's take look .\n",
       "12                                                                                                        It helpful first imagine two class would look .\n",
       "13                                                                                                   Here instance , could corpus consisting four tweet .\n",
       "14                                              Associated corpus , would set unique word , vocabulary .In example , vocabulary would eight unique word .\n",
       "15                                                                                                For particular example sentiment analysis , two class .\n",
       "16  One class associated positive sentiment negative sentiment .So taking corpus , 'd set two tweet belong positive class , set two tweet belong negat...\n",
       "17                                                                                           Let 's take set positive tweet .Now , take look vocabulary .\n",
       "18                                                                        To get positive frequency word vocabulary , count time appears positive tweet .\n",
       "19                                                 For instance , word happy appears one time first positive tweet , another time second positive tweet .\n",
       "20                                                                                                                         So 's positive frequency two .\n",
       "21                                                                                                                         The complete table look like .\n",
       "22                                                                                                                     Feel free take pause check entry .\n",
       "23                                                                                                         The logic applies getting negative frequency .\n",
       "24                                                    However , sake clarity , look example , word appears two time first tweet another time second one .\n",
       "25  So 's negative frequency three .Take look entire table negative frequency feel free check value .So entire table positive negative frequency corpu...\n",
       "26                                                                                   In next video , 're going use frequency dictionary represent tweet ."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "combined_sent = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "combined_sent.columns = ['transcript']\n",
    "combined_sent = combined_sent.sort_index()\n",
    "combined_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bcaa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it for later use\n",
    "combined_sent.to_pickle(\"Combined_wrt_topics.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
