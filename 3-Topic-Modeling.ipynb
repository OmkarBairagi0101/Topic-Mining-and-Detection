{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7084085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>attented</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>...</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>waiting</th>\n",
       "      <th>wasnnto</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>write</th>\n",
       "      <th>younes</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  algorithm  analysis  application  approximate  assignment  attented  \\\n",
       "0   0          1         1            0            1           0         0   \n",
       "1   0          0         1            1            0           0         0   \n",
       "2   0          0         0            0            0           0         0   \n",
       "3   1          0         0            0            0           0         0   \n",
       "4   0          0         0            0            0           0         0   \n",
       "5   0          0         1            0            0           1         1   \n",
       "6   0          0         0            0            0           0         0   \n",
       "7   0          0         0            0            0           0         0   \n",
       "8   0          0         1            0            0           0         0   \n",
       "9   0          0         0            0            0           0         0   \n",
       "\n",
       "   aug  awesome  bayes  ...  vector  video  visualize  waiting  wasnnto  week  \\\n",
       "0    0        0      1  ...       2      1          1        0        0     0   \n",
       "1    0        0      0  ...       0      0          0        0        0     0   \n",
       "2    0        0      0  ...       0      0          0        0        0     0   \n",
       "3    0        0      0  ...       0      0          0        0        0     0   \n",
       "4    0        0      0  ...       0      0          0        0        0     0   \n",
       "5    2        1      0  ...       1      0          0        1        1     1   \n",
       "6    0        0      0  ...       0      0          0        0        0     0   \n",
       "7    0        0      0  ...       0      0          0        0        0     0   \n",
       "8    0        0      0  ...       1      0          0        0        0     0   \n",
       "9    0        0      0  ...       0      0          0        0        0     0   \n",
       "\n",
       "   word  write  younes  youtube  \n",
       "0     3      1       0        0  \n",
       "1     0      0       0        0  \n",
       "2     0      0       0        0  \n",
       "3     0      0       1        0  \n",
       "4     0      0       0        0  \n",
       "5     1      0       0        0  \n",
       "6     0      0       0        1  \n",
       "7     0      0       0        0  \n",
       "8     0      0       0        0  \n",
       "9     0      0       0        0  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ef8f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d76bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approximate</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2  3  4  5  6  7  8  9\n",
       "ai           0  0  0  1  0  0  0  0  0  0\n",
       "algorithm    1  0  0  0  0  0  0  0  0  0\n",
       "analysis     1  1  0  0  0  1  0  0  1  0\n",
       "application  0  1  0  0  0  0  0  0  0  0\n",
       "approximate  1  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "036311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6aeee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22350489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:44,887 : INFO : using symmetric alpha at 0.5\n",
      "2022-02-24 17:27:44,890 : INFO : using symmetric eta at 0.5\n",
      "2022-02-24 17:27:44,891 : INFO : using serial LDA version on this node\n",
      "2022-02-24 17:27:44,893 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 17:27:44,911 : INFO : -5.475 per-word bound, 44.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:44,912 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 17:27:44,923 : INFO : topic #0 (0.500): 0.030*\"specialization\" + 0.026*\"course\" + 0.025*\"word\" + 0.022*\"instructor\" + 0.022*\"vector\" + 0.022*\"learning\" + 0.020*\"using\" + 0.019*\"logistic\" + 0.018*\"use\" + 0.018*\"language\"\n",
      "2022-02-24 17:27:44,924 : INFO : topic #1 (0.500): 0.030*\"course\" + 0.027*\"analysis\" + 0.019*\"aug\" + 0.018*\"specialization\" + 0.018*\"vector\" + 0.017*\"text\" + 0.015*\"word\" + 0.014*\"language\" + 0.014*\"designed\" + 0.013*\"machine\"\n",
      "2022-02-24 17:27:44,925 : INFO : topic diff=0.583701, rho=1.000000\n",
      "2022-02-24 17:27:44,938 : INFO : -5.093 per-word bound, 34.1 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:44,940 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 17:27:44,948 : INFO : topic #0 (0.500): 0.031*\"specialization\" + 0.025*\"course\" + 0.025*\"word\" + 0.024*\"instructor\" + 0.024*\"learning\" + 0.023*\"vector\" + 0.022*\"using\" + 0.022*\"logistic\" + 0.018*\"language\" + 0.018*\"use\"\n",
      "2022-02-24 17:27:44,949 : INFO : topic #1 (0.500): 0.031*\"course\" + 0.027*\"analysis\" + 0.021*\"aug\" + 0.018*\"text\" + 0.017*\"vector\" + 0.016*\"specialization\" + 0.014*\"word\" + 0.014*\"designed\" + 0.014*\"language\" + 0.014*\"machine\"\n",
      "2022-02-24 17:27:44,951 : INFO : topic diff=0.188236, rho=0.577350\n",
      "2022-02-24 17:27:44,960 : INFO : -5.039 per-word bound, 32.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:44,961 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 17:27:44,967 : INFO : topic #0 (0.500): 0.031*\"specialization\" + 0.025*\"course\" + 0.025*\"word\" + 0.024*\"instructor\" + 0.024*\"learning\" + 0.024*\"vector\" + 0.023*\"using\" + 0.023*\"logistic\" + 0.018*\"language\" + 0.018*\"use\"\n",
      "2022-02-24 17:27:44,969 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.025*\"analysis\" + 0.022*\"aug\" + 0.016*\"text\" + 0.015*\"vector\" + 0.015*\"specialization\" + 0.014*\"word\" + 0.014*\"designed\" + 0.014*\"machine\" + 0.014*\"language\"\n",
      "2022-02-24 17:27:44,970 : INFO : topic diff=0.100102, rho=0.500000\n",
      "2022-02-24 17:27:44,985 : INFO : -5.020 per-word bound, 32.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:44,987 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 17:27:44,995 : INFO : topic #0 (0.500): 0.031*\"specialization\" + 0.025*\"course\" + 0.025*\"word\" + 0.024*\"instructor\" + 0.024*\"learning\" + 0.024*\"vector\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.018*\"use\"\n",
      "2022-02-24 17:27:44,996 : INFO : topic #1 (0.500): 0.032*\"course\" + 0.025*\"analysis\" + 0.023*\"aug\" + 0.015*\"text\" + 0.015*\"vector\" + 0.014*\"specialization\" + 0.014*\"designed\" + 0.014*\"machine\" + 0.014*\"word\" + 0.014*\"embeddings\"\n",
      "2022-02-24 17:27:44,998 : INFO : topic diff=0.052283, rho=0.447214\n",
      "2022-02-24 17:27:45,006 : INFO : -5.015 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,007 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 17:27:45,016 : INFO : topic #0 (0.500): 0.031*\"specialization\" + 0.025*\"course\" + 0.025*\"word\" + 0.024*\"instructor\" + 0.024*\"learning\" + 0.024*\"vector\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.018*\"use\"\n",
      "2022-02-24 17:27:45,018 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.015*\"text\" + 0.015*\"vector\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"specialization\" + 0.014*\"word\" + 0.014*\"embeddings\"\n",
      "2022-02-24 17:27:45,019 : INFO : topic diff=0.028934, rho=0.408248\n",
      "2022-02-24 17:27:45,026 : INFO : -5.013 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,027 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 17:27:45,035 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.025*\"word\" + 0.024*\"course\" + 0.024*\"instructor\" + 0.024*\"vector\" + 0.024*\"learning\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,036 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.015*\"text\" + 0.014*\"vector\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"specialization\" + 0.014*\"word\" + 0.014*\"embeddings\"\n",
      "2022-02-24 17:27:45,037 : INFO : topic diff=0.016810, rho=0.377964\n",
      "2022-02-24 17:27:45,045 : INFO : -5.012 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,046 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 17:27:45,054 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,055 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"text\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"\n",
      "2022-02-24 17:27:45,056 : INFO : topic diff=0.010171, rho=0.353553\n",
      "2022-02-24 17:27:45,065 : INFO : -5.012 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,066 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 17:27:45,070 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,071 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"text\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"\n",
      "2022-02-24 17:27:45,072 : INFO : topic diff=0.006370, rho=0.333333\n",
      "2022-02-24 17:27:45,080 : INFO : -5.012 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,081 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 17:27:45,086 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"using\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,088 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"machine\" + 0.014*\"text\" + 0.014*\"designed\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"\n",
      "2022-02-24 17:27:45,088 : INFO : topic diff=0.004111, rho=0.316228\n",
      "2022-02-24 17:27:45,096 : INFO : -5.012 per-word bound, 32.3 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,097 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 17:27:45,103 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"using\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,104 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"text\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"\n",
      "2022-02-24 17:27:45,105 : INFO : topic diff=0.002722, rho=0.301511\n",
      "2022-02-24 17:27:45,107 : INFO : topic #0 (0.500): 0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"using\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"\n",
      "2022-02-24 17:27:45,108 : INFO : topic #1 (0.500): 0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"text\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"specialization\" + 0.024*\"word\" + 0.024*\"course\" + 0.024*\"vector\" + 0.024*\"using\" + 0.024*\"learning\" + 0.024*\"instructor\" + 0.024*\"logistic\" + 0.018*\"language\" + 0.017*\"use\"'),\n",
       " (1,\n",
       "  '0.033*\"course\" + 0.024*\"analysis\" + 0.023*\"aug\" + 0.014*\"machine\" + 0.014*\"designed\" + 0.014*\"text\" + 0.014*\"vector\" + 0.014*\"embeddings\" + 0.014*\"localitysensitive\" + 0.014*\"translation\"')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "59fe173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:45,750 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 17:27:45,751 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 17:27:45,752 : INFO : using serial LDA version on this node\n",
      "2022-02-24 17:27:45,753 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 17:27:45,768 : INFO : -6.847 per-word bound, 115.2 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,769 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 17:27:45,781 : INFO : topic #0 (0.250): 0.053*\"instructor\" + 0.036*\"specialization\" + 0.025*\"helped\" + 0.022*\"build\" + 0.021*\"text\" + 0.021*\"analysis\" + 0.020*\"learning\" + 0.020*\"deep\" + 0.020*\"bensouda\" + 0.020*\"university\"\n",
      "2022-02-24 17:27:45,783 : INFO : topic #1 (0.250): 0.042*\"course\" + 0.039*\"vector\" + 0.036*\"logistic\" + 0.035*\"word\" + 0.034*\"using\" + 0.029*\"analysis\" + 0.024*\"natural\" + 0.024*\"tweet\" + 0.024*\"use\" + 0.022*\"language\"\n",
      "2022-02-24 17:27:45,784 : INFO : topic #2 (0.250): 0.038*\"course\" + 0.032*\"learning\" + 0.031*\"machine\" + 0.029*\"aug\" + 0.018*\"specialization\" + 0.018*\"deep\" + 0.018*\"designed\" + 0.018*\"taught\" + 0.018*\"expert\" + 0.018*\"trax\"\n",
      "2022-02-24 17:27:45,785 : INFO : topic #3 (0.250): 0.031*\"model\" + 0.023*\"word\" + 0.023*\"specialization\" + 0.022*\"course\" + 0.022*\"straight\" + 0.022*\"lecture\" + 0.021*\"detailed\" + 0.021*\"youtube\" + 0.020*\"little\" + 0.020*\"forward\"\n",
      "2022-02-24 17:27:45,786 : INFO : topic diff=1.996233, rho=1.000000\n",
      "2022-02-24 17:27:45,792 : INFO : -5.267 per-word bound, 38.5 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,793 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 17:27:45,798 : INFO : topic #0 (0.250): 0.055*\"instructor\" + 0.038*\"specialization\" + 0.023*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.021*\"analysis\" + 0.021*\"deep\" + 0.021*\"learning\" + 0.021*\"bensouda\" + 0.021*\"university\"\n",
      "2022-02-24 17:27:45,799 : INFO : topic #1 (0.250): 0.040*\"course\" + 0.039*\"vector\" + 0.038*\"logistic\" + 0.037*\"word\" + 0.037*\"using\" + 0.028*\"analysis\" + 0.026*\"natural\" + 0.026*\"tweet\" + 0.026*\"use\" + 0.025*\"language\"\n",
      "2022-02-24 17:27:45,800 : INFO : topic #2 (0.250): 0.042*\"course\" + 0.032*\"learning\" + 0.031*\"machine\" + 0.030*\"aug\" + 0.018*\"deep\" + 0.018*\"specialization\" + 0.018*\"taught\" + 0.018*\"designed\" + 0.018*\"expert\" + 0.017*\"trax\"\n",
      "2022-02-24 17:27:45,801 : INFO : topic #3 (0.250): 0.033*\"model\" + 0.029*\"straight\" + 0.029*\"lecture\" + 0.028*\"detailed\" + 0.028*\"youtube\" + 0.028*\"little\" + 0.028*\"forward\" + 0.028*\"helped\" + 0.027*\"hard\" + 0.027*\"exciting\"\n",
      "2022-02-24 17:27:45,802 : INFO : topic diff=0.240570, rho=0.577350\n",
      "2022-02-24 17:27:45,810 : INFO : -5.165 per-word bound, 35.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,811 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 17:27:45,816 : INFO : topic #0 (0.250): 0.055*\"instructor\" + 0.038*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.021*\"deep\" + 0.021*\"analysis\" + 0.021*\"learning\" + 0.021*\"bensouda\" + 0.021*\"university\"\n",
      "2022-02-24 17:27:45,817 : INFO : topic #1 (0.250): 0.040*\"course\" + 0.039*\"vector\" + 0.038*\"logistic\" + 0.038*\"word\" + 0.038*\"using\" + 0.027*\"analysis\" + 0.026*\"natural\" + 0.026*\"tweet\" + 0.026*\"use\" + 0.026*\"language\"\n",
      "2022-02-24 17:27:45,818 : INFO : topic #2 (0.250): 0.043*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"deep\" + 0.017*\"designed\" + 0.017*\"specialization\" + 0.017*\"trax\"\n",
      "2022-02-24 17:27:45,819 : INFO : topic #3 (0.250): 0.034*\"model\" + 0.032*\"straight\" + 0.032*\"lecture\" + 0.031*\"detailed\" + 0.031*\"youtube\" + 0.031*\"little\" + 0.031*\"forward\" + 0.031*\"helped\" + 0.031*\"hard\" + 0.031*\"exciting\"\n",
      "2022-02-24 17:27:45,819 : INFO : topic diff=0.122711, rho=0.500000\n",
      "2022-02-24 17:27:45,826 : INFO : -5.138 per-word bound, 35.2 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,827 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 17:27:45,831 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"deep\" + 0.022*\"learning\" + 0.021*\"bensouda\" + 0.021*\"analysis\" + 0.021*\"university\"\n",
      "2022-02-24 17:27:45,833 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"logistic\" + 0.038*\"word\" + 0.038*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"tweet\" + 0.027*\"use\" + 0.026*\"language\"\n",
      "2022-02-24 17:27:45,834 : INFO : topic #2 (0.250): 0.044*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"deep\" + 0.017*\"designed\" + 0.017*\"trax\" + 0.017*\"tensorflow\"\n",
      "2022-02-24 17:27:45,835 : INFO : topic #3 (0.250): 0.034*\"model\" + 0.033*\"straight\" + 0.033*\"lecture\" + 0.033*\"detailed\" + 0.033*\"youtube\" + 0.033*\"little\" + 0.033*\"forward\" + 0.033*\"helped\" + 0.033*\"hard\" + 0.033*\"exciting\"\n",
      "2022-02-24 17:27:45,836 : INFO : topic diff=0.067153, rho=0.447214\n",
      "2022-02-24 17:27:45,843 : INFO : -5.130 per-word bound, 35.0 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,843 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 17:27:45,847 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"deep\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\" + 0.022*\"mourri\"\n",
      "2022-02-24 17:27:45,849 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"logistic\" + 0.039*\"word\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"tweet\" + 0.027*\"use\" + 0.027*\"language\"\n",
      "2022-02-24 17:27:45,850 : INFO : topic #2 (0.250): 0.044*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"deep\" + 0.017*\"trax\" + 0.017*\"designed\" + 0.017*\"tensorflow\"\n",
      "2022-02-24 17:27:45,852 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.034*\"straight\" + 0.034*\"lecture\" + 0.034*\"detailed\" + 0.034*\"youtube\" + 0.034*\"little\" + 0.034*\"forward\" + 0.034*\"helped\" + 0.034*\"hard\" + 0.033*\"exciting\"\n",
      "2022-02-24 17:27:45,853 : INFO : topic diff=0.038194, rho=0.408248\n",
      "2022-02-24 17:27:45,862 : INFO : -5.127 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,863 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 17:27:45,868 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"deep\" + 0.022*\"learning\" + 0.022*\"designed\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,869 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"logistic\" + 0.039*\"word\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"tweet\" + 0.027*\"use\" + 0.027*\"language\"\n",
      "2022-02-24 17:27:45,871 : INFO : topic #2 (0.250): 0.044*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"deep\" + 0.017*\"paper\"\n",
      "2022-02-24 17:27:45,872 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.034*\"straight\" + 0.034*\"lecture\" + 0.034*\"detailed\" + 0.034*\"youtube\" + 0.034*\"helped\" + 0.034*\"little\" + 0.034*\"forward\" + 0.034*\"hard\" + 0.034*\"exciting\"\n",
      "2022-02-24 17:27:45,873 : INFO : topic diff=0.022479, rho=0.377964\n",
      "2022-02-24 17:27:45,880 : INFO : -5.126 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,882 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 17:27:45,886 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"deep\" + 0.022*\"designed\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,887 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"tweet\" + 0.027*\"use\" + 0.027*\"language\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:45,888 : INFO : topic #2 (0.250): 0.044*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"\n",
      "2022-02-24 17:27:45,890 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.034*\"straight\" + 0.034*\"lecture\" + 0.034*\"detailed\" + 0.034*\"helped\" + 0.034*\"youtube\" + 0.034*\"little\" + 0.034*\"forward\" + 0.034*\"hard\" + 0.034*\"exciting\"\n",
      "2022-02-24 17:27:45,892 : INFO : topic diff=0.013665, rho=0.353553\n",
      "2022-02-24 17:27:45,899 : INFO : -5.125 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,900 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 17:27:45,904 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"deep\" + 0.022*\"text\" + 0.022*\"designed\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,905 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"tweet\" + 0.027*\"use\" + 0.027*\"language\"\n",
      "2022-02-24 17:27:45,906 : INFO : topic #2 (0.250): 0.045*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"\n",
      "2022-02-24 17:27:45,906 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.035*\"straight\" + 0.035*\"lecture\" + 0.035*\"detailed\" + 0.035*\"helped\" + 0.035*\"youtube\" + 0.035*\"little\" + 0.035*\"forward\" + 0.035*\"hard\" + 0.035*\"exciting\"\n",
      "2022-02-24 17:27:45,907 : INFO : topic diff=0.008560, rho=0.333333\n",
      "2022-02-24 17:27:45,917 : INFO : -5.125 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,918 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 17:27:45,922 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"helped\" + 0.022*\"deep\" + 0.022*\"build\" + 0.022*\"designed\" + 0.022*\"text\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,923 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"natural\" + 0.027*\"specialization\" + 0.027*\"tweet\" + 0.027*\"use\"\n",
      "2022-02-24 17:27:45,924 : INFO : topic #2 (0.250): 0.045*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"\n",
      "2022-02-24 17:27:45,926 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.035*\"straight\" + 0.035*\"lecture\" + 0.035*\"helped\" + 0.035*\"detailed\" + 0.035*\"youtube\" + 0.035*\"little\" + 0.035*\"forward\" + 0.035*\"hard\" + 0.035*\"exciting\"\n",
      "2022-02-24 17:27:45,926 : INFO : topic diff=0.005510, rho=0.316228\n",
      "2022-02-24 17:27:45,935 : INFO : -5.125 per-word bound, 34.9 perplexity estimate based on a held-out corpus of 10 documents with 147 words\n",
      "2022-02-24 17:27:45,936 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 17:27:45,942 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"deep\" + 0.022*\"designed\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,942 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"specialization\" + 0.027*\"natural\" + 0.027*\"language\" + 0.027*\"tweet\"\n",
      "2022-02-24 17:27:45,944 : INFO : topic #2 (0.250): 0.045*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"\n",
      "2022-02-24 17:27:45,945 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.035*\"helped\" + 0.035*\"straight\" + 0.035*\"lecture\" + 0.035*\"detailed\" + 0.035*\"youtube\" + 0.035*\"little\" + 0.035*\"forward\" + 0.035*\"hard\" + 0.035*\"exciting\"\n",
      "2022-02-24 17:27:45,948 : INFO : topic diff=0.003634, rho=0.301511\n",
      "2022-02-24 17:27:45,949 : INFO : topic #0 (0.250): 0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"deep\" + 0.022*\"designed\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"\n",
      "2022-02-24 17:27:45,950 : INFO : topic #1 (0.250): 0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"specialization\" + 0.027*\"natural\" + 0.027*\"language\" + 0.027*\"tweet\"\n",
      "2022-02-24 17:27:45,951 : INFO : topic #2 (0.250): 0.045*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"\n",
      "2022-02-24 17:27:45,952 : INFO : topic #3 (0.250): 0.035*\"model\" + 0.035*\"helped\" + 0.035*\"straight\" + 0.035*\"lecture\" + 0.035*\"detailed\" + 0.035*\"youtube\" + 0.035*\"little\" + 0.035*\"forward\" + 0.035*\"hard\" + 0.035*\"exciting\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.056*\"instructor\" + 0.039*\"specialization\" + 0.022*\"deep\" + 0.022*\"designed\" + 0.022*\"helped\" + 0.022*\"build\" + 0.022*\"text\" + 0.022*\"learning\" + 0.022*\"bensouda\" + 0.022*\"university\"'),\n",
       " (1,\n",
       "  '0.039*\"course\" + 0.039*\"vector\" + 0.039*\"word\" + 0.039*\"logistic\" + 0.039*\"using\" + 0.027*\"analysis\" + 0.027*\"specialization\" + 0.027*\"natural\" + 0.027*\"language\" + 0.027*\"tweet\"'),\n",
       " (2,\n",
       "  '0.045*\"course\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"aug\" + 0.017*\"taught\" + 0.017*\"expert\" + 0.017*\"trax\" + 0.017*\"tensorflow\" + 0.017*\"paper\" + 0.017*\"kaiser\"'),\n",
       " (3,\n",
       "  '0.035*\"model\" + 0.035*\"helped\" + 0.035*\"straight\" + 0.035*\"lecture\" + 0.035*\"detailed\" + 0.035*\"youtube\" + 0.035*\"little\" + 0.035*\"forward\" + 0.035*\"hard\" + 0.035*\"exciting\"')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4b6dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "77935955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  locality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0     rating  â â  students enrolled course   nat...\n",
       "1  by end specialization  designed nlp applicatio...\n",
       "2  this specialization designed taught two expert...\n",
       "3  younes bensouda mourri instructor ai stanford ...\n",
       "4  åukasz kaiser staff research scientist google...\n",
       "5  machine translation  word embeddings  locality...\n",
       "6  the lecture exciting detailed  though little h...\n",
       "7                          other  i informative fun \n",
       "8  from lesson sentiment analysis logistic regres...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02b8af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â students course language processing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>specialization application perform sentiment a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes instructor ai stanford university learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture straight regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentiment analysis regression learn feature ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â students course language processing s...\n",
       "1  specialization application perform sentiment a...\n",
       "2                    specialization machine learning\n",
       "3  younes instructor ai stanford university learn...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings sentiment ...\n",
       "6                  lecture straight regression model\n",
       "7                                                fun\n",
       "8  sentiment analysis regression learn feature ve...\n",
       "9         instructor instructor curriculum developer"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "009ca2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>brain</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>coauthor</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  assignment  aug  awesome  bayes  brain  chatbot  \\\n",
       "0   0         1            0           0    0        0      1      0        0   \n",
       "1   0         1            1           0    0        0      0      0        1   \n",
       "2   0         0            0           0    0        0      0      0        0   \n",
       "3   1         0            0           0    0        0      0      0        0   \n",
       "4   0         0            0           0    0        0      0      1        0   \n",
       "5   0         1            0           1    1        1      0      0        0   \n",
       "6   0         0            0           0    0        0      0      0        0   \n",
       "7   0         0            0           0    0        0      0      0        0   \n",
       "8   0         1            0           0    0        0      0      0        0   \n",
       "9   0         0            0           0    0        0      0      0        0   \n",
       "\n",
       "   coauthor  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0         0  ...      1      0           0    2       2      1          1   \n",
       "1         0  ...      0      0           0    0       0      0          0   \n",
       "2         0  ...      0      0           0    0       0      0          0   \n",
       "3         0  ...      0      0           1    0       0      0          0   \n",
       "4         1  ...      0      1           0    0       0      0          0   \n",
       "5         0  ...      0      0           0    0       1      0          0   \n",
       "6         0  ...      0      0           0    0       0      0          0   \n",
       "7         0  ...      0      0           0    0       0      0          0   \n",
       "8         0  ...      1      0           0    0       1      0          0   \n",
       "9         0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6e9ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "32882e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:50,780 : INFO : using symmetric alpha at 0.25\n",
      "2022-02-24 17:27:50,781 : INFO : using symmetric eta at 0.25\n",
      "2022-02-24 17:27:50,782 : INFO : using serial LDA version on this node\n",
      "2022-02-24 17:27:50,784 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 17:27:50,796 : INFO : -6.247 per-word bound, 75.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,798 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 17:27:50,806 : INFO : topic #0 (0.250): 0.043*\"specialization\" + 0.043*\"sentiment\" + 0.043*\"chatbot\" + 0.043*\"tool\" + 0.043*\"translate\" + 0.043*\"analysis\" + 0.043*\"language\" + 0.043*\"perform\" + 0.043*\"application\" + 0.043*\"text\"\n",
      "2022-02-24 17:27:50,807 : INFO : topic #1 (0.250): 0.050*\"regression\" + 0.037*\"ukasz\" + 0.037*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.035*\"brain\" + 0.035*\"paper\" + 0.034*\"transformer\" + 0.033*\"research\"\n",
      "2022-02-24 17:27:50,808 : INFO : topic #2 (0.250): 0.067*\"course\" + 0.045*\"vector\" + 0.043*\"sentiment\" + 0.039*\"analysis\" + 0.032*\"word\" + 0.031*\"fun\" + 0.030*\"space\" + 0.029*\"embeddings\" + 0.029*\"regression\" + 0.029*\"rating\"\n",
      "2022-02-24 17:27:50,809 : INFO : topic #3 (0.250): 0.054*\"specialization\" + 0.051*\"course\" + 0.042*\"word\" + 0.042*\"instructor\" + 0.030*\"space\" + 0.030*\"vector\" + 0.029*\"learning\" + 0.028*\"regression\" + 0.028*\"processing\" + 0.028*\"model\"\n",
      "2022-02-24 17:27:50,810 : INFO : topic diff=2.022783, rho=1.000000\n",
      "2022-02-24 17:27:50,816 : INFO : -4.854 per-word bound, 28.9 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,816 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 17:27:50,822 : INFO : topic #0 (0.250): 0.045*\"sentiment\" + 0.045*\"specialization\" + 0.045*\"analysis\" + 0.045*\"chatbot\" + 0.045*\"tool\" + 0.045*\"translate\" + 0.045*\"language\" + 0.045*\"perform\" + 0.045*\"application\" + 0.045*\"text\"\n",
      "2022-02-24 17:27:50,822 : INFO : topic #1 (0.250): 0.058*\"regression\" + 0.037*\"ukasz\" + 0.037*\"coauthor\" + 0.037*\"scientist\" + 0.037*\"trax\" + 0.037*\"google\" + 0.037*\"brain\" + 0.037*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,823 : INFO : topic #2 (0.250): 0.077*\"course\" + 0.042*\"vector\" + 0.042*\"sentiment\" + 0.039*\"analysis\" + 0.032*\"word\" + 0.032*\"fun\" + 0.031*\"space\" + 0.031*\"embeddings\" + 0.031*\"rating\" + 0.030*\"models\"\n",
      "2022-02-24 17:27:50,824 : INFO : topic #3 (0.250): 0.056*\"specialization\" + 0.047*\"course\" + 0.043*\"word\" + 0.043*\"instructor\" + 0.030*\"space\" + 0.030*\"vector\" + 0.030*\"learning\" + 0.029*\"processing\" + 0.029*\"regression\" + 0.029*\"model\"\n",
      "2022-02-24 17:27:50,825 : INFO : topic diff=0.168058, rho=0.577350\n",
      "2022-02-24 17:27:50,835 : INFO : -4.796 per-word bound, 27.8 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,836 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 17:27:50,841 : INFO : topic #0 (0.250): 0.046*\"sentiment\" + 0.046*\"specialization\" + 0.046*\"analysis\" + 0.046*\"chatbot\" + 0.046*\"tool\" + 0.046*\"translate\" + 0.046*\"perform\" + 0.046*\"language\" + 0.046*\"application\" + 0.046*\"text\"\n",
      "2022-02-24 17:27:50,842 : INFO : topic #1 (0.250): 0.062*\"regression\" + 0.037*\"ukasz\" + 0.037*\"coauthor\" + 0.037*\"scientist\" + 0.037*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,843 : INFO : topic #2 (0.250): 0.082*\"course\" + 0.038*\"vector\" + 0.038*\"sentiment\" + 0.036*\"analysis\" + 0.033*\"word\" + 0.033*\"fun\" + 0.032*\"space\" + 0.032*\"embeddings\" + 0.032*\"rating\" + 0.032*\"models\"\n",
      "2022-02-24 17:27:50,845 : INFO : topic #3 (0.250): 0.056*\"specialization\" + 0.045*\"course\" + 0.043*\"word\" + 0.043*\"instructor\" + 0.030*\"space\" + 0.030*\"vector\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"model\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,846 : INFO : topic diff=0.088266, rho=0.500000\n",
      "2022-02-24 17:27:50,852 : INFO : -4.778 per-word bound, 27.4 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,853 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 17:27:50,857 : INFO : topic #0 (0.250): 0.047*\"sentiment\" + 0.047*\"specialization\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"language\" + 0.047*\"text\"\n",
      "2022-02-24 17:27:50,858 : INFO : topic #1 (0.250): 0.064*\"regression\" + 0.037*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,859 : INFO : topic #2 (0.250): 0.084*\"course\" + 0.036*\"vector\" + 0.036*\"sentiment\" + 0.035*\"analysis\" + 0.033*\"word\" + 0.033*\"fun\" + 0.033*\"space\" + 0.033*\"embeddings\" + 0.033*\"rating\" + 0.033*\"models\"\n",
      "2022-02-24 17:27:50,859 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.045*\"course\" + 0.043*\"word\" + 0.043*\"instructor\" + 0.030*\"space\" + 0.030*\"vector\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"model\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,861 : INFO : topic diff=0.046314, rho=0.447214\n",
      "2022-02-24 17:27:50,870 : INFO : -4.772 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,872 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 17:27:50,876 : INFO : topic #0 (0.250): 0.047*\"sentiment\" + 0.047*\"specialization\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,878 : INFO : topic #1 (0.250): 0.064*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,881 : INFO : topic #2 (0.250): 0.085*\"course\" + 0.035*\"vector\" + 0.035*\"sentiment\" + 0.034*\"analysis\" + 0.033*\"word\" + 0.033*\"fun\" + 0.033*\"space\" + 0.033*\"embeddings\" + 0.033*\"machine\" + 0.033*\"rating\"\n",
      "2022-02-24 17:27:50,883 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"space\" + 0.030*\"vector\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"model\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,884 : INFO : topic diff=0.025850, rho=0.408248\n",
      "2022-02-24 17:27:50,889 : INFO : -4.770 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,891 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 17:27:50,897 : INFO : topic #0 (0.250): 0.047*\"sentiment\" + 0.047*\"specialization\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,898 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,900 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"vector\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.033*\"machine\" + 0.033*\"word\" + 0.033*\"fun\" + 0.033*\"space\" + 0.033*\"embeddings\" + 0.033*\"rating\"\n",
      "2022-02-24 17:27:50,902 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"relationship\" + 0.030*\"model\"\n",
      "2022-02-24 17:27:50,903 : INFO : topic diff=0.015080, rho=0.377964\n",
      "2022-02-24 17:27:50,909 : INFO : -4.770 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,910 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 17:27:50,917 : INFO : topic #0 (0.250): 0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,918 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:50,919 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"vector\" + 0.034*\"sentiment\" + 0.034*\"analysis\" + 0.033*\"machine\" + 0.033*\"word\" + 0.033*\"fun\" + 0.033*\"space\" + 0.033*\"embeddings\" + 0.033*\"rating\"\n",
      "2022-02-24 17:27:50,920 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"relationship\" + 0.030*\"model\"\n",
      "2022-02-24 17:27:50,921 : INFO : topic diff=0.009128, rho=0.353553\n",
      "2022-02-24 17:27:50,926 : INFO : -4.770 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,927 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 17:27:50,933 : INFO : topic #0 (0.250): 0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,934 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,935 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.034*\"analysis\" + 0.033*\"machine\" + 0.033*\"word\" + 0.033*\"embeddings\" + 0.033*\"space\" + 0.033*\"fun\" + 0.033*\"rating\"\n",
      "2022-02-24 17:27:50,936 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,937 : INFO : topic diff=0.005706, rho=0.333333\n",
      "2022-02-24 17:27:50,946 : INFO : -4.769 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,947 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 17:27:50,952 : INFO : topic #0 (0.250): 0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,953 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,953 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.034*\"analysis\" + 0.034*\"machine\" + 0.033*\"word\" + 0.033*\"embeddings\" + 0.033*\"space\" + 0.033*\"rating\" + 0.033*\"fun\"\n",
      "2022-02-24 17:27:50,954 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"learning\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,955 : INFO : topic diff=0.003670, rho=0.316228\n",
      "2022-02-24 17:27:50,964 : INFO : -4.769 per-word bound, 27.3 perplexity estimate based on a held-out corpus of 10 documents with 107 words\n",
      "2022-02-24 17:27:50,966 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 17:27:50,970 : INFO : topic #0 (0.250): 0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,971 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,972 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"machine\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.033*\"analysis\" + 0.033*\"embeddings\" + 0.033*\"word\" + 0.033*\"rating\" + 0.033*\"space\" + 0.033*\"fun\"\n",
      "2022-02-24 17:27:50,972 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"learning\" + 0.030*\"relationship\"\n",
      "2022-02-24 17:27:50,973 : INFO : topic diff=0.002419, rho=0.301511\n",
      "2022-02-24 17:27:50,974 : INFO : topic #0 (0.250): 0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"\n",
      "2022-02-24 17:27:50,976 : INFO : topic #1 (0.250): 0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"\n",
      "2022-02-24 17:27:50,978 : INFO : topic #2 (0.250): 0.086*\"course\" + 0.034*\"machine\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.033*\"analysis\" + 0.033*\"embeddings\" + 0.033*\"word\" + 0.033*\"rating\" + 0.033*\"space\" + 0.033*\"fun\"\n",
      "2022-02-24 17:27:50,980 : INFO : topic #3 (0.250): 0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"learning\" + 0.030*\"relationship\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.047*\"specialization\" + 0.047*\"sentiment\" + 0.047*\"analysis\" + 0.047*\"perform\" + 0.047*\"chatbot\" + 0.047*\"tool\" + 0.047*\"translate\" + 0.047*\"application\" + 0.047*\"text\" + 0.047*\"language\"'),\n",
       " (1,\n",
       "  '0.065*\"regression\" + 0.036*\"ukasz\" + 0.036*\"coauthor\" + 0.036*\"scientist\" + 0.036*\"trax\" + 0.036*\"google\" + 0.036*\"brain\" + 0.036*\"paper\" + 0.036*\"transformer\" + 0.036*\"research\"'),\n",
       " (2,\n",
       "  '0.086*\"course\" + 0.034*\"machine\" + 0.034*\"sentiment\" + 0.034*\"vector\" + 0.033*\"analysis\" + 0.033*\"embeddings\" + 0.033*\"word\" + 0.033*\"rating\" + 0.033*\"space\" + 0.033*\"fun\"'),\n",
       " (3,\n",
       "  '0.057*\"specialization\" + 0.044*\"course\" + 0.044*\"word\" + 0.044*\"instructor\" + 0.030*\"vector\" + 0.030*\"space\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"learning\" + 0.030*\"relationship\"')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d643340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "924e6236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating â â students course natural language pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end specialization nlp application perform sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specialization expert nlp machine deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes mourri instructor ai stanford universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz staff research scientist google brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation word embeddings localityse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lecture detailed little hard straight helped r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lesson sentiment analysis logistic regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  rating â â students course natural language pr...\n",
       "1  end specialization nlp application perform sen...\n",
       "2    specialization expert nlp machine deep learning\n",
       "3  younes mourri instructor ai stanford universit...\n",
       "4  åukasz staff research scientist google brain ...\n",
       "5  machine translation word embeddings localityse...\n",
       "6  lecture detailed little hard straight helped r...\n",
       "7                            other i informative fun\n",
       "8  lesson sentiment analysis logistic regression ...\n",
       "9  instructor instructor senior curriculum developer"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8d43240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>aug</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bayes</th>\n",
       "      <th>best</th>\n",
       "      <th>binary</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ukasz</th>\n",
       "      <th>university</th>\n",
       "      <th>use</th>\n",
       "      <th>vector</th>\n",
       "      <th>video</th>\n",
       "      <th>visualize</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>younes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  analysis  application  approximate  assignment  aug  awesome  bayes  \\\n",
       "0   0         1            0            1           0    0        0      1   \n",
       "1   0         1            1            0           0    0        0      0   \n",
       "2   0         0            0            0           0    0        0      0   \n",
       "3   1         0            0            0           0    0        0      0   \n",
       "4   0         0            0            0           0    0        0      0   \n",
       "5   0         1            0            0           1    1        1      0   \n",
       "6   0         0            0            0           0    0        0      0   \n",
       "7   0         0            0            0           0    0        0      0   \n",
       "8   0         1            0            0           0    0        0      0   \n",
       "9   0         0            0            0           0    0        0      0   \n",
       "\n",
       "   best  binary  ...  tweet  ukasz  university  use  vector  video  visualize  \\\n",
       "0     0       0  ...      1      0           0    2       2      1          1   \n",
       "1     0       0  ...      0      0           0    0       0      0          0   \n",
       "2     0       0  ...      0      0           0    0       0      0          0   \n",
       "3     0       0  ...      0      0           1    0       0      0          0   \n",
       "4     0       0  ...      0      1           0    0       0      0          0   \n",
       "5     1       0  ...      0      0           0    0       1      0          0   \n",
       "6     0       0  ...      0      0           0    0       0      0          0   \n",
       "7     0       0  ...      0      0           0    0       0      0          0   \n",
       "8     0       1  ...      1      0           0    0       1      0          0   \n",
       "9     0       0  ...      0      0           0    0       0      0          0   \n",
       "\n",
       "   week  word  younes  \n",
       "0     0     3       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       1  \n",
       "4     0     0       0  \n",
       "5     1     1       0  \n",
       "6     0     0       0  \n",
       "7     0     0       0  \n",
       "8     0     0       0  \n",
       "9     0     0       0  \n",
       "\n",
       "[10 rows x 98 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bd222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4daee25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,064 : INFO : using symmetric alpha at 0.2\n",
      "2022-02-24 17:27:56,066 : INFO : using symmetric eta at 0.2\n",
      "2022-02-24 17:27:56,067 : INFO : using serial LDA version on this node\n",
      "2022-02-24 17:27:56,069 : INFO : running online (multi-pass) LDA training, 5 topics, 80 passes over the supplied corpus of 10 documents, updating model once every 10 documents, evaluating perplexity every 10 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-24 17:27:56,086 : INFO : -7.534 per-word bound, 185.3 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,087 : INFO : PROGRESS: pass 0, at document #10/10\n",
      "2022-02-24 17:27:56,096 : INFO : topic #0 (0.200): 0.058*\"regression\" + 0.058*\"logistic\" + 0.032*\"analysis\" + 0.032*\"vector\" + 0.032*\"text\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,097 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.031*\"course\" + 0.030*\"word\" + 0.028*\"regression\" + 0.026*\"model\" + 0.022*\"vector\" + 0.022*\"relationship\" + 0.021*\"language\" + 0.020*\"processing\" + 0.020*\"natural\"\n",
      "2022-02-24 17:27:56,097 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.031*\"transformer\" + 0.031*\"paper\" + 0.031*\"ukasz\" + 0.031*\"google\" + 0.031*\"brain\" + 0.031*\"staff\" + 0.031*\"deep\" + 0.031*\"scientist\" + 0.031*\"research\"\n",
      "2022-02-24 17:27:56,099 : INFO : topic #3 (0.200): 0.059*\"course\" + 0.039*\"instructor\" + 0.029*\"word\" + 0.027*\"space\" + 0.026*\"vector\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.022*\"translation\" + 0.022*\"sentiment\" + 0.022*\"rating\"\n",
      "2022-02-24 17:27:56,100 : INFO : topic #4 (0.200): 0.038*\"language\" + 0.038*\"specialization\" + 0.034*\"perform\" + 0.033*\"sentiment\" + 0.033*\"analysis\" + 0.026*\"nlp\" + 0.026*\"text\" + 0.026*\"chatbot\" + 0.026*\"application\" + 0.026*\"tool\"\n",
      "2022-02-24 17:27:56,100 : INFO : topic diff=2.737671, rho=1.000000\n",
      "2022-02-24 17:27:56,108 : INFO : -5.363 per-word bound, 41.2 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,109 : INFO : PROGRESS: pass 1, at document #10/10\n",
      "2022-02-24 17:27:56,113 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.058*\"logistic\" + 0.032*\"analysis\" + 0.032*\"text\" + 0.032*\"sentiment\" + 0.032*\"vector\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,114 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.035*\"course\" + 0.034*\"word\" + 0.026*\"regression\" + 0.025*\"model\" + 0.024*\"vector\" + 0.024*\"relationship\" + 0.024*\"language\" + 0.024*\"processing\" + 0.024*\"natural\"\n",
      "2022-02-24 17:27:56,115 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.031*\"deep\" + 0.031*\"transformer\" + 0.031*\"paper\" + 0.031*\"ukasz\" + 0.031*\"google\" + 0.031*\"brain\" + 0.031*\"staff\" + 0.031*\"scientist\" + 0.031*\"research\"\n",
      "2022-02-24 17:27:56,116 : INFO : topic #3 (0.200): 0.061*\"course\" + 0.041*\"instructor\" + 0.026*\"word\" + 0.025*\"space\" + 0.025*\"vector\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"translation\" + 0.023*\"sentiment\" + 0.023*\"rating\"\n",
      "2022-02-24 17:27:56,116 : INFO : topic #4 (0.200): 0.038*\"language\" + 0.038*\"specialization\" + 0.036*\"perform\" + 0.036*\"sentiment\" + 0.035*\"analysis\" + 0.032*\"nlp\" + 0.032*\"chatbot\" + 0.032*\"application\" + 0.032*\"text\" + 0.032*\"tool\"\n",
      "2022-02-24 17:27:56,117 : INFO : topic diff=0.246040, rho=0.577350\n",
      "2022-02-24 17:27:56,123 : INFO : -5.239 per-word bound, 37.8 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,124 : INFO : PROGRESS: pass 2, at document #10/10\n",
      "2022-02-24 17:27:56,131 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"text\" + 0.032*\"tweet\" + 0.032*\"vector\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,133 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.036*\"course\" + 0.036*\"word\" + 0.026*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,134 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.032*\"deep\" + 0.032*\"transformer\" + 0.032*\"learning\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,134 : INFO : topic #3 (0.200): 0.062*\"course\" + 0.042*\"instructor\" + 0.025*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.023*\"embeddings\" + 0.023*\"localitysensitive\" + 0.023*\"translation\" + 0.023*\"sentiment\" + 0.023*\"rating\"\n",
      "2022-02-24 17:27:56,135 : INFO : topic #4 (0.200): 0.038*\"language\" + 0.038*\"specialization\" + 0.037*\"perform\" + 0.037*\"sentiment\" + 0.037*\"analysis\" + 0.035*\"nlp\" + 0.035*\"chatbot\" + 0.035*\"application\" + 0.035*\"tool\" + 0.035*\"end\"\n",
      "2022-02-24 17:27:56,136 : INFO : topic diff=0.129318, rho=0.500000\n",
      "2022-02-24 17:27:56,144 : INFO : -5.207 per-word bound, 36.9 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,145 : INFO : PROGRESS: pass 3, at document #10/10\n",
      "2022-02-24 17:27:56,149 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"text\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"vector\" + 0.032*\"feature\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,150 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.036*\"course\" + 0.036*\"word\" + 0.026*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,151 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,152 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.023*\"rating\"\n",
      "2022-02-24 17:27:56,152 : INFO : topic #4 (0.200): 0.038*\"language\" + 0.038*\"specialization\" + 0.037*\"perform\" + 0.037*\"sentiment\" + 0.037*\"analysis\" + 0.036*\"nlp\" + 0.036*\"chatbot\" + 0.036*\"application\" + 0.036*\"tool\" + 0.036*\"end\"\n",
      "2022-02-24 17:27:56,154 : INFO : topic diff=0.073126, rho=0.447214\n",
      "2022-02-24 17:27:56,162 : INFO : -5.196 per-word bound, 36.7 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,163 : INFO : PROGRESS: pass 4, at document #10/10\n",
      "2022-02-24 17:27:56,167 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,168 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.026*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,169 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,170 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,170 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"perform\" + 0.038*\"sentiment\" + 0.037*\"analysis\" + 0.037*\"nlp\" + 0.037*\"chatbot\" + 0.037*\"application\" + 0.037*\"tool\" + 0.037*\"end\"\n",
      "2022-02-24 17:27:56,171 : INFO : topic diff=0.042521, rho=0.408248\n",
      "2022-02-24 17:27:56,181 : INFO : -5.192 per-word bound, 36.6 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,181 : INFO : PROGRESS: pass 5, at document #10/10\n",
      "2022-02-24 17:27:56,184 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,186 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,186 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,186 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,188 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"perform\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.037*\"nlp\" + 0.037*\"chatbot\" + 0.037*\"application\" + 0.037*\"tool\" + 0.037*\"end\"\n",
      "2022-02-24 17:27:56,189 : INFO : topic diff=0.025381, rho=0.377964\n",
      "2022-02-24 17:27:56,196 : INFO : -5.191 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,196 : INFO : PROGRESS: pass 6, at document #10/10\n",
      "2022-02-24 17:27:56,200 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,202 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,202 : INFO : topic #2 (0.200): 0.032*\"machine\" + 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,203 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,204 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"perform\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.037*\"chatbot\" + 0.037*\"application\" + 0.037*\"tool\" + 0.037*\"end\"\n",
      "2022-02-24 17:27:56,206 : INFO : topic diff=0.015567, rho=0.353553\n",
      "2022-02-24 17:27:56,213 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,215 : INFO : PROGRESS: pass 7, at document #10/10\n",
      "2022-02-24 17:27:56,219 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,219 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,220 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"machine\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,221 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,222 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"perform\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\" + 0.038*\"text\"\n",
      "2022-02-24 17:27:56,223 : INFO : topic diff=0.009806, rho=0.333333\n",
      "2022-02-24 17:27:56,231 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,232 : INFO : PROGRESS: pass 8, at document #10/10\n",
      "2022-02-24 17:27:56,236 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,237 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"regression\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,237 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"machine\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,238 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,239 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,240 : INFO : topic diff=0.006335, rho=0.316228\n",
      "2022-02-24 17:27:56,247 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,248 : INFO : PROGRESS: pass 9, at document #10/10\n",
      "2022-02-24 17:27:56,251 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"text\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,252 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"model\" + 0.025*\"regression\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,253 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"machine\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,253 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,255 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,256 : INFO : topic diff=0.004188, rho=0.301511\n",
      "2022-02-24 17:27:56,263 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,264 : INFO : PROGRESS: pass 10, at document #10/10\n",
      "2022-02-24 17:27:56,268 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"binary\" + 0.032*\"feature\" + 0.032*\"classifier\" + 0.032*\"learn\"\n",
      "2022-02-24 17:27:56,269 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"course\" + 0.037*\"word\" + 0.025*\"model\" + 0.025*\"regression\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,270 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,271 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,271 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,272 : INFO : topic diff=0.002827, rho=0.288675\n",
      "2022-02-24 17:27:56,279 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,280 : INFO : PROGRESS: pass 11, at document #10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,283 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"binary\" + 0.032*\"feature\" + 0.032*\"classifier\" + 0.032*\"learn\"\n",
      "2022-02-24 17:27:56,284 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"regression\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,285 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,286 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,287 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"analysis\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,288 : INFO : topic diff=0.001944, rho=0.277350\n",
      "2022-02-24 17:27:56,295 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,296 : INFO : PROGRESS: pass 12, at document #10/10\n",
      "2022-02-24 17:27:56,299 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"classifier\" + 0.032*\"learn\"\n",
      "2022-02-24 17:27:56,300 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"relationship\" + 0.025*\"language\" + 0.025*\"regression\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,301 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,302 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.043*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"sentiment\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,302 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,304 : INFO : topic diff=0.001360, rho=0.267261\n",
      "2022-02-24 17:27:56,311 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,312 : INFO : PROGRESS: pass 13, at document #10/10\n",
      "2022-02-24 17:27:56,317 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"classifier\" + 0.032*\"feature\" + 0.032*\"learn\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,317 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"space\"\n",
      "2022-02-24 17:27:56,319 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,320 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"machine\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,321 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,322 : INFO : topic diff=0.000966, rho=0.258199\n",
      "2022-02-24 17:27:56,328 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,329 : INFO : PROGRESS: pass 14, at document #10/10\n",
      "2022-02-24 17:27:56,332 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"classifier\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"learn\"\n",
      "2022-02-24 17:27:56,333 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"space\"\n",
      "2022-02-24 17:27:56,335 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,336 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"vector\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,337 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"language\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"nlp\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,339 : INFO : topic diff=0.000695, rho=0.250000\n",
      "2022-02-24 17:27:56,345 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,346 : INFO : PROGRESS: pass 15, at document #10/10\n",
      "2022-02-24 17:27:56,349 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"classifier\" + 0.032*\"feature\" + 0.032*\"binary\" + 0.032*\"learn\"\n",
      "2022-02-24 17:27:56,350 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"space\"\n",
      "2022-02-24 17:27:56,351 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"ukasz\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,352 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"word\" + 0.024*\"space\" + 0.024*\"machine\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,353 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"language\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,354 : INFO : topic diff=0.000507, rho=0.242536\n",
      "2022-02-24 17:27:56,360 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,361 : INFO : PROGRESS: pass 16, at document #10/10\n",
      "2022-02-24 17:27:56,365 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"classifier\" + 0.032*\"feature\" + 0.032*\"learn\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,365 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"model\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,367 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"ukasz\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,367 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"space\" + 0.024*\"word\" + 0.024*\"vector\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\"\n",
      "2022-02-24 17:27:56,368 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,369 : INFO : topic diff=0.000373, rho=0.235702\n",
      "2022-02-24 17:27:56,376 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,377 : INFO : PROGRESS: pass 17, at document #10/10\n",
      "2022-02-24 17:27:56,382 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"binary\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"extract\" + 0.032*\"feature\"\n",
      "2022-02-24 17:27:56,383 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"model\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,384 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"ukasz\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"staff\" + 0.032*\"brain\" + 0.032*\"research\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,385 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"space\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"word\" + 0.024*\"vector\"\n",
      "2022-02-24 17:27:56,386 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,386 : INFO : topic diff=0.000278, rho=0.229416\n",
      "2022-02-24 17:27:56,393 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,395 : INFO : PROGRESS: pass 18, at document #10/10\n",
      "2022-02-24 17:27:56,398 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,399 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"model\" + 0.025*\"language\" + 0.025*\"relationship\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,400 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"google\" + 0.032*\"ukasz\" + 0.032*\"paper\" + 0.032*\"brain\" + 0.032*\"staff\" + 0.032*\"tensorflow\" + 0.032*\"scientist\"\n",
      "2022-02-24 17:27:56,401 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"space\" + 0.024*\"sentiment\" + 0.024*\"vector\"\n",
      "2022-02-24 17:27:56,403 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,403 : INFO : topic diff=0.000209, rho=0.223607\n",
      "2022-02-24 17:27:56,410 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,411 : INFO : PROGRESS: pass 19, at document #10/10\n",
      "2022-02-24 17:27:56,416 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,416 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"model\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,418 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"transformer\" + 0.032*\"ukasz\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"staff\" + 0.032*\"brain\" + 0.032*\"scientist\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,419 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"space\" + 0.024*\"sentiment\" + 0.024*\"analysis\"\n",
      "2022-02-24 17:27:56,420 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,421 : INFO : topic diff=0.000158, rho=0.218218\n",
      "2022-02-24 17:27:56,427 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,428 : INFO : PROGRESS: pass 20, at document #10/10\n",
      "2022-02-24 17:27:56,432 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,433 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"model\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,434 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"brain\" + 0.032*\"transformer\" + 0.032*\"staff\" + 0.032*\"tensorflow\" + 0.032*\"coauthor\"\n",
      "2022-02-24 17:27:56,435 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"space\" + 0.024*\"analysis\"\n",
      "2022-02-24 17:27:56,436 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,437 : INFO : topic diff=0.000121, rho=0.213201\n",
      "2022-02-24 17:27:56,443 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,444 : INFO : PROGRESS: pass 21, at document #10/10\n",
      "2022-02-24 17:27:56,448 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,449 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"model\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,450 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"tensorflow\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,450 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"analysis\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:56,451 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"language\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,452 : INFO : topic diff=0.000093, rho=0.208514\n",
      "2022-02-24 17:27:56,458 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,459 : INFO : PROGRESS: pass 22, at document #10/10\n",
      "2022-02-24 17:27:56,464 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"analysis\" + 0.032*\"sentiment\" + 0.032*\"tweet\" + 0.032*\"extract\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"classifier\"\n",
      "2022-02-24 17:27:56,464 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,465 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"tensorflow\" + 0.032*\"staff\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,466 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"analysis\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:56,467 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"language\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,468 : INFO : topic diff=0.000072, rho=0.204124\n",
      "2022-02-24 17:27:56,474 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,475 : INFO : PROGRESS: pass 23, at document #10/10\n",
      "2022-02-24 17:27:56,479 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,480 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,481 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"ukasz\" + 0.032*\"google\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"trax\" + 0.032*\"coauthor\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,482 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"analysis\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:56,482 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,483 : INFO : topic diff=0.000056, rho=0.200000\n",
      "2022-02-24 17:27:56,492 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,493 : INFO : PROGRESS: pass 24, at document #10/10\n",
      "2022-02-24 17:27:56,497 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,499 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,500 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"ukasz\" + 0.032*\"staff\" + 0.032*\"coauthor\" + 0.032*\"trax\"\n",
      "2022-02-24 17:27:56,501 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"analysis\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:56,502 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,503 : INFO : topic diff=0.000044, rho=0.196116\n",
      "2022-02-24 17:27:56,511 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,512 : INFO : PROGRESS: pass 25, at document #10/10\n",
      "2022-02-24 17:27:56,516 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,517 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,518 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"ukasz\" + 0.032*\"transformer\" + 0.032*\"google\" + 0.032*\"scientist\" + 0.032*\"coauthor\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,519 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"analysis\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:56,520 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,521 : INFO : topic diff=0.000035, rho=0.192450\n",
      "2022-02-24 17:27:56,527 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,528 : INFO : PROGRESS: pass 26, at document #10/10\n",
      "2022-02-24 17:27:56,531 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,532 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,533 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"google\" + 0.032*\"ukasz\" + 0.032*\"transformer\" + 0.032*\"coauthor\" + 0.032*\"staff\" + 0.032*\"research\"\n",
      "2022-02-24 17:27:56,534 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"sentiment\" + 0.024*\"oa\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,535 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,536 : INFO : topic diff=0.000028, rho=0.188982\n",
      "2022-02-24 17:27:56,547 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,548 : INFO : PROGRESS: pass 27, at document #10/10\n",
      "2022-02-24 17:27:56,553 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,555 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,557 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"ukasz\" + 0.032*\"transformer\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"scientist\" + 0.032*\"coauthor\"\n",
      "2022-02-24 17:27:56,559 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"sentiment\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,561 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,562 : INFO : topic diff=0.000022, rho=0.185695\n",
      "2022-02-24 17:27:56,574 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,575 : INFO : PROGRESS: pass 28, at document #10/10\n",
      "2022-02-24 17:27:56,580 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,581 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"model\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,582 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"research\" + 0.032*\"paper\" + 0.032*\"coauthor\" + 0.032*\"staff\" + 0.032*\"scientist\" + 0.032*\"transformer\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,582 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"cover\" + 0.024*\"good\"\n",
      "2022-02-24 17:27:56,583 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"end\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,585 : INFO : topic diff=0.000018, rho=0.182574\n",
      "2022-02-24 17:27:56,591 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,592 : INFO : PROGRESS: pass 29, at document #10/10\n",
      "2022-02-24 17:27:56,599 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,601 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"model\" + 0.025*\"natural\" + 0.025*\"processing\"\n",
      "2022-02-24 17:27:56,602 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"research\" + 0.032*\"paper\" + 0.032*\"scientist\" + 0.032*\"coauthor\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"trax\"\n",
      "2022-02-24 17:27:56,603 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"cover\" + 0.024*\"week\"\n",
      "2022-02-24 17:27:56,604 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"tool\" + 0.038*\"chatbot\" + 0.038*\"end\" + 0.038*\"application\"\n",
      "2022-02-24 17:27:56,606 : INFO : topic diff=0.000014, rho=0.179605\n",
      "2022-02-24 17:27:56,616 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,617 : INFO : PROGRESS: pass 30, at document #10/10\n",
      "2022-02-24 17:27:56,623 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,624 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"model\" + 0.025*\"processing\" + 0.025*\"natural\"\n",
      "2022-02-24 17:27:56,627 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"research\" + 0.032*\"paper\" + 0.032*\"scientist\" + 0.032*\"coauthor\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,628 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"week\"\n",
      "2022-02-24 17:27:56,628 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"translate\" + 0.038*\"end\" + 0.038*\"tool\"\n",
      "2022-02-24 17:27:56,629 : INFO : topic diff=0.000011, rho=0.176777\n",
      "2022-02-24 17:27:56,640 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,641 : INFO : PROGRESS: pass 31, at document #10/10\n",
      "2022-02-24 17:27:56,648 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,650 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"model\"\n",
      "2022-02-24 17:27:56,651 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"research\" + 0.032*\"paper\" + 0.032*\"scientist\" + 0.032*\"coauthor\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"trax\"\n",
      "2022-02-24 17:27:56,652 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,653 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"end\" + 0.038*\"chatbot\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,654 : INFO : topic diff=0.000009, rho=0.174078\n",
      "2022-02-24 17:27:56,663 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,665 : INFO : PROGRESS: pass 32, at document #10/10\n",
      "2022-02-24 17:27:56,675 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,676 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,677 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"scientist\" + 0.032*\"coauthor\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,679 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,680 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"end\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,681 : INFO : topic diff=0.000008, rho=0.171499\n",
      "2022-02-24 17:27:56,690 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,692 : INFO : PROGRESS: pass 33, at document #10/10\n",
      "2022-02-24 17:27:56,698 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,699 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,700 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"transformer\"\n",
      "2022-02-24 17:27:56,702 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,703 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"tool\" + 0.038*\"chatbot\" + 0.038*\"end\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,704 : INFO : topic diff=0.000006, rho=0.169031\n",
      "2022-02-24 17:27:56,716 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,717 : INFO : PROGRESS: pass 34, at document #10/10\n",
      "2022-02-24 17:27:56,726 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,728 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,729 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,730 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,731 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"tool\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,732 : INFO : topic diff=0.000005, rho=0.166667\n",
      "2022-02-24 17:27:56,741 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,742 : INFO : PROGRESS: pass 35, at document #10/10\n",
      "2022-02-24 17:27:56,747 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,748 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,749 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,750 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,751 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"end\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,752 : INFO : topic diff=0.000004, rho=0.164399\n",
      "2022-02-24 17:27:56,762 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,763 : INFO : PROGRESS: pass 36, at document #10/10\n",
      "2022-02-24 17:27:56,770 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"numerical\" + 0.032*\"binary\" + 0.032*\"learn\" + 0.032*\"feature\" + 0.032*\"extract\"\n",
      "2022-02-24 17:27:56,772 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,773 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"coauthor\" + 0.032*\"research\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,774 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,776 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"end\" + 0.038*\"application\" + 0.038*\"tool\" + 0.038*\"chatbot\"\n",
      "2022-02-24 17:27:56,778 : INFO : topic diff=0.000004, rho=0.162221\n",
      "2022-02-24 17:27:56,788 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,790 : INFO : PROGRESS: pass 37, at document #10/10\n",
      "2022-02-24 17:27:56,797 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"binary\" + 0.032*\"classifier\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"feature\"\n",
      "2022-02-24 17:27:56,799 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,800 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"trax\"\n",
      "2022-02-24 17:27:56,801 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,803 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"end\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,804 : INFO : topic diff=0.000003, rho=0.160128\n",
      "2022-02-24 17:27:56,813 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,814 : INFO : PROGRESS: pass 38, at document #10/10\n",
      "2022-02-24 17:27:56,822 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"binary\" + 0.032*\"classifier\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"feature\"\n",
      "2022-02-24 17:27:56,823 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,824 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"staff\" + 0.032*\"paper\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"brain\" + 0.032*\"google\" + 0.032*\"tensorflow\"\n",
      "2022-02-24 17:27:56,826 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"cover\" + 0.024*\"good\"\n",
      "2022-02-24 17:27:56,827 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"translate\" + 0.038*\"end\"\n",
      "2022-02-24 17:27:56,828 : INFO : topic diff=0.000002, rho=0.158114\n",
      "2022-02-24 17:27:56,838 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,839 : INFO : PROGRESS: pass 39, at document #10/10\n",
      "2022-02-24 17:27:56,847 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,848 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,850 : INFO : topic #2 (0.200): 0.032*\"deep\" + 0.032*\"learning\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"coauthor\" + 0.032*\"research\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\" + 0.032*\"trax\"\n",
      "2022-02-24 17:27:56,853 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"cover\" + 0.024*\"good\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,854 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"chatbot\" + 0.038*\"application\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,855 : INFO : topic diff=0.000002, rho=0.156174\n",
      "2022-02-24 17:27:56,865 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,866 : INFO : PROGRESS: pass 40, at document #10/10\n",
      "2022-02-24 17:27:56,874 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,877 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,878 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,880 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,881 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,883 : INFO : topic diff=0.000002, rho=0.154303\n",
      "2022-02-24 17:27:56,896 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,897 : INFO : PROGRESS: pass 41, at document #10/10\n",
      "2022-02-24 17:27:56,903 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,905 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,906 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,907 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"week\"\n",
      "2022-02-24 17:27:56,907 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,909 : INFO : topic diff=0.000001, rho=0.152499\n",
      "2022-02-24 17:27:56,919 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,920 : INFO : PROGRESS: pass 42, at document #10/10\n",
      "2022-02-24 17:27:56,926 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,927 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,929 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,929 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"week\"\n",
      "2022-02-24 17:27:56,930 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,931 : INFO : topic diff=0.000001, rho=0.150756\n",
      "2022-02-24 17:27:56,939 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,941 : INFO : PROGRESS: pass 43, at document #10/10\n",
      "2022-02-24 17:27:56,946 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,947 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,948 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,949 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"rating\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,951 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,952 : INFO : topic diff=0.000001, rho=0.149071\n",
      "2022-02-24 17:27:56,959 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,960 : INFO : PROGRESS: pass 44, at document #10/10\n",
      "2022-02-24 17:27:56,964 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,966 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:56,967 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:56,967 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"embeddings\" + 0.024*\"rating\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:56,968 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,969 : INFO : topic diff=0.000001, rho=0.147442\n",
      "2022-02-24 17:27:56,981 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:56,982 : INFO : PROGRESS: pass 45, at document #10/10\n",
      "2022-02-24 17:27:56,987 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:56,988 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"use\" + 0.025*\"natural\" + 0.025*\"processing\"\n",
      "2022-02-24 17:27:56,990 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:56,992 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"oa\" + 0.024*\"good\" + 0.024*\"ha\"\n",
      "2022-02-24 17:27:56,994 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:56,994 : INFO : topic diff=0.000001, rho=0.145865\n",
      "2022-02-24 17:27:57,001 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,001 : INFO : PROGRESS: pass 46, at document #10/10\n",
      "2022-02-24 17:27:57,005 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,006 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,007 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,008 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"localitysensitive\" + 0.024*\"good\" + 0.024*\"cover\" + 0.024*\"oa\"\n",
      "2022-02-24 17:27:57,010 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,010 : INFO : topic diff=0.000001, rho=0.144338\n",
      "2022-02-24 17:27:57,016 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,016 : INFO : PROGRESS: pass 47, at document #10/10\n",
      "2022-02-24 17:27:57,022 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,023 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,025 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,025 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"embeddings\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,026 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,027 : INFO : topic diff=0.000001, rho=0.142857\n",
      "2022-02-24 17:27:57,037 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,037 : INFO : PROGRESS: pass 48, at document #10/10\n",
      "2022-02-24 17:27:57,042 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,043 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"relationship\" + 0.025*\"use\" + 0.025*\"natural\" + 0.025*\"processing\"\n",
      "2022-02-24 17:27:57,044 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,045 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"embeddings\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,046 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,046 : INFO : topic diff=0.000000, rho=0.141421\n",
      "2022-02-24 17:27:57,054 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,055 : INFO : PROGRESS: pass 49, at document #10/10\n",
      "2022-02-24 17:27:57,060 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,061 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"relationship\" + 0.025*\"natural\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,062 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,063 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"translation\" + 0.024*\"embeddings\" + 0.024*\"rating\" + 0.024*\"oa\" + 0.024*\"week\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,064 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,065 : INFO : topic diff=0.000000, rho=0.140028\n",
      "2022-02-24 17:27:57,072 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,074 : INFO : PROGRESS: pass 50, at document #10/10\n",
      "2022-02-24 17:27:57,080 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,082 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"natural\" + 0.025*\"processing\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,083 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,084 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,085 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,086 : INFO : topic diff=0.000000, rho=0.138675\n",
      "2022-02-24 17:27:57,093 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,095 : INFO : PROGRESS: pass 51, at document #10/10\n",
      "2022-02-24 17:27:57,102 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,103 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,105 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,106 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,107 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,108 : INFO : topic diff=0.000000, rho=0.137361\n",
      "2022-02-24 17:27:57,115 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,116 : INFO : PROGRESS: pass 52, at document #10/10\n",
      "2022-02-24 17:27:57,121 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,126 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,129 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,130 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,131 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,132 : INFO : topic diff=0.000000, rho=0.136083\n",
      "2022-02-24 17:27:57,142 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,143 : INFO : PROGRESS: pass 53, at document #10/10\n",
      "2022-02-24 17:27:57,150 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,151 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,151 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,152 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,153 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,154 : INFO : topic diff=0.000000, rho=0.134840\n",
      "2022-02-24 17:27:57,163 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,164 : INFO : PROGRESS: pass 54, at document #10/10\n",
      "2022-02-24 17:27:57,169 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,170 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,171 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,172 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,173 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,174 : INFO : topic diff=0.000000, rho=0.133631\n",
      "2022-02-24 17:27:57,183 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,184 : INFO : PROGRESS: pass 55, at document #10/10\n",
      "2022-02-24 17:27:57,189 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,191 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,192 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,192 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,193 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,194 : INFO : topic diff=0.000000, rho=0.132453\n",
      "2022-02-24 17:27:57,202 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,202 : INFO : PROGRESS: pass 56, at document #10/10\n",
      "2022-02-24 17:27:57,208 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,209 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,210 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,211 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,212 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,212 : INFO : topic diff=0.000000, rho=0.131306\n",
      "2022-02-24 17:27:57,221 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,223 : INFO : PROGRESS: pass 57, at document #10/10\n",
      "2022-02-24 17:27:57,229 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,231 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,232 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,233 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,234 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,235 : INFO : topic diff=0.000000, rho=0.130189\n",
      "2022-02-24 17:27:57,245 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,246 : INFO : PROGRESS: pass 58, at document #10/10\n",
      "2022-02-24 17:27:57,251 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,253 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\" + 0.025*\"use\"\n",
      "2022-02-24 17:27:57,254 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,256 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,257 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,258 : INFO : topic diff=0.000000, rho=0.129099\n",
      "2022-02-24 17:27:57,267 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,268 : INFO : PROGRESS: pass 59, at document #10/10\n",
      "2022-02-24 17:27:57,272 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,273 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,274 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,275 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,276 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,277 : INFO : topic diff=0.000000, rho=0.128037\n",
      "2022-02-24 17:27:57,285 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,286 : INFO : PROGRESS: pass 60, at document #10/10\n",
      "2022-02-24 17:27:57,291 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,292 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,293 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,294 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,295 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,296 : INFO : topic diff=0.000000, rho=0.127000\n",
      "2022-02-24 17:27:57,305 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,306 : INFO : PROGRESS: pass 61, at document #10/10\n",
      "2022-02-24 17:27:57,313 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,314 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,315 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,316 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,317 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,318 : INFO : topic diff=0.000000, rho=0.125988\n",
      "2022-02-24 17:27:57,326 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,327 : INFO : PROGRESS: pass 62, at document #10/10\n",
      "2022-02-24 17:27:57,332 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,333 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,335 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,336 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,337 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,338 : INFO : topic diff=0.000000, rho=0.125000\n",
      "2022-02-24 17:27:57,346 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,347 : INFO : PROGRESS: pass 63, at document #10/10\n",
      "2022-02-24 17:27:57,353 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,355 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,356 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,357 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,358 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,359 : INFO : topic diff=0.000000, rho=0.124035\n",
      "2022-02-24 17:27:57,367 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,368 : INFO : PROGRESS: pass 64, at document #10/10\n",
      "2022-02-24 17:27:57,372 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,373 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,374 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,375 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,376 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,377 : INFO : topic diff=0.000000, rho=0.123091\n",
      "2022-02-24 17:27:57,387 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,388 : INFO : PROGRESS: pass 65, at document #10/10\n",
      "2022-02-24 17:27:57,394 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,395 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,396 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,397 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,398 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,399 : INFO : topic diff=0.000000, rho=0.122169\n",
      "2022-02-24 17:27:57,407 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,409 : INFO : PROGRESS: pass 66, at document #10/10\n",
      "2022-02-24 17:27:57,415 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,416 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,417 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,417 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,418 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,419 : INFO : topic diff=0.000000, rho=0.121268\n",
      "2022-02-24 17:27:57,428 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,428 : INFO : PROGRESS: pass 67, at document #10/10\n",
      "2022-02-24 17:27:57,434 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,435 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,436 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,437 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,439 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,440 : INFO : topic diff=0.000000, rho=0.120386\n",
      "2022-02-24 17:27:57,449 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,449 : INFO : PROGRESS: pass 68, at document #10/10\n",
      "2022-02-24 17:27:57,455 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,456 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,457 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,458 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,460 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,460 : INFO : topic diff=0.000000, rho=0.119523\n",
      "2022-02-24 17:27:57,469 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,470 : INFO : PROGRESS: pass 69, at document #10/10\n",
      "2022-02-24 17:27:57,474 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,476 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,477 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,478 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,479 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,480 : INFO : topic diff=0.000000, rho=0.118678\n",
      "2022-02-24 17:27:57,485 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,486 : INFO : PROGRESS: pass 70, at document #10/10\n",
      "2022-02-24 17:27:57,491 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,492 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,493 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,494 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,495 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,496 : INFO : topic diff=0.000000, rho=0.117851\n",
      "2022-02-24 17:27:57,502 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,503 : INFO : PROGRESS: pass 71, at document #10/10\n",
      "2022-02-24 17:27:57,509 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,510 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,511 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,513 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,514 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"sentiment\" + 0.038*\"analysis\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,514 : INFO : topic diff=0.000000, rho=0.117041\n",
      "2022-02-24 17:27:57,525 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,527 : INFO : PROGRESS: pass 72, at document #10/10\n",
      "2022-02-24 17:27:57,532 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,533 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,534 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,536 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,537 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,539 : INFO : topic diff=0.000000, rho=0.116248\n",
      "2022-02-24 17:27:57,547 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,548 : INFO : PROGRESS: pass 73, at document #10/10\n",
      "2022-02-24 17:27:57,553 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,554 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,555 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,556 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,557 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,558 : INFO : topic diff=0.000000, rho=0.115470\n",
      "2022-02-24 17:27:57,564 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,565 : INFO : PROGRESS: pass 74, at document #10/10\n",
      "2022-02-24 17:27:57,570 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,571 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,572 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,573 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,575 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,576 : INFO : topic diff=0.000000, rho=0.114708\n",
      "2022-02-24 17:27:57,584 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,584 : INFO : PROGRESS: pass 75, at document #10/10\n",
      "2022-02-24 17:27:57,590 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,591 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,593 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,594 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,596 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,597 : INFO : topic diff=0.000000, rho=0.113961\n",
      "2022-02-24 17:27:57,604 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,605 : INFO : PROGRESS: pass 76, at document #10/10\n",
      "2022-02-24 17:27:57,611 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,612 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,613 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,614 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,615 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,616 : INFO : topic diff=0.000000, rho=0.113228\n",
      "2022-02-24 17:27:57,623 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,623 : INFO : PROGRESS: pass 77, at document #10/10\n",
      "2022-02-24 17:27:57,629 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,630 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,632 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,633 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,634 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,635 : INFO : topic diff=0.000000, rho=0.112509\n",
      "2022-02-24 17:27:57,643 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,644 : INFO : PROGRESS: pass 78, at document #10/10\n",
      "2022-02-24 17:27:57,650 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,651 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,652 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,653 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,654 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,655 : INFO : topic diff=0.000000, rho=0.111803\n",
      "2022-02-24 17:27:57,664 : INFO : -5.190 per-word bound, 36.5 perplexity estimate based on a held-out corpus of 10 documents with 146 words\n",
      "2022-02-24 17:27:57,665 : INFO : PROGRESS: pass 79, at document #10/10\n",
      "2022-02-24 17:27:57,672 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,673 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,675 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 17:27:57,676 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,677 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n",
      "2022-02-24 17:27:57,678 : INFO : topic diff=0.000000, rho=0.111111\n",
      "2022-02-24 17:27:57,681 : INFO : topic #0 (0.200): 0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"\n",
      "2022-02-24 17:27:57,681 : INFO : topic #1 (0.200): 0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"\n",
      "2022-02-24 17:27:57,682 : INFO : topic #2 (0.200): 0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"\n",
      "2022-02-24 17:27:57,683 : INFO : topic #3 (0.200): 0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"\n",
      "2022-02-24 17:27:57,685 : INFO : topic #4 (0.200): 0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"regression\" + 0.059*\"logistic\" + 0.032*\"sentiment\" + 0.032*\"analysis\" + 0.032*\"tweet\" + 0.032*\"classifier\" + 0.032*\"learn\" + 0.032*\"numerical\" + 0.032*\"lesson\" + 0.032*\"binary\"'),\n",
       " (1,\n",
       "  '0.037*\"specialization\" + 0.037*\"word\" + 0.037*\"course\" + 0.025*\"vector\" + 0.025*\"language\" + 0.025*\"space\" + 0.025*\"use\" + 0.025*\"processing\" + 0.025*\"natural\" + 0.025*\"relationship\"'),\n",
       " (2,\n",
       "  '0.032*\"learning\" + 0.032*\"deep\" + 0.032*\"brain\" + 0.032*\"paper\" + 0.032*\"library\" + 0.032*\"research\" + 0.032*\"coauthor\" + 0.032*\"scientist\" + 0.032*\"google\" + 0.032*\"staff\"'),\n",
       " (3,\n",
       "  '0.063*\"course\" + 0.044*\"instructor\" + 0.024*\"machine\" + 0.024*\"localitysensitive\" + 0.024*\"rating\" + 0.024*\"embeddings\" + 0.024*\"translation\" + 0.024*\"solve\" + 0.024*\"deeplearnigai\" + 0.024*\"cover\"'),\n",
       " (4,\n",
       "  '0.038*\"specialization\" + 0.038*\"nlp\" + 0.038*\"analysis\" + 0.038*\"sentiment\" + 0.038*\"perform\" + 0.038*\"text\" + 0.038*\"application\" + 0.038*\"chatbot\" + 0.038*\"tool\" + 0.038*\"translate\"')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3bec4a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(1, 0.9842049)], 0),\n",
       " ([(0, 0.015472394),\n",
       "   (1, 0.015496536),\n",
       "   (2, 0.015443179),\n",
       "   (3, 0.015427861),\n",
       "   (4, 0.93816)],\n",
       "  1),\n",
       " ([(0, 0.02857671),\n",
       "   (1, 0.028900608),\n",
       "   (2, 0.88494927),\n",
       "   (3, 0.028670197),\n",
       "   (4, 0.028903188)],\n",
       "  2),\n",
       " ([(0, 0.020007774),\n",
       "   (1, 0.91934437),\n",
       "   (2, 0.020343369),\n",
       "   (3, 0.020250553),\n",
       "   (4, 0.020053955)],\n",
       "  3),\n",
       " ([(0, 0.015387398),\n",
       "   (1, 0.015385863),\n",
       "   (2, 0.9384521),\n",
       "   (3, 0.015386697),\n",
       "   (4, 0.015387925)],\n",
       "  4),\n",
       " ([(3, 0.9702209)], 5),\n",
       " ([(0, 0.022410458),\n",
       "   (1, 0.9108991),\n",
       "   (2, 0.022230314),\n",
       "   (3, 0.022228278),\n",
       "   (4, 0.022231828)],\n",
       "  6),\n",
       " ([(0, 0.73328745),\n",
       "   (1, 0.06667272),\n",
       "   (2, 0.06668023),\n",
       "   (3, 0.0666768),\n",
       "   (4, 0.06668278)],\n",
       "  7),\n",
       " ([(0, 0.9527546),\n",
       "   (1, 0.011833893),\n",
       "   (2, 0.011766516),\n",
       "   (3, 0.011808598),\n",
       "   (4, 0.011836318)],\n",
       "  8),\n",
       " ([(0, 0.033339832),\n",
       "   (1, 0.033427913),\n",
       "   (2, 0.033339832),\n",
       "   (3, 0.8665514),\n",
       "   (4, 0.03334105)],\n",
       "  9)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "137cda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dom_Topic  Topic_Contri                                           Keywords\n",
      "0        1.0        0.9842  specialization, word, course, vector, language...\n",
      "1        4.0        0.9382  specialization, nlp, analysis, sentiment, perf...\n",
      "2        2.0        0.8850  learning, deep, brain, paper, library, researc...\n",
      "3        1.0        0.9193  specialization, word, course, vector, language...\n",
      "4        2.0        0.9385  learning, deep, brain, paper, library, researc...\n",
      "5        3.0        0.9702  course, instructor, machine, localitysensitive...\n",
      "6        1.0        0.9109  specialization, word, course, vector, language...\n",
      "7        0.0        0.7333  regression, logistic, sentiment, analysis, twe...\n",
      "8        0.0        0.9528  regression, logistic, sentiment, analysis, twe...\n",
      "9        3.0        0.8666  course, instructor, machine, localitysensitive...\n"
     ]
    }
   ],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(ldana[corpusna]):\n",
    "        row = row_list[0] if ldana.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldana.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "sent_topics_df.columns = ['Dom_Topic', 'Topic_Contri', 'Keywords']\n",
    "print(sent_topics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
