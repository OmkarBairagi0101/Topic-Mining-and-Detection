{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb90c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# Scrapes transcript data from scrapsfromtheloft.com\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find_all('p')]\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "# URLs of transcripts in scope\n",
    "urls = [\"https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/gNXI3/vocabulary-feature-extraction\",\n",
    "       \"https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/dDdRc/welcome-to-the-nlp-specialization\",\n",
    "       \"https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/cITmZ/negative-and-positive-frequencies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb36507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/gNXI3/vocabulary-feature-extraction\n",
      "https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/dDdRc/welcome-to-the-nlp-specialization\n",
      "https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/cITmZ/negative-and-positive-frequencies\n"
     ]
    }
   ],
   "source": [
    "transcripts = [url_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc49a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4.6 (3,347 ratings)', 'Â\\xa0|Â\\xa0', '99K Students Enrolled', 'Course 1 of 4 in the Natural Language Processing Specialization', 'This Course', 'Video Transcript', '', 'In Course 1 of the Natural Language Processing Specialization, you will:   \\n\\na) Perform sentiment analysis of tweets using logistic regression and then naÃ¯ve Bayes, \\nb) Use vector space models to discover relationships between words and use PCA to reduce the dimensionality of the vector space and visualize those relationships, and\\nc) Write a simple English to French translation algorithm using pre-computed word embeddings and locality-sensitive hashing to relate words via approximate k-nearest neighbor search.  \\n    \\n  \\nBy the end of this Specialization, you will have designed NLP applications that perform question-answering and sentiment analysis, created tools to translate languages and summarize text, and even built a chatbot!   \\n   \\nThis Specialization is designed and taught by two experts in NLP, machine learning, and deep learning. Younes Bensouda Mourri is an Instructor of AI at Stanford University who also helped build the Deep Learning Specialization. Å\\x81ukasz Kaiser is a Staff Research Scientist at Google Brain and the co-author of Tensorflow, the Tensor2Tensor and Trax libraries, and the Transformer paper.', 'Machine Translation, Word Embeddings, Locality-Sensitive Hashing, Sentiment Analysis, Vector Space Models', '4.6 (3,347 ratings)', 'HA', 'Aug 9, 2020', 'one of the Best course that i had attented in deeplearnig.ai the last week assignment was\\\\n\\\\nto good to solve which cover up all which we studied in entire course waiting for course 4 of nlp eagerly', 'OA', 'Aug 17, 2020', 'Awesome. The lecture are very exciting and detailed, though little hard and too straight forward sometimes, but Youtube helped in Regression models. Other then that, I was very informative and fun.', 'From the lesson', 'Sentiment Analysis with Logistic Regression', 'Learn to extract features from text into numerical vectors, then build a binary classifier for tweets using a logistic regression! ', 'Instructor', 'Instructor ', 'Senior Curriculum Developer'], ['4.6 (3,348 ratings)', 'Â\\xa0|Â\\xa0', '99K Students Enrolled', 'Course 1 of 4 in the Natural Language Processing Specialization', 'This Course', 'Video Transcript', '', 'In Course 1 of the Natural Language Processing Specialization, you will:   \\n\\na) Perform sentiment analysis of tweets using logistic regression and then naÃ¯ve Bayes, \\nb) Use vector space models to discover relationships between words and use PCA to reduce the dimensionality of the vector space and visualize those relationships, and\\nc) Write a simple English to French translation algorithm using pre-computed word embeddings and locality-sensitive hashing to relate words via approximate k-nearest neighbor search.  \\n    \\n  \\nBy the end of this Specialization, you will have designed NLP applications that perform question-answering and sentiment analysis, created tools to translate languages and summarize text, and even built a chatbot!   \\n   \\nThis Specialization is designed and taught by two experts in NLP, machine learning, and deep learning. Younes Bensouda Mourri is an Instructor of AI at Stanford University who also helped build the Deep Learning Specialization. Å\\x81ukasz Kaiser is a Staff Research Scientist at Google Brain and the co-author of Tensorflow, the Tensor2Tensor and Trax libraries, and the Transformer paper.', 'Machine Translation, Word Embeddings, Locality-Sensitive Hashing, Sentiment Analysis, Vector Space Models', '4.6 (3,348 ratings)', 'HA', 'Aug 9, 2020', 'one of the Best course that i had attented in deeplearnig.ai the last week assignment was\\\\n\\\\nto good to solve which cover up all which we studied in entire course waiting for course 4 of nlp eagerly', 'OA', 'Aug 17, 2020', 'Awesome. The lecture are very exciting and detailed, though little hard and too straight forward sometimes, but Youtube helped in Regression models. Other then that, I was very informative and fun.', 'From the lesson', 'Sentiment Analysis with Logistic Regression', 'Learn to extract features from text into numerical vectors, then build a binary classifier for tweets using a logistic regression! ', 'Instructor', 'Instructor ', 'Senior Curriculum Developer'], ['4.6 (3,348 ratings)', 'Â\\xa0|Â\\xa0', '99K Students Enrolled', 'Course 1 of 4 in the Natural Language Processing Specialization', 'This Course', 'Video Transcript', '', 'In Course 1 of the Natural Language Processing Specialization, you will:   \\n\\na) Perform sentiment analysis of tweets using logistic regression and then naÃ¯ve Bayes, \\nb) Use vector space models to discover relationships between words and use PCA to reduce the dimensionality of the vector space and visualize those relationships, and\\nc) Write a simple English to French translation algorithm using pre-computed word embeddings and locality-sensitive hashing to relate words via approximate k-nearest neighbor search.  \\n    \\n  \\nBy the end of this Specialization, you will have designed NLP applications that perform question-answering and sentiment analysis, created tools to translate languages and summarize text, and even built a chatbot!   \\n   \\nThis Specialization is designed and taught by two experts in NLP, machine learning, and deep learning. Younes Bensouda Mourri is an Instructor of AI at Stanford University who also helped build the Deep Learning Specialization. Å\\x81ukasz Kaiser is a Staff Research Scientist at Google Brain and the co-author of Tensorflow, the Tensor2Tensor and Trax libraries, and the Transformer paper.', 'Machine Translation, Word Embeddings, Locality-Sensitive Hashing, Sentiment Analysis, Vector Space Models', '4.6 (3,348 ratings)', 'HA', 'Aug 9, 2020', 'one of the Best course that i had attented in deeplearnig.ai the last week assignment was\\\\n\\\\nto good to solve which cover up all which we studied in entire course waiting for course 4 of nlp eagerly', 'OA', 'Aug 17, 2020', 'Awesome. The lecture are very exciting and detailed, though little hard and too straight forward sometimes, but Youtube helped in Regression models. Other then that, I was very informative and fun.', 'From the lesson', 'Sentiment Analysis with Logistic Regression', 'Learn to extract features from text into numerical vectors, then build a binary classifier for tweets using a logistic regression! ', 'Instructor', 'Instructor ', 'Senior Curriculum Developer', \"We'll now learn to generates counts,  which you can then use as features into  your logistic regression classifier.  Specifically, given a word,  you want to keep track of the number of times,  that's where it shows up as the positive class.  Given another word you want  to keep track of the number of  times that word showed up in the negative class.  Using both those counts,  you can then extract features and use  those features into your logistic regression classifier.  So let's take a look at how you can do that.  It is helpful to first  imagine how these two classes would look.  Here for instance, you could  have a corpus consisting of four tweets.  Associated with that corpus,  you would have a set of unique words, your vocabulary.  In this example,  your vocabulary would have eight unique words.  For this particular example of sentiment analysis,  you have two classes.  One class associated with  positive sentiment and the other with negative sentiment.  So taking your corpus,  you'd have a set of two tweets  that belong to the positive class,  and the sets of two tweets that  belong to the negative class.  Let's take the sets of positive tweets.  Now, take a look at your vocabulary.  To get the positive frequency in  any word in your vocabulary,  you will have to count the times as it  appears in the positive tweets.  For instance, the word happy  appears one time in the first positive tweet,  and another time in the second positive tweet.  So it's positive frequency is two.  The complete table looks like this.  Feel free to take a pause and check any of its entries.  The same logic applies for  getting the negative frequency.  However, for the sake of clarity,  look at some example,  the word am appears two times in  the first tweet and another time in the second one.  So it's negative frequency is three.  Take a look at the entire table for  negative frequencies and feel free to check its values.  So this is the entire table with  the positive and negative frequencies for your corpus.  In practice when coding,  this table is a dictionary mapping from  a word class there to its frequency.  So it maps the word and its corresponding class to  the frequency or the number of  times that's where it showed up in the class.  You now know how to create a frequency dictionary,  which maps a word and the class to  the number of times that word  showed up in the corresponding class.  In the next video, you're going to use  your frequency dictionary to represent a tweet.\"]]\n"
     ]
    }
   ],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb8f28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file transcripts already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir transcripts\n",
    "\n",
    "for i in range(len(transcripts)):\n",
    "    with open(\"Transcripts.txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file)\n",
    "        paragraph = transcripts[i]\n",
    "str1 = \" \"\n",
    "str1 = str1.join(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d9d98d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .', 'By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !', 'This Specialization designed taught two expert NLP , machine learning , deep learning .', 'Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .', 'Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .', 'Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .', 'The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .', 'Other , I informative fun .', 'From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !', \"Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\", \"Specifically , given word , want keep track number time , 's show positive class .\", 'Given another word want keep track number time word showed negative class .', 'Using count , extract feature use feature logistic regression classifier .', \"So let 's take look .\", 'It helpful first imagine two class would look .', 'Here instance , could corpus consisting four tweet .', 'Associated corpus , would set unique word , vocabulary .', 'In example , vocabulary would eight unique word .', 'For particular example sentiment analysis , two class .', 'One class associated positive sentiment negative sentiment .', \"So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\", \"Let 's take set positive tweet .\", 'Now , take look vocabulary .', 'To get positive frequency word vocabulary , count time appears positive tweet .', 'For instance , word happy appears one time first positive tweet , another time second positive tweet .', \"So 's positive frequency two .\", 'The complete table look like .', 'Feel free take pause check entry .', 'The logic applies getting negative frequency .', 'However , sake clarity , look example , word appears two time first tweet another time second one .', \"So 's negative frequency three .\", 'Take look entire table negative frequency feel free check value .', 'So entire table positive negative frequency corpus .', 'In practice coding , table dictionary mapping word class frequency .', \"So map word corresponding class frequency number time 's showed class .\", 'You know create frequency dictionary , map word class number time word showed corresponding class .', \"In next video , 're going use frequency dictionary represent tweet .\"]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences = nltk.sent_tokenize(str1)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_sentence = []\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "#     print(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e77fb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .',\n",
       " 1: 'By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !',\n",
       " 2: 'This Specialization designed taught two expert NLP , machine learning , deep learning .',\n",
       " 3: 'Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .',\n",
       " 4: 'Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .',\n",
       " 5: 'Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .',\n",
       " 6: 'The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .',\n",
       " 7: 'Other , I informative fun .',\n",
       " 8: 'From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !',\n",
       " 9: \"Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\",\n",
       " 10: \"Specifically , given word , want keep track number time , 's show positive class .\",\n",
       " 11: 'Given another word want keep track number time word showed negative class .',\n",
       " 12: 'Using count , extract feature use feature logistic regression classifier .',\n",
       " 13: \"So let 's take look .\",\n",
       " 14: 'It helpful first imagine two class would look .',\n",
       " 15: 'Here instance , could corpus consisting four tweet .',\n",
       " 16: 'Associated corpus , would set unique word , vocabulary .',\n",
       " 17: 'In example , vocabulary would eight unique word .',\n",
       " 18: 'For particular example sentiment analysis , two class .',\n",
       " 19: 'One class associated positive sentiment negative sentiment .',\n",
       " 20: \"So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\",\n",
       " 21: \"Let 's take set positive tweet .\",\n",
       " 22: 'Now , take look vocabulary .',\n",
       " 23: 'To get positive frequency word vocabulary , count time appears positive tweet .',\n",
       " 24: 'For instance , word happy appears one time first positive tweet , another time second positive tweet .',\n",
       " 25: \"So 's positive frequency two .\",\n",
       " 26: 'The complete table look like .',\n",
       " 27: 'Feel free take pause check entry .',\n",
       " 28: 'The logic applies getting negative frequency .',\n",
       " 29: 'However , sake clarity , look example , word appears two time first tweet another time second one .',\n",
       " 30: \"So 's negative frequency three .\",\n",
       " 31: 'Take look entire table negative frequency feel free check value .',\n",
       " 32: 'So entire table positive negative frequency corpus .',\n",
       " 33: 'In practice coding , table dictionary mapping word class frequency .',\n",
       " 34: \"So map word corresponding class frequency number time 's showed class .\",\n",
       " 35: 'You know create frequency dictionary , map word class number time word showed corresponding class .',\n",
       " 36: \"In next video , 're going use frequency dictionary represent tweet .\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for i in range(len(sentences)):\n",
    "    data[i] = sentences[i]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f516225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: sentence_id, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f55bd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .'],\n",
       " 1: ['By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , even built chatbot !'],\n",
       " 2: ['This Specialization designed taught two expert NLP , machine learning , deep learning .'],\n",
       " 3: ['Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .'],\n",
       " 4: ['Å\\x81ukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .'],\n",
       " 5: ['Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020 one Best course attented deeplearnig.ai last week assignment was\\\\n\\\\nto good solve cover studied entire course waiting course 4 nlp eagerly OA Aug 17 , 2020 Awesome .'],\n",
       " 6: ['The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .'],\n",
       " 7: ['Other , I informative fun .'],\n",
       " 8: ['From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regression !'],\n",
       " 9: [\"Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\"],\n",
       " 10: [\"Specifically , given word , want keep track number time , 's show positive class .\"],\n",
       " 11: ['Given another word want keep track number time word showed negative class .'],\n",
       " 12: ['Using count , extract feature use feature logistic regression classifier .'],\n",
       " 13: [\"So let 's take look .\"],\n",
       " 14: ['It helpful first imagine two class would look .'],\n",
       " 15: ['Here instance , could corpus consisting four tweet .'],\n",
       " 16: ['Associated corpus , would set unique word , vocabulary .'],\n",
       " 17: ['In example , vocabulary would eight unique word .'],\n",
       " 18: ['For particular example sentiment analysis , two class .'],\n",
       " 19: ['One class associated positive sentiment negative sentiment .'],\n",
       " 20: [\"So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\"],\n",
       " 21: [\"Let 's take set positive tweet .\"],\n",
       " 22: ['Now , take look vocabulary .'],\n",
       " 23: ['To get positive frequency word vocabulary , count time appears positive tweet .'],\n",
       " 24: ['For instance , word happy appears one time first positive tweet , another time second positive tweet .'],\n",
       " 25: [\"So 's positive frequency two .\"],\n",
       " 26: ['The complete table look like .'],\n",
       " 27: ['Feel free take pause check entry .'],\n",
       " 28: ['The logic applies getting negative frequency .'],\n",
       " 29: ['However , sake clarity , look example , word appears two time first tweet another time second one .'],\n",
       " 30: [\"So 's negative frequency three .\"],\n",
       " 31: ['Take look entire table negative frequency feel free check value .'],\n",
       " 32: ['So entire table positive negative frequency corpus .'],\n",
       " 33: ['In practice coding , table dictionary mapping word class frequency .'],\n",
       " 34: [\"So map word corresponding class frequency number time 's showed class .\"],\n",
       " 35: ['You know create frequency dictionary , map word class number time word showed corresponding class .'],\n",
       " 36: [\"In next video , 're going use frequency dictionary represent tweet .\"]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69e8e902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Specialization designed taught two expert NLP , machine learning , deep learning .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other , I informative fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Specifically , given word , want keep track number time , 's show positive class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Given another word want keep track number time word showed negative class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Using count , extract feature use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>So let 's take look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>It helpful first imagine two class would look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Here instance , could corpus consisting four tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Associated corpus , would set unique word , vocabulary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In example , vocabulary would eight unique word .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>For particular example sentiment analysis , two class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>One class associated positive sentiment negative sentiment .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Let 's take set positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Now , take look vocabulary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>To get positive frequency word vocabulary , count time appears positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>For instance , word happy appears one time first positive tweet , another time second positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>So 's positive frequency two .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The complete table look like .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Feel free take pause check entry .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The logic applies getting negative frequency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>However , sake clarity , look example , word appears two time first tweet another time second one .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>So 's negative frequency three .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Take look entire table negative frequency feel free check value .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>So entire table positive negative frequency corpus .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In practice coding , table dictionary mapping word class frequency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>So map word corresponding class frequency number time 's showed class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>You know create frequency dictionary , map word class number time word showed corresponding class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>In next video , 're going use frequency dictionary represent tweet .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               transcript\n",
       "0   4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...\n",
       "1   By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...\n",
       "2                                                                 This Specialization designed taught two expert NLP , machine learning , deep learning .\n",
       "3                                               Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .\n",
       "4                            Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .\n",
       "5   Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...\n",
       "6                                       The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .\n",
       "7                                                                                                                             Other , I informative fun .\n",
       "8   From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...\n",
       "9                           Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\n",
       "10                                                                     Specifically , given word , want keep track number time , 's show positive class .\n",
       "11                                                                            Given another word want keep track number time word showed negative class .\n",
       "12                                                                             Using count , extract feature use feature logistic regression classifier .\n",
       "13                                                                                                                                  So let 's take look .\n",
       "14                                                                                                        It helpful first imagine two class would look .\n",
       "15                                                                                                   Here instance , could corpus consisting four tweet .\n",
       "16                                                                                               Associated corpus , would set unique word , vocabulary .\n",
       "17                                                                                                      In example , vocabulary would eight unique word .\n",
       "18                                                                                                For particular example sentiment analysis , two class .\n",
       "19                                                                                           One class associated positive sentiment negative sentiment .\n",
       "20                                                      So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\n",
       "21                                                                                                                       Let 's take set positive tweet .\n",
       "22                                                                                                                           Now , take look vocabulary .\n",
       "23                                                                        To get positive frequency word vocabulary , count time appears positive tweet .\n",
       "24                                                 For instance , word happy appears one time first positive tweet , another time second positive tweet .\n",
       "25                                                                                                                         So 's positive frequency two .\n",
       "26                                                                                                                         The complete table look like .\n",
       "27                                                                                                                     Feel free take pause check entry .\n",
       "28                                                                                                         The logic applies getting negative frequency .\n",
       "29                                                    However , sake clarity , look example , word appears two time first tweet another time second one .\n",
       "30                                                                                                                       So 's negative frequency three .\n",
       "31                                                                                      Take look entire table negative frequency feel free check value .\n",
       "32                                                                                                   So entire table positive negative frequency corpus .\n",
       "33                                                                                   In practice coding , table dictionary mapping word class frequency .\n",
       "34                                                                                So map word corresponding class frequency number time 's showed class .\n",
       "35                                                    You know create frequency dictionary , map word class number time word showed corresponding class .\n",
       "36                                                                                   In next video , 're going use frequency dictionary represent tweet ."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b456fd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Natural Language Processing Specialization , : ) Perform sentiment analysis tweet using logistic regression naÃ¯ve Bayes , b ) Use vector space model discover relationship word use PCA reduce dimensionality vector space visualize relationship , c ) Write simple English French translation algorithm using pre-computed word embeddings locality-sensitive hashing relate word via approximate k-nearest neighbor search .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the transcript for Ali Wong\n",
    "data_df.transcript.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ad7f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe6b03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   natural language processing specialization this course video transcript in course  natural language pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp application perform questionanswering sentiment analysis  created tool translate language summarize text  even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert nlp  machine learning  deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford university also helped build deep learning specialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google brain coauthor tensorflow   trax library  transformer paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  localitysensitive hashing  sentiment analysis  vector space models    rating  ha aug    one best course atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little hard straight forward sometimes  youtube helped regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regression learn extract feature text numerical vector  build binary classifier tweet using logistic regre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer we ll learn generates count  use feature logistic regression classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specifically  given word  want keep track number time  s show positive class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>given another word want keep track number time word showed negative class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>using count  extract feature use feature logistic regression classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>so let s take look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it helpful first imagine two class would look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here instance  could corpus consisting four tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>associated corpus  would set unique word  vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>in example  vocabulary would eight unique word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for particular example sentiment analysis  two class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>one class associated positive sentiment negative sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>so taking corpus  d set two tweet belong positive class  set two tweet belong negative class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>let s take set positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>now  take look vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>to get positive frequency word vocabulary  count time appears positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for instance  word happy appears one time first positive tweet  another time second positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>so s positive frequency two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the complete table look like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>feel free take pause check entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the logic applies getting negative frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>however  sake clarity  look example  word appears two time first tweet another time second one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>so s negative frequency three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>take look entire table negative frequency feel free check value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>so entire table positive negative frequency corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>in practice coding  table dictionary mapping word class frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>so map word corresponding class frequency number time s showed class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>you know create frequency dictionary  map word class number time word showed corresponding class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>in next video  re going use frequency dictionary represent tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               transcript\n",
       "0      rating  â â  students enrolled course   natural language processing specialization this course video transcript in course  natural language pro...\n",
       "1   by end specialization  designed nlp application perform questionanswering sentiment analysis  created tool translate language summarize text  even...\n",
       "2                                                                    this specialization designed taught two expert nlp  machine learning  deep learning \n",
       "3                                                younes bensouda mourri instructor ai stanford university also helped build deep learning specialization \n",
       "4                                             åukasz kaiser staff research scientist google brain coauthor tensorflow   trax library  transformer paper \n",
       "5   machine translation  word embeddings  localitysensitive hashing  sentiment analysis  vector space models    rating  ha aug    one best course atte...\n",
       "6                                          the lecture exciting detailed  though little hard straight forward sometimes  youtube helped regression model \n",
       "7                                                                                                                               other  i informative fun \n",
       "8   from lesson sentiment analysis logistic regression learn extract feature text numerical vector  build binary classifier tweet using logistic regre...\n",
       "9                              instructor instructor senior curriculum developer we ll learn generates count  use feature logistic regression classifier \n",
       "10                                                                          specifically  given word  want keep track number time  s show positive class \n",
       "11                                                                             given another word want keep track number time word showed negative class \n",
       "12                                                                               using count  extract feature use feature logistic regression classifier \n",
       "13                                                                                                                                    so let s take look \n",
       "14                                                                                                         it helpful first imagine two class would look \n",
       "15                                                                                                     here instance  could corpus consisting four tweet \n",
       "16                                                                                                  associated corpus  would set unique word  vocabulary \n",
       "17                                                                                                        in example  vocabulary would eight unique word \n",
       "18                                                                                                  for particular example sentiment analysis  two class \n",
       "19                                                                                            one class associated positive sentiment negative sentiment \n",
       "20                                                          so taking corpus  d set two tweet belong positive class  set two tweet belong negative class \n",
       "21                                                                                                                         let s take set positive tweet \n",
       "22                                                                                                                             now  take look vocabulary \n",
       "23                                                                          to get positive frequency word vocabulary  count time appears positive tweet \n",
       "24                                                    for instance  word happy appears one time first positive tweet  another time second positive tweet \n",
       "25                                                                                                                           so s positive frequency two \n",
       "26                                                                                                                          the complete table look like \n",
       "27                                                                                                                      feel free take pause check entry \n",
       "28                                                                                                          the logic applies getting negative frequency \n",
       "29                                                        however  sake clarity  look example  word appears two time first tweet another time second one \n",
       "30                                                                                                                         so s negative frequency three \n",
       "31                                                                                       take look entire table negative frequency feel free check value \n",
       "32                                                                                                    so entire table positive negative frequency corpus \n",
       "33                                                                                     in practice coding  table dictionary mapping word class frequency \n",
       "34                                                                                  so map word corresponding class frequency number time s showed class \n",
       "35                                                      you know create frequency dictionary  map word class number time word showed corresponding class \n",
       "36                                                                                      in next video  re going use frequency dictionary represent tweet "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4562f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8f05019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating  â â  students enrolled course   natural language processing specialization this course video transcript in course  natural language pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by end specialization  designed nlp application perform questionanswering sentiment analysis  created tool translate language summarize text  even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this specialization designed taught two expert nlp  machine learning  deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>younes bensouda mourri instructor ai stanford university also helped build deep learning specialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åukasz kaiser staff research scientist google brain coauthor tensorflow   trax library  transformer paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine translation  word embeddings  localitysensitive hashing  sentiment analysis  vector space models    rating  ha aug    one best course atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the lecture exciting detailed  though little hard straight forward sometimes  youtube helped regression model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other  i informative fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>from lesson sentiment analysis logistic regression learn extract feature text numerical vector  build binary classifier tweet using logistic regre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor instructor senior curriculum developer we ll learn generates count  use feature logistic regression classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>specifically  given word  want keep track number time  s show positive class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>given another word want keep track number time word showed negative class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>using count  extract feature use feature logistic regression classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>so let s take look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it helpful first imagine two class would look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here instance  could corpus consisting four tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>associated corpus  would set unique word  vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>in example  vocabulary would eight unique word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for particular example sentiment analysis  two class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>one class associated positive sentiment negative sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>so taking corpus  d set two tweet belong positive class  set two tweet belong negative class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>let s take set positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>now  take look vocabulary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>to get positive frequency word vocabulary  count time appears positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for instance  word happy appears one time first positive tweet  another time second positive tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>so s positive frequency two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the complete table look like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>feel free take pause check entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the logic applies getting negative frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>however  sake clarity  look example  word appears two time first tweet another time second one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>so s negative frequency three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>take look entire table negative frequency feel free check value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>so entire table positive negative frequency corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>in practice coding  table dictionary mapping word class frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>so map word corresponding class frequency number time s showed class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>you know create frequency dictionary  map word class number time word showed corresponding class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>in next video  re going use frequency dictionary represent tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               transcript\n",
       "0      rating  â â  students enrolled course   natural language processing specialization this course video transcript in course  natural language pro...\n",
       "1   by end specialization  designed nlp application perform questionanswering sentiment analysis  created tool translate language summarize text  even...\n",
       "2                                                                    this specialization designed taught two expert nlp  machine learning  deep learning \n",
       "3                                                younes bensouda mourri instructor ai stanford university also helped build deep learning specialization \n",
       "4                                             åukasz kaiser staff research scientist google brain coauthor tensorflow   trax library  transformer paper \n",
       "5   machine translation  word embeddings  localitysensitive hashing  sentiment analysis  vector space models    rating  ha aug    one best course atte...\n",
       "6                                          the lecture exciting detailed  though little hard straight forward sometimes  youtube helped regression model \n",
       "7                                                                                                                               other  i informative fun \n",
       "8   from lesson sentiment analysis logistic regression learn extract feature text numerical vector  build binary classifier tweet using logistic regre...\n",
       "9                              instructor instructor senior curriculum developer we ll learn generates count  use feature logistic regression classifier \n",
       "10                                                                          specifically  given word  want keep track number time  s show positive class \n",
       "11                                                                             given another word want keep track number time word showed negative class \n",
       "12                                                                               using count  extract feature use feature logistic regression classifier \n",
       "13                                                                                                                                    so let s take look \n",
       "14                                                                                                         it helpful first imagine two class would look \n",
       "15                                                                                                     here instance  could corpus consisting four tweet \n",
       "16                                                                                                  associated corpus  would set unique word  vocabulary \n",
       "17                                                                                                        in example  vocabulary would eight unique word \n",
       "18                                                                                                  for particular example sentiment analysis  two class \n",
       "19                                                                                            one class associated positive sentiment negative sentiment \n",
       "20                                                          so taking corpus  d set two tweet belong positive class  set two tweet belong negative class \n",
       "21                                                                                                                         let s take set positive tweet \n",
       "22                                                                                                                             now  take look vocabulary \n",
       "23                                                                          to get positive frequency word vocabulary  count time appears positive tweet \n",
       "24                                                    for instance  word happy appears one time first positive tweet  another time second positive tweet \n",
       "25                                                                                                                           so s positive frequency two \n",
       "26                                                                                                                          the complete table look like \n",
       "27                                                                                                                      feel free take pause check entry \n",
       "28                                                                                                          the logic applies getting negative frequency \n",
       "29                                                        however  sake clarity  look example  word appears two time first tweet another time second one \n",
       "30                                                                                                                         so s negative frequency three \n",
       "31                                                                                       take look entire table negative frequency feel free check value \n",
       "32                                                                                                    so entire table positive negative frequency corpus \n",
       "33                                                                                     in practice coding  table dictionary mapping word class frequency \n",
       "34                                                                                  so map word corresponding class frequency number time s showed class \n",
       "35                                                      you know create frequency dictionary  map word class number time word showed corresponding class \n",
       "36                                                                                      in next video  re going use frequency dictionary represent tweet "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0de954eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Specialization designed taught two expert NLP , machine learning , deep learning .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other , I informative fun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Specifically , given word , want keep track number time , 's show positive class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Given another word want keep track number time word showed negative class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Using count , extract feature use feature logistic regression classifier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>So let 's take look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>It helpful first imagine two class would look .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Here instance , could corpus consisting four tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Associated corpus , would set unique word , vocabulary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In example , vocabulary would eight unique word .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>For particular example sentiment analysis , two class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>One class associated positive sentiment negative sentiment .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Let 's take set positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Now , take look vocabulary .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>To get positive frequency word vocabulary , count time appears positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>For instance , word happy appears one time first positive tweet , another time second positive tweet .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>So 's positive frequency two .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The complete table look like .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Feel free take pause check entry .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The logic applies getting negative frequency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>However , sake clarity , look example , word appears two time first tweet another time second one .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>So 's negative frequency three .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Take look entire table negative frequency feel free check value .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>So entire table positive negative frequency corpus .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In practice coding , table dictionary mapping word class frequency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>So map word corresponding class frequency number time 's showed class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>You know create frequency dictionary , map word class number time word showed corresponding class .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>In next video , 're going use frequency dictionary represent tweet .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               transcript\n",
       "0   4.6 ( 3,348 rating ) Â |Â 99K Students Enrolled Course 1 4 Natural Language Processing Specialization This Course Video Transcript In Course 1 Nat...\n",
       "1   By end Specialization , designed NLP application perform question-answering sentiment analysis , created tool translate language summarize text , ...\n",
       "2                                                                 This Specialization designed taught two expert NLP , machine learning , deep learning .\n",
       "3                                               Younes Bensouda Mourri Instructor AI Stanford University also helped build Deep Learning Specialization .\n",
       "4                            Åukasz Kaiser Staff Research Scientist Google Brain co-author Tensorflow , Tensor2Tensor Trax library , Transformer paper .\n",
       "5   Machine Translation , Word Embeddings , Locality-Sensitive Hashing , Sentiment Analysis , Vector Space Models 4.6 ( 3,348 rating ) HA Aug 9 , 2020...\n",
       "6                                       The lecture exciting detailed , though little hard straight forward sometimes , Youtube helped Regression model .\n",
       "7                                                                                                                             Other , I informative fun .\n",
       "8   From lesson Sentiment Analysis Logistic Regression Learn extract feature text numerical vector , build binary classifier tweet using logistic regr...\n",
       "9                           Instructor Instructor Senior Curriculum Developer We 'll learn generates count , use feature logistic regression classifier .\n",
       "10                                                                     Specifically , given word , want keep track number time , 's show positive class .\n",
       "11                                                                            Given another word want keep track number time word showed negative class .\n",
       "12                                                                             Using count , extract feature use feature logistic regression classifier .\n",
       "13                                                                                                                                  So let 's take look .\n",
       "14                                                                                                        It helpful first imagine two class would look .\n",
       "15                                                                                                   Here instance , could corpus consisting four tweet .\n",
       "16                                                                                               Associated corpus , would set unique word , vocabulary .\n",
       "17                                                                                                      In example , vocabulary would eight unique word .\n",
       "18                                                                                                For particular example sentiment analysis , two class .\n",
       "19                                                                                           One class associated positive sentiment negative sentiment .\n",
       "20                                                      So taking corpus , 'd set two tweet belong positive class , set two tweet belong negative class .\n",
       "21                                                                                                                       Let 's take set positive tweet .\n",
       "22                                                                                                                           Now , take look vocabulary .\n",
       "23                                                                        To get positive frequency word vocabulary , count time appears positive tweet .\n",
       "24                                                 For instance , word happy appears one time first positive tweet , another time second positive tweet .\n",
       "25                                                                                                                         So 's positive frequency two .\n",
       "26                                                                                                                         The complete table look like .\n",
       "27                                                                                                                     Feel free take pause check entry .\n",
       "28                                                                                                         The logic applies getting negative frequency .\n",
       "29                                                    However , sake clarity , look example , word appears two time first tweet another time second one .\n",
       "30                                                                                                                       So 's negative frequency three .\n",
       "31                                                                                      Take look entire table negative frequency feel free check value .\n",
       "32                                                                                                   So entire table positive negative frequency corpus .\n",
       "33                                                                                   In practice coding , table dictionary mapping word class frequency .\n",
       "34                                                                                So map word corresponding class frequency number time 's showed class .\n",
       "35                                                    You know create frequency dictionary , map word class number time word showed corresponding class .\n",
       "36                                                                                   In next video , 're going use frequency dictionary represent tweet ."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at our dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ab7019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ed8c8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>analysis</th>\n",
       "      <th>appears</th>\n",
       "      <th>application</th>\n",
       "      <th>applies</th>\n",
       "      <th>approximate</th>\n",
       "      <th>assignment</th>\n",
       "      <th>associated</th>\n",
       "      <th>attented</th>\n",
       "      <th>...</th>\n",
       "      <th>visualize</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>waiting</th>\n",
       "      <th>want</th>\n",
       "      <th>wasnnto</th>\n",
       "      <th>week</th>\n",
       "      <th>word</th>\n",
       "      <th>write</th>\n",
       "      <th>younes</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ai  algorithm  analysis  appears  application  applies  approximate  \\\n",
       "0    0          1         1        0            0        0            1   \n",
       "1    0          0         1        0            1        0            0   \n",
       "2    0          0         0        0            0        0            0   \n",
       "3    1          0         0        0            0        0            0   \n",
       "4    0          0         0        0            0        0            0   \n",
       "5    0          0         1        0            0        0            0   \n",
       "6    0          0         0        0            0        0            0   \n",
       "7    0          0         0        0            0        0            0   \n",
       "8    0          0         1        0            0        0            0   \n",
       "9    0          0         0        0            0        0            0   \n",
       "10   0          0         0        0            0        0            0   \n",
       "11   0          0         0        0            0        0            0   \n",
       "12   0          0         0        0            0        0            0   \n",
       "13   0          0         0        0            0        0            0   \n",
       "14   0          0         0        0            0        0            0   \n",
       "15   0          0         0        0            0        0            0   \n",
       "16   0          0         0        0            0        0            0   \n",
       "17   0          0         0        0            0        0            0   \n",
       "18   0          0         1        0            0        0            0   \n",
       "19   0          0         0        0            0        0            0   \n",
       "20   0          0         0        0            0        0            0   \n",
       "21   0          0         0        0            0        0            0   \n",
       "22   0          0         0        0            0        0            0   \n",
       "23   0          0         0        1            0        0            0   \n",
       "24   0          0         0        1            0        0            0   \n",
       "25   0          0         0        0            0        0            0   \n",
       "26   0          0         0        0            0        0            0   \n",
       "27   0          0         0        0            0        0            0   \n",
       "28   0          0         0        0            0        1            0   \n",
       "29   0          0         0        1            0        0            0   \n",
       "30   0          0         0        0            0        0            0   \n",
       "31   0          0         0        0            0        0            0   \n",
       "32   0          0         0        0            0        0            0   \n",
       "33   0          0         0        0            0        0            0   \n",
       "34   0          0         0        0            0        0            0   \n",
       "35   0          0         0        0            0        0            0   \n",
       "36   0          0         0        0            0        0            0   \n",
       "\n",
       "    assignment  associated  attented  ...  visualize  vocabulary  waiting  \\\n",
       "0            0           0         0  ...          1           0        0   \n",
       "1            0           0         0  ...          0           0        0   \n",
       "2            0           0         0  ...          0           0        0   \n",
       "3            0           0         0  ...          0           0        0   \n",
       "4            0           0         0  ...          0           0        0   \n",
       "5            1           0         1  ...          0           0        1   \n",
       "6            0           0         0  ...          0           0        0   \n",
       "7            0           0         0  ...          0           0        0   \n",
       "8            0           0         0  ...          0           0        0   \n",
       "9            0           0         0  ...          0           0        0   \n",
       "10           0           0         0  ...          0           0        0   \n",
       "11           0           0         0  ...          0           0        0   \n",
       "12           0           0         0  ...          0           0        0   \n",
       "13           0           0         0  ...          0           0        0   \n",
       "14           0           0         0  ...          0           0        0   \n",
       "15           0           0         0  ...          0           0        0   \n",
       "16           0           1         0  ...          0           1        0   \n",
       "17           0           0         0  ...          0           1        0   \n",
       "18           0           0         0  ...          0           0        0   \n",
       "19           0           1         0  ...          0           0        0   \n",
       "20           0           0         0  ...          0           0        0   \n",
       "21           0           0         0  ...          0           0        0   \n",
       "22           0           0         0  ...          0           1        0   \n",
       "23           0           0         0  ...          0           1        0   \n",
       "24           0           0         0  ...          0           0        0   \n",
       "25           0           0         0  ...          0           0        0   \n",
       "26           0           0         0  ...          0           0        0   \n",
       "27           0           0         0  ...          0           0        0   \n",
       "28           0           0         0  ...          0           0        0   \n",
       "29           0           0         0  ...          0           0        0   \n",
       "30           0           0         0  ...          0           0        0   \n",
       "31           0           0         0  ...          0           0        0   \n",
       "32           0           0         0  ...          0           0        0   \n",
       "33           0           0         0  ...          0           0        0   \n",
       "34           0           0         0  ...          0           0        0   \n",
       "35           0           0         0  ...          0           0        0   \n",
       "36           0           0         0  ...          0           0        0   \n",
       "\n",
       "    want  wasnnto  week  word  write  younes  youtube  \n",
       "0      0        0     0     3      1       0        0  \n",
       "1      0        0     0     0      0       0        0  \n",
       "2      0        0     0     0      0       0        0  \n",
       "3      0        0     0     0      0       1        0  \n",
       "4      0        0     0     0      0       0        0  \n",
       "5      0        1     1     1      0       0        0  \n",
       "6      0        0     0     0      0       0        1  \n",
       "7      0        0     0     0      0       0        0  \n",
       "8      0        0     0     0      0       0        0  \n",
       "9      0        0     0     0      0       0        0  \n",
       "10     1        0     0     1      0       0        0  \n",
       "11     1        0     0     2      0       0        0  \n",
       "12     0        0     0     0      0       0        0  \n",
       "13     0        0     0     0      0       0        0  \n",
       "14     0        0     0     0      0       0        0  \n",
       "15     0        0     0     0      0       0        0  \n",
       "16     0        0     0     1      0       0        0  \n",
       "17     0        0     0     1      0       0        0  \n",
       "18     0        0     0     0      0       0        0  \n",
       "19     0        0     0     0      0       0        0  \n",
       "20     0        0     0     0      0       0        0  \n",
       "21     0        0     0     0      0       0        0  \n",
       "22     0        0     0     0      0       0        0  \n",
       "23     0        0     0     1      0       0        0  \n",
       "24     0        0     0     1      0       0        0  \n",
       "25     0        0     0     0      0       0        0  \n",
       "26     0        0     0     0      0       0        0  \n",
       "27     0        0     0     0      0       0        0  \n",
       "28     0        0     0     0      0       0        0  \n",
       "29     0        0     0     1      0       0        0  \n",
       "30     0        0     0     0      0       0        0  \n",
       "31     0        0     0     0      0       0        0  \n",
       "32     0        0     0     0      0       0        0  \n",
       "33     0        0     0     1      0       0        0  \n",
       "34     0        0     0     1      0       0        0  \n",
       "35     0        0     0     2      0       0        0  \n",
       "36     0        0     0     0      0       0        0  \n",
       "\n",
       "[37 rows x 179 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06a38c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2b9b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the cleaned data (before putting it in document-term matrix format) and the CountVectorizer object\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"count_vectorizer.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
